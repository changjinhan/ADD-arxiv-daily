[![Contributors][contributors-shield]][contributors-url]
[![Forks][forks-shield]][forks-url]
[![Stargazers][stars-shield]][stars-url]
[![Issues][issues-shield]][issues-url]

# Talking-Face Research Papers (With GPT Analysis)
> Updated on 2024.03.12
Current Search Keywords: `Talking Face`, `Talking Head`, `Visual Dubbing`, `Face Genertation`, `Lip Sync`, `Talker`, `Portrait`, `Talking Video`, `Head Synthesis`, `Face Reenactment`, `Wav2Lip`, `Talking Avatar`, `Lip Generation`, `Lip-Synchronization`, `Portrait Animation`, `Facial Animation`, `Lip Expert`

> If you have any other keywords, please feel free to let us know :) 

> We now offer support for article analysis through large language models. You can view this feature by clicking the `Paper Analysis` link below. Currently, we are experimenting with `Claude.ai` and plan to also integrate `GPT-4-turbo` for a comparative analysis of articles. This is to help everyone **quickly skim** through the latest research papers. 

 

<details>
  <summary>Recent Trends (by Claude.ai)</summary>
  <ol>
    <li> Based on analyzing the collection of academic papers, I have identified five prominent recent trends in text-to-video generation models, with a key focus on controllability, multimodality, video quality improvements, image animation, and personalized video generation. 

<b>Controllability</b>: Recent works have focused on improving control over generated videos through multimodal inputs like text, images, depth maps, etc. Models leverage techniques like attention mechanisms, ControlNets, and classifier guidance to enable precise conditioning on desired visual semantics. This facilitates applications like video editing, geometry-controlled generation, and subject customization.

<b>Multimodality</b>: Leveraging multiple input modalities like text, images, audio, depth maps etc. is an emerging trend to improve conditioning and allow for fine-grained control over generated video attributes. Models employ techniques like cross-attention over multimodal embeddings and masked blending to effectively utilize complementary strengths of different input types.

<b>Video quality improvements</b>: Recent video diffusion models aim to enhance the visual quality and temporal consistency of generated videos using spatial-temporal architectures, noise scheduling optimizations and loss functions tailored for video data. Models also reuse weights from high-quality image diffusion models as initialization to benefit from their superior generative capabilities.

<b>Image animation</b>: Animating still images into video using textual prompts is an area of increasing research focus. Methods employ techniques like masked conditions, viewpoint consistency losses, and warping-based rendering to transform input images into realistic and temporally coherent videos.

<b>Personalized video generation</b>: Customizing foundation models using few samples for user-specific video generation enables applications like talking avatars and virtual assistants. Approaches rely on model fine-tuning, adapters, or additional cross-attention layers to inject personalization without requiring extensive retraining.</li>
  </ol>
</details>

[>>>> Each Paper Analysis (by Claude.ai) <<<<](https://github.com/liutaocode/talking-face-arxiv-daily/blob/main/analysis_by_claude_ai.md) 

[Web Page](https://liutaocode.github.io/talking-face-arxiv-daily/) ([Scrape Code](https://github.com/liutaocode/talking-face-arxiv-daily)) 

<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#talking-face>Talking Face</a></li>
    <li><a href=#image-animation>Image Animation</a></li>
    <li><a href=#tts>TTS</a></li>
    <li><a href=#video-diffusion>Video Diffusion</a></li>
  </ol>
</details>

## Talking Face

- 2023-12-13, **uTalk: Bridging the Gap Between Humans and AI**, Hussam Azzuni et.al., Paper: [http://arxiv.org/abs/2310.02739](http://arxiv.org/abs/2310.02739)
- 2023-05-09, **Zero-shot personalized lip-to-speech synthesis with face image based voice control**, Zheng-Yan Sheng et.al., Paper: [http://arxiv.org/abs/2305.14359](http://arxiv.org/abs/2305.14359)
- 2021-05-07, **Write-a-speaker: Text-based Emotional and Rhythmic Talking-head Generation**, Lincheng Li et.al., Paper: [http://arxiv.org/abs/2104.07995](http://arxiv.org/abs/2104.07995), Code: **[https://github.com/FuxiVirtualHuman/Write-a-Speaker](https://github.com/FuxiVirtualHuman/Write-a-Speaker)**
- 2020-05-07, **What comprises a good talking-head video generation?: A Survey and Benchmark**, Lele Chen et.al., Paper: [http://arxiv.org/abs/2005.03201](http://arxiv.org/abs/2005.03201), Code: **[https://github.com/lelechen63/talking-head-generation-survey](https://github.com/lelechen63/talking-head-generation-survey)**
- 2023-12-07, **VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D Hybrid Prior**, Xusen Sun et.al., Paper: [http://arxiv.org/abs/2312.01841](http://arxiv.org/abs/2312.01841)
- 2022-07-22, **Visual Speech-Aware Perceptual 3D Facial Expression Reconstruction from Videos**, Panagiotis P. Filntisis et.al., Paper: [http://arxiv.org/abs/2207.11094](http://arxiv.org/abs/2207.11094), Code: **[https://github.com/filby89/spectre](https://github.com/filby89/spectre)**
- 2018-05-24, **VisemeNet: Audio-Driven Animator-Centric Speech Animation**, Yang Zhou et.al., Paper: [http://arxiv.org/abs/1805.09488](http://arxiv.org/abs/1805.09488)
- 2022-07-20, **VisageSynTalk: Unseen Speaker Video-to-Speech Synthesis via Speech-Visage Feature Selection**, Joanna Hong et.al., Paper: [http://arxiv.org/abs/2206.07458](http://arxiv.org/abs/2206.07458)
- 2022-11-27, **VideoReTalking: Audio-based Lip Synchronization for Talking Head Video Editing In the Wild**, Kun Cheng et.al., Paper: [http://arxiv.org/abs/2211.14758](http://arxiv.org/abs/2211.14758)
- 2021-10-26, **ViDA-MAN: Visual Dialog with Digital Humans**, Tong Shen et.al., Paper: [http://arxiv.org/abs/2110.13384](http://arxiv.org/abs/2110.13384)
- 2023-08-11, **Versatile Face Animator: Driving Arbitrary 3D Facial Avatar in RGBD Space**, Haoyu Wang et.al., Paper: [http://arxiv.org/abs/2308.06076](http://arxiv.org/abs/2308.06076), Code: **[https://github.com/why986/VFA](https://github.com/why986/VFA)**
- 2023-12-18, **VectorTalker: SVG Talking Face Generation with Progressive Vectorisation**, Hao Hu et.al., Paper: [http://arxiv.org/abs/2312.11568](http://arxiv.org/abs/2312.11568)
- 2023-04-24, **VR Facial Animation for Immersive Telepresence Avatars**, Andre Rochow et.al., Paper: [http://arxiv.org/abs/2304.12051](http://arxiv.org/abs/2304.12051)
- 2023-08-11, **VAST: Vivify Your Talking Avatar via Zero-Shot Expressive Facial Style Transfer**, Liyang Chen et.al., Paper: [http://arxiv.org/abs/2308.04830](http://arxiv.org/abs/2308.04830)
- 2022-05-27, **Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast**, Boqing Zhu et.al., Paper: [http://arxiv.org/abs/2204.14057](http://arxiv.org/abs/2204.14057), Code: **[https://github.com/cocoxili/cmpc](https://github.com/cocoxili/cmpc)**
- 2023-09-01, **Unsupervised Learning of Style-Aware Facial Animation from Real Acting Performances**, Wolfgang Paier et.al., Paper: [http://arxiv.org/abs/2306.10006](http://arxiv.org/abs/2306.10006)
- 2021-08-12, **UniFaceGAN: A Unified Framework for Temporally Consistent Facial Video Editing**, Meng Cao et.al., Paper: [http://arxiv.org/abs/2108.05650](http://arxiv.org/abs/2108.05650)
- 2023-05-19, **UniFLG: Unified Facial Landmark Generator from Text or Speech**, Kentaro Mitsui et.al., Paper: [http://arxiv.org/abs/2302.14337](http://arxiv.org/abs/2302.14337)
- 2022-04-03, **Txt2Vid: Ultra-Low Bitrate Compression of Talking-Head Videos via Text**, Pulkit Tandon et.al., Paper: [http://arxiv.org/abs/2106.14014](http://arxiv.org/abs/2106.14014), Code: **[https://github.com/tpulkit/txt2vid](https://github.com/tpulkit/txt2vid)**
- 2022-04-06, **Transformer-S2A: Robust and Efficient Speech-to-Animation**, Liyang Chen et.al., Paper: [http://arxiv.org/abs/2111.09771](http://arxiv.org/abs/2111.09771)
- 2023-12-23, **TransFace: Unit-Based Audio-Visual Speech Synthesizer for Talking Head Translation**, Xize Cheng et.al., Paper: [http://arxiv.org/abs/2312.15197](http://arxiv.org/abs/2312.15197)
- 2024-01-02, **Towards a Simultaneous and Granular Identity-Expression Control in Personalized Face Generation**, Renshuai Liu et.al., Paper: [http://arxiv.org/abs/2401.01207](http://arxiv.org/abs/2401.01207)
- 2022-01-17, **Towards Realistic Visual Dubbing with Heterogeneous Sources**, Tianyi Xie et.al., Paper: [http://arxiv.org/abs/2201.06260](http://arxiv.org/abs/2201.06260)
- 2022-10-04, **Towards MOOCs for Lipreading: Using Synthetic Talking Heads to Train Humans in Lipreading at Scale**, Aditya Agarwal et.al., Paper: [http://arxiv.org/abs/2208.09796](http://arxiv.org/abs/2208.09796)
- 2018-11-22, **Towards Highly Accurate and Stable Face Alignment for High-Resolution Videos**, Ying Tai et.al., Paper: [http://arxiv.org/abs/1811.00342](http://arxiv.org/abs/1811.00342), Code: **[https://github.com/tyshiwo/FHR_alignment](https://github.com/tyshiwo/FHR_alignment)**
- 2020-03-01, **Towards Automatic Face-to-Face Translation**, Prajwal K R et.al., Paper: [http://arxiv.org/abs/2003.00418](http://arxiv.org/abs/2003.00418), Code: **[https://github.com/Rudrabha/LipGAN](https://github.com/Rudrabha/LipGAN)**
- 2023-08-24, **ToonTalker: Cross-Domain Face Reenactment**, Yuan Gong et.al., Paper: [http://arxiv.org/abs/2308.12866](http://arxiv.org/abs/2308.12866)
- 2022-02-22, **Thinking the Fusion Strategy of Multi-reference Face Reenactment**, Takuya Yashima et.al., Paper: [http://arxiv.org/abs/2202.10758](http://arxiv.org/abs/2202.10758)
- 2022-03-29, **Thin-Plate Spline Motion Model for Image Animation**, Jian Zhao et.al., Paper: [http://arxiv.org/abs/2203.14367](http://arxiv.org/abs/2203.14367), Code: **[https://github.com/yoyo-nb/thin-plate-spline-motion-model](https://github.com/yoyo-nb/thin-plate-spline-motion-model)**
- 2023-10-23, **The Self 2.0: How AI-Enhanced Self-Clones Transform Self-Perception and Improve Presentation Skills**, Qingxiao Zheng et.al., Paper: [http://arxiv.org/abs/2310.15112](http://arxiv.org/abs/2310.15112)
- 2023-09-18, **That's What I Said: Fully-Controllable Talking Face Generation**, Youngjoon Jang et.al., Paper: [http://arxiv.org/abs/2304.03275](http://arxiv.org/abs/2304.03275)
- 2022-01-22, **Text2Video: Text-driven Talking-head Video Synthesis with Personalized Phoneme-Pose Dictionary**, Sibo Zhang et.al., Paper: [http://arxiv.org/abs/2104.14631](http://arxiv.org/abs/2104.14631)
- 2022-05-31, **Text/Speech-Driven Full-Body Animation**, Wenlin Zhuang et.al., Paper: [http://arxiv.org/abs/2205.15573](http://arxiv.org/abs/2205.15573)
- 2023-08-12, **Text-to-Video: a Two-stage Framework for Zero-shot Identity-agnostic Talking-head Generation**, Zhichao Wang et.al., Paper: [http://arxiv.org/abs/2308.06457](http://arxiv.org/abs/2308.06457), Code: **[https://github.com/zhichaowang970201/text-to-video](https://github.com/zhichaowang970201/text-to-video)**
- 2024-01-18, **Text-driven Talking Face Synthesis by Reprogramming Audio-driven Models**, Jeongsoo Choi et.al., Paper: [http://arxiv.org/abs/2306.16003](http://arxiv.org/abs/2306.16003)
- 2019-06-04, **Text-based Editing of Talking-head Video**, Ohad Fried et.al., Paper: [http://arxiv.org/abs/1906.01524](http://arxiv.org/abs/1906.01524)
- 2020-07-16, **Talking-head Generation with Rhythmic Head Motion**, Lele Chen et.al., Paper: [http://arxiv.org/abs/2007.08547](http://arxiv.org/abs/2007.08547), Code: **[https://github.com/lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion](https://github.com/lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion)**
- 2020-03-05, **Talking-Heads Attention**, Noam Shazeer et.al., Paper: [http://arxiv.org/abs/2003.02436](http://arxiv.org/abs/2003.02436), Code: **[https://github.com/zygmuntz/hyperband](https://github.com/zygmuntz/hyperband)**
- 2023-11-30, **Talking Head(?) Anime from a Single Image 4: Improved Model and Its Distillation**, Pramook Khungurn et.al., Paper: [http://arxiv.org/abs/2311.17409](http://arxiv.org/abs/2311.17409)
- 2022-09-09, **Talking Head from Speech Audio using a Pre-trained Image Generator**, Mohammed M. Alghamdi et.al., Paper: [http://arxiv.org/abs/2209.04252](http://arxiv.org/abs/2209.04252)
- 2022-12-07, **Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors**, Zhentao Yu et.al., Paper: [http://arxiv.org/abs/2212.04248](http://arxiv.org/abs/2212.04248)
- 2021-10-19, **Talking Head Generation with Audio and Speech Related Facial Action Units**, Sen Chen et.al., Paper: [http://arxiv.org/abs/2110.09951](http://arxiv.org/abs/2110.09951)
- 2022-04-27, **Talking Head Generation Driven by Speech-Related Facial Action Units and Audio- Based on Multimodal Representation Fusion**, Sen Chen et.al., Paper: [http://arxiv.org/abs/2204.12756](http://arxiv.org/abs/2204.12756)
- 2022-05-13, **Talking Face Generation with Multilingual TTS**, Hyoung-Kyu Song et.al., Paper: [http://arxiv.org/abs/2205.06421](http://arxiv.org/abs/2205.06421)
- 2019-07-25, **Talking Face Generation by Conditional Recurrent Adversarial Network**, Yang Song et.al., Paper: [http://arxiv.org/abs/1804.04786](http://arxiv.org/abs/1804.04786), Code: **[https://github.com/susanqq/Talking_Face_Generation](https://github.com/susanqq/Talking_Face_Generation)**
- 2019-04-23, **Talking Face Generation by Adversarially Disentangled Audio-Visual Representation**, Hang Zhou et.al., Paper: [http://arxiv.org/abs/1807.07860](http://arxiv.org/abs/1807.07860)
- 2023-04-01, **TalkCLIP: Talking Head Generation with Text-Guided Expressive Speaking Styles**, Yifeng Ma et.al., Paper: [http://arxiv.org/abs/2304.00334](http://arxiv.org/abs/2304.00334)
- 2023-11-28, **THInImg: Cross-modal Steganography for Presenting Talking Heads in Images**, Lin Zhao et.al., Paper: [http://arxiv.org/abs/2311.17177](http://arxiv.org/abs/2311.17177)
- 2023-11-08, **Synthetic Speaking Children -- Why We Need Them and How to Make Them**, Muhammad Ali Farooq et.al., Paper: [http://arxiv.org/abs/2311.06307](http://arxiv.org/abs/2311.06307)
- 2023-03-24, **Synthesizing Photorealistic Virtual Humans Through Cross-modal Disentanglement**, Siddarth Ravichandran et.al., Paper: [http://arxiv.org/abs/2209.01320](http://arxiv.org/abs/2209.01320)
- 2022-11-03, **SyncTalkFace: Talking Face Generation with Precise Lip-Syncing via Audio-Lip Memory**, Se Jin Park et.al., Paper: [http://arxiv.org/abs/2211.00924](http://arxiv.org/abs/2211.00924)
- 2023-11-29, **SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis**, Ziqiao Peng et.al., Paper: [http://arxiv.org/abs/2311.17590](http://arxiv.org/abs/2311.17590), Code: **[https://github.com/ZiqiaoPeng/SyncTalk](https://github.com/ZiqiaoPeng/SyncTalk)**
- 2022-08-23, **StyleTalker: One-shot Style-based Audio-driven Talking Head Video Generation**, Dongchan Min et.al., Paper: [http://arxiv.org/abs/2208.10922](http://arxiv.org/abs/2208.10922)
- 2023-06-10, **StyleTalk: One-shot Talking Head Generation with Controllable Speaking Styles**, Yifeng Ma et.al., Paper: [http://arxiv.org/abs/2301.01081](http://arxiv.org/abs/2301.01081), Code: **[https://github.com/fuxivirtualhuman/styletalk](https://github.com/fuxivirtualhuman/styletalk)**
- 2023-05-09, **StyleSync: High-Fidelity Generalized and Personalized Lip Sync in Style-based Generator**, Jiazhi Guan et.al., Paper: [http://arxiv.org/abs/2305.05445](http://arxiv.org/abs/2305.05445)
- 2022-09-27, **StyleMask: Disentangling the Style Space of StyleGAN2 for Neural Face Reenactment**, Stella Bounareli et.al., Paper: [http://arxiv.org/abs/2209.13375](http://arxiv.org/abs/2209.13375), Code: **[https://github.com/stelabou/stylemask](https://github.com/stelabou/stylemask)**
- 2024-02-12, **StyleLipSync: Style-based Personalized Lip-sync Video Generation**, Taekyung Ki et.al., Paper: [http://arxiv.org/abs/2305.00521](http://arxiv.org/abs/2305.00521)
- 2022-03-17, **StyleHEAT: One-Shot High-Resolution Editable Talking Face Generation via Pre-trained StyleGAN**, Fei Yin et.al., Paper: [http://arxiv.org/abs/2203.04036](http://arxiv.org/abs/2203.04036), Code: **[https://github.com/FeiiYin/StyleHEAT](https://github.com/FeiiYin/StyleHEAT)**
- 2024-02-21, **StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing**, Gaoxiang Cong et.al., Paper: [http://arxiv.org/abs/2402.12636](http://arxiv.org/abs/2402.12636)
- 2023-05-01, **StyleAvatar: Real-time Photo-realistic Portrait Avatar from a Single Video**, Lizhen Wang et.al., Paper: [http://arxiv.org/abs/2305.00942](http://arxiv.org/abs/2305.00942), Code: **[https://github.com/lizhenwangt/styleavatar](https://github.com/lizhenwangt/styleavatar)**
- 2023-03-22, **Style Transfer for 2D Talking Head Animation**, Trong-Thang Pham et.al., Paper: [http://arxiv.org/abs/2303.09799](http://arxiv.org/abs/2303.09799), Code: **[https://github.com/aioz-ai/audiodrivenstyletransfer](https://github.com/aioz-ai/audiodrivenstyletransfer)**
- 2023-12-11, **Study of Non-Verbal Behavior in Conversational Agents**, Camila Vicari Maccari et.al., Paper: [http://arxiv.org/abs/2312.06530](http://arxiv.org/abs/2312.06530)
- 2021-10-07, **Streaming Transformer Transducer Based Speech Recognition Using Non-Causal Convolution**, Yangyang Shi et.al., Paper: [http://arxiv.org/abs/2110.05241](http://arxiv.org/abs/2110.05241)
- 2020-11-21, **Stochastic Talking Face Generation Using Latent Distribution Matching**, Ravindra Yadav et.al., Paper: [http://arxiv.org/abs/2011.10727](http://arxiv.org/abs/2011.10727), Code: **[https://github.com/ry85/Stochastic-Talking-Face-Generation-Using-Latent-Distribution-Matching](https://github.com/ry85/Stochastic-Talking-Face-Generation-Using-Latent-Distribution-Matching)**
- 2022-01-21, **Stitch it in Time: GAN-Based Facial Editing of Real Videos**, Rotem Tzaban et.al., Paper: [http://arxiv.org/abs/2201.08361](http://arxiv.org/abs/2201.08361), Code: **[https://github.com/rotemtzaban/STIT](https://github.com/rotemtzaban/STIT)**
- 2022-08-29, **StableFace: Analyzing and Improving Motion Stability for Talking Face Generation**, Jun Ling et.al., Paper: [http://arxiv.org/abs/2208.13717](http://arxiv.org/abs/2208.13717)
- 2021-07-10, **Speech2Video: Cross-Modal Distillation for Speech to Video Generation**, Shijing Si et.al., Paper: [http://arxiv.org/abs/2107.04806](http://arxiv.org/abs/2107.04806)
- 2023-09-09, **Speech2Lip: High-fidelity Speech to Lip Generation by Learning from a Short Video**, Xiuzhe Wu et.al., Paper: [http://arxiv.org/abs/2309.04814](http://arxiv.org/abs/2309.04814), Code: **[https://github.com/cvmi-lab/speech2lip](https://github.com/cvmi-lab/speech2lip)**
- 2020-02-19, **Speech-driven facial animation using polynomial fusion of features**, Triantafyllos Kefalas et.al., Paper: [http://arxiv.org/abs/1912.05833](http://arxiv.org/abs/1912.05833)
- 2021-11-29, **Speech Drives Templates: Co-Speech Gesture Synthesis with Learned Templates**, Shenhan Qian et.al., Paper: [http://arxiv.org/abs/2108.08020](http://arxiv.org/abs/2108.08020), Code: **[https://github.com/shenhanqian/speechdrivestemplates](https://github.com/shenhanqian/speechdrivestemplates)**
- 2021-07-21, **Speech Driven Talking Face Generation from a Single Image and an Emotion Condition**, Sefik Emre Eskimez et.al., Paper: [http://arxiv.org/abs/2008.03592](http://arxiv.org/abs/2008.03592), Code: **[https://github.com/eeskimez/emotalkingface](https://github.com/eeskimez/emotalkingface)**
- 2020-08-04, **Speaker dependent acoustic-to-articulatory inversion using real-time MRI of the vocal tract**, Tamás Gábor Csapó et.al., Paper: [http://arxiv.org/abs/2008.02098](http://arxiv.org/abs/2008.02098), Code: **[https://github.com/BME-SmartLab/speech2mri](https://github.com/BME-SmartLab/speech2mri)**
- 2020-06-20, **Speaker Independent and Multilingual/Mixlingual Speech-Driven Talking Head Generation Using Phonetic Posteriorgrams**, Huirong Huang et.al., Paper: [http://arxiv.org/abs/2006.11610](http://arxiv.org/abs/2006.11610)
- 2022-10-13, **Sparse in Space and Time: Audio-visual Synchronisation with Trainable Selectors**, Vladimir Iashin et.al., Paper: [http://arxiv.org/abs/2210.07055](http://arxiv.org/abs/2210.07055), Code: **[https://github.com/v-iashin/sparsesync](https://github.com/v-iashin/sparsesync)**
- 2021-08-06, **SofGAN: A Portrait Image Generator with Dynamic Styling**, Anpei Chen et.al., Paper: [http://arxiv.org/abs/2007.03780](http://arxiv.org/abs/2007.03780), Code: **[https://github.com/apchenstu/sofgan](https://github.com/apchenstu/sofgan)**
- 2021-04-07, **Single Source One Shot Reenactment using Weighted motion From Paired Feature Points**, Soumya Tripathy et.al., Paper: [http://arxiv.org/abs/2104.03117](http://arxiv.org/abs/2104.03117)
- 2023-12-08, **SingingHead: A Large-scale 4D Dataset for Singing Head Animation**, Sijing Wu et.al., Paper: [http://arxiv.org/abs/2312.04369](http://arxiv.org/abs/2312.04369)
- 2023-08-30, **SelfTalk: A Self-Supervised Commutative Training Diagram to Comprehend 3D Talking Faces**, Ziqiao Peng et.al., Paper: [http://arxiv.org/abs/2306.10799](http://arxiv.org/abs/2306.10799), Code: **[https://github.com/psyai-net/SelfTalk_release](https://github.com/psyai-net/SelfTalk_release)**
- 2022-01-24, **Selective Listening by Synchronizing Speech with Lips**, Zexu Pan et.al., Paper: [http://arxiv.org/abs/2106.07150](http://arxiv.org/abs/2106.07150), Code: **[https://github.com/zexupan/reentry](https://github.com/zexupan/reentry)**
- 2020-09-02, **Seeing wake words: Audio-visual Keyword Spotting**, Liliane Momeni et.al., Paper: [http://arxiv.org/abs/2009.01225](http://arxiv.org/abs/2009.01225)
- 2023-03-29, **Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert**, Jiadong Wang et.al., Paper: [http://arxiv.org/abs/2303.17480](http://arxiv.org/abs/2303.17480), Code: **[https://github.com/sxjdwang/talklip](https://github.com/sxjdwang/talklip)**
- 2023-03-13, **SadTalker: Learning Realistic 3D Motion Coefficients for Stylized Audio-Driven Single Image Talking Face Animation**, Wenxuan Zhang et.al., Paper: [http://arxiv.org/abs/2211.12194](http://arxiv.org/abs/2211.12194), Code: **[https://github.com/winfredy/sadtalker](https://github.com/winfredy/sadtalker)**
- 2022-12-07, **SPACE: Speech-driven Portrait Animation with Controllable Expression**, Siddharth Gururani et.al., Paper: [http://arxiv.org/abs/2211.09809](http://arxiv.org/abs/2211.09809)
- 2020-10-05, **SMILE: Semantically-guided Multi-attribute Image and Layout Editing**, Andrés Romero et.al., Paper: [http://arxiv.org/abs/2010.02315](http://arxiv.org/abs/2010.02315), Code: **[https://github.com/affromero/SMILE](https://github.com/affromero/SMILE)**
- 2024-01-25, **SAiD: Speech-driven Blendshape Facial Animation with Diffusion**, Inkyu Park et.al., Paper: [http://arxiv.org/abs/2401.08655](http://arxiv.org/abs/2401.08655), Code: **[https://github.com/yunik1004/said](https://github.com/yunik1004/said)**
- 2023-07-03, **RobustL2S: Speaker-Specific Lip-to-Speech Synthesis exploiting Self-Supervised Representations**, Neha Sahipjohn et.al., Paper: [http://arxiv.org/abs/2307.01233](http://arxiv.org/abs/2307.01233)
- 2020-12-14, **Robust One Shot Audio to Video Generation**, Neeraj Kumar et.al., Paper: [http://arxiv.org/abs/2012.07842](http://arxiv.org/abs/2012.07842)
- 2022-09-07, **Restructurable Activation Networks**, Kartikeya Bhardwaj et.al., Paper: [http://arxiv.org/abs/2208.08562](http://arxiv.org/abs/2208.08562), Code: **[https://github.com/arm-software/ml-restructurable-activation-networks](https://github.com/arm-software/ml-restructurable-activation-networks)**
- 2022-07-20, **Responsive Listening Head Generation: A Benchmark Dataset and Baseline**, Mohan Zhou et.al., Paper: [http://arxiv.org/abs/2112.13548](http://arxiv.org/abs/2112.13548)
- 2024-02-26, **Resolution-Agnostic Neural Compression for High-Fidelity Portrait Video Conferencing via Implicit Radiance Fields**, Yifei Li et.al., Paper: [http://arxiv.org/abs/2402.16599](http://arxiv.org/abs/2402.16599)
- 2023-05-22, **RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars**, Dongwei Pan et.al., Paper: [http://arxiv.org/abs/2305.13353](http://arxiv.org/abs/2305.13353), Code: **[https://github.com/renderme-360/renderme-360](https://github.com/renderme-360/renderme-360)**
- 2023-06-08, **ReliableSwap: Boosting General Face Swapping Via Reliable Supervision**, Ge Yuan et.al., Paper: [http://arxiv.org/abs/2306.05356](http://arxiv.org/abs/2306.05356), Code: **[https://github.com/ygtxr1997/reliableswap](https://github.com/ygtxr1997/reliableswap)**
- 2018-07-29, **ReenactGAN: Learning to Reenact Faces via Boundary Transfer**, Wayne Wu et.al., Paper: [http://arxiv.org/abs/1807.11079](http://arxiv.org/abs/1807.11079), Code: **[https://github.com/wywu/ReenactGAN](https://github.com/wywu/ReenactGAN)**
- 2019-06-14, **Realistic Speech-Driven Facial Animation with GANs**, Konstantinos Vougioukas et.al., Paper: [http://arxiv.org/abs/1906.06337](http://arxiv.org/abs/1906.06337)
- 2020-03-29, **Realistic Face Reenactment via Self-Supervised Disentangling of Identity and Pose**, Xianfang Zeng et.al., Paper: [http://arxiv.org/abs/2003.12957](http://arxiv.org/abs/2003.12957)
- 2024-01-20, **Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis**, Zhenhui Ye et.al., Paper: [http://arxiv.org/abs/2401.08503](http://arxiv.org/abs/2401.08503)
- 2021-03-05, **Real-time RGBD-based Extended Body Pose Estimation**, Renat Bashirov et.al., Paper: [http://arxiv.org/abs/2103.03663](http://arxiv.org/abs/2103.03663), Code: **[https://github.com/rmbashirov/rgbd-kinect-pose](https://github.com/rmbashirov/rgbd-kinect-pose)**
- 2022-11-22, **Real-time Neural Radiance Talking Portrait Synthesis via Audio-spatial Decomposition**, Jiaxiang Tang et.al., Paper: [http://arxiv.org/abs/2211.12368](http://arxiv.org/abs/2211.12368)
- 2019-10-19, **Real-Time Lip Sync for Live 2D Animation**, Deepali Aneja et.al., Paper: [http://arxiv.org/abs/1910.08685](http://arxiv.org/abs/1910.08685), Code: **[https://github.com/deepalianeja/CharacterLipSync2D](https://github.com/deepalianeja/CharacterLipSync2D)**
- 2020-08-04, **Real-Time Cleaning and Refinement of Facial Animation Signals**, Eloïse Berson et.al., Paper: [http://arxiv.org/abs/2008.01332](http://arxiv.org/abs/2008.01332)
- 2023-11-06, **RADIO: Reference-Agnostic Dubbing Video Synthesis**, Dongyeun Lee et.al., Paper: [http://arxiv.org/abs/2309.01950](http://arxiv.org/abs/2309.01950)
- 2023-12-09, **R2-Talker: Realistic Real-Time Talking Head Synthesis with Hash Grid Landmarks Encoding and Progressive Multilayer Conditioning**, Zhiling Ye et.al., Paper: [http://arxiv.org/abs/2312.05572](http://arxiv.org/abs/2312.05572)
- 2019-08-20, **Prosodic Phrase Alignment for Machine Dubbing**, Alp Öktem et.al., Paper: [http://arxiv.org/abs/1908.07226](http://arxiv.org/abs/1908.07226), Code: **[https://github.com/alpoktem/MachineDub](https://github.com/alpoktem/MachineDub)**
- 2022-11-26, **Progressive Disentangled Representation Learning for Fine-Grained Controllable Talking Head Synthesis**, Duomin Wang et.al., Paper: [http://arxiv.org/abs/2211.14506](http://arxiv.org/abs/2211.14506), Code: **[https://github.com/Dorniwang/PD-FGC-inference](https://github.com/Dorniwang/PD-FGC-inference)**
- 2023-07-09, **Predictive Coding For Animation-Based Video Compression**, Goluck Konuko et.al., Paper: [http://arxiv.org/abs/2307.04187](http://arxiv.org/abs/2307.04187)
- 2022-10-13, **Pre-Avatar: An Automatic Presentation Generation Framework Leveraging Talking Avatar**, Aolan Sun et.al., Paper: [http://arxiv.org/abs/2210.06877](http://arxiv.org/abs/2210.06877)
- 2021-04-22, **Pose-Controllable Talking Face Generation by Implicitly Modularized Audio-Visual Representation**, Hang Zhou et.al., Paper: [http://arxiv.org/abs/2104.11116](http://arxiv.org/abs/2104.11116), Code: **[https://github.com/Hangz-nju-cuhk/Talking-Face_PC-AVS](https://github.com/Hangz-nju-cuhk/Talking-Face_PC-AVS)**
- 2023-02-24, **Pose-Controllable 3D Facial Animation Synthesis using Hierarchical Audio-Vertex Attention**, Bin Liu et.al., Paper: [http://arxiv.org/abs/2302.12532](http://arxiv.org/abs/2302.12532)
- 2023-10-25, **Personalized Speech-driven Expressive 3D Facial Animation Synthesis with Style Control**, Elif Bozkurt et.al., Paper: [http://arxiv.org/abs/2310.17011](http://arxiv.org/abs/2310.17011)
- 2022-08-02, **Perceptual Conversational Head Generation with Regularized Driver and Enhanced Renderer**, Ailin Huang et.al., Paper: [http://arxiv.org/abs/2206.12837](http://arxiv.org/abs/2206.12837), Code: **[https://github.com/megvii-research/MM2022-ViCoPerceptualHeadGeneration](https://github.com/megvii-research/MM2022-ViCoPerceptualHeadGeneration)**
- 2023-06-13, **Parametric Implicit Face Representation for Audio-Driven Facial Reenactment**, Ricong Huang et.al., Paper: [http://arxiv.org/abs/2306.07579](http://arxiv.org/abs/2306.07579)
- 2021-12-20, **Parallel and High-Fidelity Text-to-Lip Generation**, Jinglin Liu et.al., Paper: [http://arxiv.org/abs/2107.06831](http://arxiv.org/abs/2107.06831), Code: **[https://github.com/Dianezzy/ParaLip](https://github.com/Dianezzy/ParaLip)**
- 2023-08-29, **Papeos: Augmenting Research Papers with Talk Videos**, Tae Soo Kim et.al., Paper: [http://arxiv.org/abs/2308.15224](http://arxiv.org/abs/2308.15224)
- 2023-03-23, **PanoHead: Geometry-Aware 3D Full-Head Synthesis in 360 $^{\circ}$**, Sizhe An et.al., Paper: [http://arxiv.org/abs/2303.13071](http://arxiv.org/abs/2303.13071)
- 2023-12-05, **PMMTalk: Speech-Driven 3D Facial Animation from Complementary Pseudo Multi-modal Features**, Tianshun Han et.al., Paper: [http://arxiv.org/abs/2312.02781](http://arxiv.org/abs/2312.02781)
- 2023-09-13, **PIAVE: A Pose-Invariant Audio-Visual Speaker Extraction Network**, Qinghua Liu et.al., Paper: [http://arxiv.org/abs/2309.06723](http://arxiv.org/abs/2309.06723)
- 2021-12-06, **One-shot Talking Face Generation from Single-speaker Audio-Visual Correlation Learning**, Suzhen Wang et.al., Paper: [http://arxiv.org/abs/2112.02749](http://arxiv.org/abs/2112.02749)
- 2024-02-05, **One-shot Neural Face Reenactment via Finding Directions in GAN's Latent Space**, Stella Bounareli et.al., Paper: [http://arxiv.org/abs/2402.03553](http://arxiv.org/abs/2402.03553)
- 2021-04-26, **One-shot Face Reenactment Using Appearance Adaptive Normalization**, Guangming Yao et.al., Paper: [http://arxiv.org/abs/2102.03984](http://arxiv.org/abs/2102.03984)
- 2019-08-05, **One-shot Face Reenactment**, Yunxuan Zhang et.al., Paper: [http://arxiv.org/abs/1908.03251](http://arxiv.org/abs/1908.03251), Code: **[https://github.com/bj80heyue/Learning_One_Shot_Face_Reenactment](https://github.com/bj80heyue/Learning_One_Shot_Face_Reenactment)**
- 2023-04-11, **One-Shot High-Fidelity Talking-Head Synthesis with Deformable Neural Radiance Field**, Weichuang Li et.al., Paper: [http://arxiv.org/abs/2304.05097](http://arxiv.org/abs/2304.05097)
- 2021-04-02, **One-Shot Free-View Neural Talking-Head Synthesis for Video Conferencing**, Ting-Chun Wang et.al., Paper: [http://arxiv.org/abs/2011.15126](http://arxiv.org/abs/2011.15126)
- 2022-05-26, **One-Shot Face Reenactment on Megapixels**, Wonjun Kang et.al., Paper: [http://arxiv.org/abs/2205.13368](http://arxiv.org/abs/2205.13368)
- 2021-02-19, **One Shot Audio to Animated Video Generation**, Neeraj Kumar et.al., Paper: [http://arxiv.org/abs/2102.09737](http://arxiv.org/abs/2102.09737)
- 2022-11-10, **On the role of Lip Articulation in Visual Speech Perception**, Zakaria Aldeneh et.al., Paper: [http://arxiv.org/abs/2203.10117](http://arxiv.org/abs/2203.10117)
- 2023-10-29, **On the Vulnerability of DeepFake Detectors to Attacks Generated by Denoising Diffusion Models**, Marija Ivanovska et.al., Paper: [http://arxiv.org/abs/2307.05397](http://arxiv.org/abs/2307.05397)
- 2023-03-27, **OmniAvatar: Geometry-Guided Controllable 3D Head Synthesis**, Hongyi Xu et.al., Paper: [http://arxiv.org/abs/2303.15539](http://arxiv.org/abs/2303.15539)
- 2023-03-26, **OTAvatar: One-shot Talking Face Avatar with Controllable Tri-plane Rendering**, Zhiyuan Ma et.al., Paper: [http://arxiv.org/abs/2303.14662](http://arxiv.org/abs/2303.14662), Code: **[https://github.com/theericma/otavatar](https://github.com/theericma/otavatar)**
- 2023-09-28, **OSM-Net: One-to-Many One-shot Talking Head Generation with Spontaneous Head Motions**, Jin Liu et.al., Paper: [http://arxiv.org/abs/2309.16148](http://arxiv.org/abs/2309.16148)
- 2023-02-16, **OPT: One-shot Pose-Controllable Talking Head Generation**, Jin Liu et.al., Paper: [http://arxiv.org/abs/2302.08197](http://arxiv.org/abs/2302.08197)
- 2023-07-19, **OPHAvatars: One-shot Photo-realistic Head Avatars**, Shaoxu Li et.al., Paper: [http://arxiv.org/abs/2307.09153](http://arxiv.org/abs/2307.09153), Code: **[https://github.com/lsx0101/ophavatars](https://github.com/lsx0101/ophavatars)**
- 2021-03-20, **Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and Localization**, Komal Chugh et.al., Paper: [http://arxiv.org/abs/2005.14405](http://arxiv.org/abs/2005.14405), Code: **[https://github.com/abhinavdhall/deepfake](https://github.com/abhinavdhall/deepfake)**
- 2021-09-28, **NimbRo Avatar: Interactive Immersive Telepresence with Force-Feedback Telemanipulation**, Max Schwarz et.al., Paper: [http://arxiv.org/abs/2109.13772](http://arxiv.org/abs/2109.13772)
- 2023-01-20, **Neural Volumetric Blendshapes: Computationally Efficient Physics-Based Facial Blendshapes**, Nicolas Wagner et.al., Paper: [http://arxiv.org/abs/2212.14784](http://arxiv.org/abs/2212.14784)
- 2020-07-29, **Neural Voice Puppetry: Audio-driven Facial Reenactment**, Justus Thies et.al., Paper: [http://arxiv.org/abs/1912.05566](http://arxiv.org/abs/1912.05566), Code: **[https://github.com/miu200521358/NeuralVoicePuppetryMMD](https://github.com/miu200521358/NeuralVoicePuppetryMMD)**
- 2023-12-11, **Neural Text to Articulate Talk: Deep Text to Audiovisual Speech Synthesis achieving both Auditory and Photo-realism**, Georgios Milis et.al., Paper: [http://arxiv.org/abs/2312.06613](http://arxiv.org/abs/2312.06613), Code: **[https://github.com/g-milis/NEUTART](https://github.com/g-milis/NEUTART)**
- 2019-09-06, **Neural Style-Preserving Visual Dubbing**, Hyeongwoo Kim et.al., Paper: [http://arxiv.org/abs/1909.02518](http://arxiv.org/abs/1909.02518)
- 2023-08-10, **Near-realtime Facial Animation by Deep 3D Simulation Super-Resolution**, Hyojoon Park et.al., Paper: [http://arxiv.org/abs/2305.03216](http://arxiv.org/abs/2305.03216)
- 2024-01-23, **NeRF-AD: Neural Radiance Field with Attention-based Disentanglement for Talking Face Synthesis**, Chongke Bi et.al., Paper: [http://arxiv.org/abs/2401.12568](http://arxiv.org/abs/2401.12568)
- 2023-06-12, **NPVForensics: Jointing Non-critical Phonemes and Visemes for Deepfake Detection**, Yu Chen et.al., Paper: [http://arxiv.org/abs/2306.06885](http://arxiv.org/abs/2306.06885)
- 2022-07-20, **NARRATE: A Normal Assisted Free-View Portrait Stylizer**, Youjia Wang et.al., Paper: [http://arxiv.org/abs/2207.00974](http://arxiv.org/abs/2207.00974)
- 2023-12-05, **MyPortrait: Morphable Prior-Guided Personalized Portrait Generation**, Bo Ding et.al., Paper: [http://arxiv.org/abs/2312.02703](http://arxiv.org/abs/2312.02703)
- 2023-05-09, **Multimodal-driven Talking Face Generation via a Unified Diffusion-based Generator**, Chao Xu et.al., Paper: [http://arxiv.org/abs/2305.02594](http://arxiv.org/abs/2305.02594)
- 2022-03-04, **Multi-modality Deep Restoration of Extremely Compressed Face Videos**, Xi Zhang et.al., Paper: [http://arxiv.org/abs/2107.05548](http://arxiv.org/abs/2107.05548)
- 2020-12-14, **Multi Modal Adaptive Normalization for Audio to Video Generation**, Neeraj Kumar et.al., Paper: [http://arxiv.org/abs/2012.07304](http://arxiv.org/abs/2012.07304)
- 2020-05-27, **Modality Dropout for Improved Performance-driven Talking Faces**, Ahmed Hussen Abdelaziz et.al., Paper: [http://arxiv.org/abs/2005.13616](http://arxiv.org/abs/2005.13616)
- 2023-12-18, **Mimic: Speaking Style Disentanglement for Speech-Driven 3D Facial Animation**, Hui Fu et.al., Paper: [http://arxiv.org/abs/2312.10877](http://arxiv.org/abs/2312.10877)
- 2023-03-27, **MetaPortrait: Identity-Preserving Talking Head Generation with Fast Personalized Adaptation**, Bowen Zhang et.al., Paper: [http://arxiv.org/abs/2212.08062](http://arxiv.org/abs/2212.08062), Code: **[https://github.com/Meta-Portrait/MetaPortrait](https://github.com/Meta-Portrait/MetaPortrait)**
- 2022-05-20, **MeshTalk: 3D Face Animation from Speech using Cross-Modality Disentanglement**, Alexander Richard et.al., Paper: [http://arxiv.org/abs/2104.08223](http://arxiv.org/abs/2104.08223), Code: **[https://github.com/facebookresearch/meshtalk](https://github.com/facebookresearch/meshtalk)**
- 2020-09-18, **Mesh Guided One-shot Face Reenactment using Graph Convolutional Networks**, Guangming Yao et.al., Paper: [http://arxiv.org/abs/2008.07783](http://arxiv.org/abs/2008.07783)
- 2022-05-24, **Merkel Podcast Corpus: A Multimodal Dataset Compiled from 16 Years of Angela Merkel's Weekly Video Podcasts**, Debjoy Saha et.al., Paper: [http://arxiv.org/abs/2205.12194](http://arxiv.org/abs/2205.12194), Code: **[https://github.com/deeplsd/merkel-podcast-corpus](https://github.com/deeplsd/merkel-podcast-corpus)**
- 2023-11-20, **MemoryCompanion: A Smart Healthcare Solution to Empower Efficient Alzheimer's Care Via Unleashing Generative AI**, Lifei Zheng et.al., Paper: [http://arxiv.org/abs/2311.14730](http://arxiv.org/abs/2311.14730)
- 2023-02-27, **Memory-augmented Contrastive Learning for Talking Head Generation**, Jianrong Wang et.al., Paper: [http://arxiv.org/abs/2302.13469](http://arxiv.org/abs/2302.13469), Code: **[https://github.com/yaxinzhao97/macl](https://github.com/yaxinzhao97/macl)**
- 2024-03-05, **Memories are One-to-Many Mapping Alleviators in Talking Face Generation**, Anni Tang et.al., Paper: [http://arxiv.org/abs/2212.05005](http://arxiv.org/abs/2212.05005)
- 2024-01-30, **Media2Face: Co-speech Facial Animation Generation With Multi-Modality Guidance**, Qingcheng Zhao et.al., Paper: [http://arxiv.org/abs/2401.15687](http://arxiv.org/abs/2401.15687)
- 2022-12-09, **Masked Lip-Sync Prediction by Audio-Visual Contextual Exploitation in Transformers**, Yasheng Sun et.al., Paper: [http://arxiv.org/abs/2212.04970](http://arxiv.org/abs/2212.04970)
- 2023-09-10, **MaskRenderer: 3D-Infused Multi-Mask Realistic Face Reenactment**, Tina Behrouzi et.al., Paper: [http://arxiv.org/abs/2309.05095](http://arxiv.org/abs/2309.05095)
- 2019-11-19, **MarioNETte: Few-shot Face Reenactment Preserving Identity of Unseen Targets**, Sungjoo Ha et.al., Paper: [http://arxiv.org/abs/1911.08139](http://arxiv.org/abs/1911.08139)
- 2021-02-25, **MakeItTalk: Speaker-Aware Talking-Head Animation**, Yang Zhou et.al., Paper: [http://arxiv.org/abs/2004.12992](http://arxiv.org/abs/2004.12992)
- 2023-07-19, **MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions**, Yunfei Liu et.al., Paper: [http://arxiv.org/abs/2307.10008](http://arxiv.org/abs/2307.10008)
- 2023-12-13, **MMFace4D: A Large-Scale Multi-Modal 4D Face Dataset for Audio-Driven 3D Face Animation**, Haozhe Wu et.al., Paper: [http://arxiv.org/abs/2303.09797](http://arxiv.org/abs/2303.09797)
- 2024-01-31, **MM-TTS: Multi-modal Prompt based Style Transfer for Expressive Text-to-Speech Synthesis**, Wenhao Guan et.al., Paper: [http://arxiv.org/abs/2312.10687](http://arxiv.org/abs/2312.10687)
- 2023-03-22, **MARLIN: Masked Autoencoder for facial video Representation LearnINg**, Zhixi Cai et.al., Paper: [http://arxiv.org/abs/2211.06627](http://arxiv.org/abs/2211.06627), Code: **[https://github.com/ControlNet/MARLIN](https://github.com/ControlNet/MARLIN)**
- 2021-09-24, **Live Speech Portraits: Real-Time Photorealistic Talking-Head Animation**, Yuanxun Lu et.al., Paper: [http://arxiv.org/abs/2109.10595](http://arxiv.org/abs/2109.10595)
- 2024-01-28, **Lips Are Lying: Spotting the Temporal Inconsistency between Audio and Visual in Lip-Syncing DeepFakes**, Weifeng Liu et.al., Paper: [http://arxiv.org/abs/2401.15668](http://arxiv.org/abs/2401.15668), Code: **[https://github.com/aaroncomo/lipfd](https://github.com/aaroncomo/lipfd)**
- 2021-06-08, **LipSync3D: Data-Efficient Learning of Personalized 3D Talking Faces from Video using Pose and Lighting Normalization**, Avisek Lahiri et.al., Paper: [http://arxiv.org/abs/2106.04185](http://arxiv.org/abs/2106.04185)
- 2022-10-21, **Leveraging Real Talking Faces via Self-Supervision for Robust Forgery Detection**, Alexandros Haliassos et.al., Paper: [http://arxiv.org/abs/2201.07131](http://arxiv.org/abs/2201.07131), Code: **[https://github.com/ahaliassos/RealForensics](https://github.com/ahaliassos/RealForensics)**
- 2024-02-29, **Learning a Generalized Physical Face Model From Data**, Lingchen Yang et.al., Paper: [http://arxiv.org/abs/2402.19477](http://arxiv.org/abs/2402.19477)
- 2020-07-08, **Learning Speech Representations from Raw Audio by Joint Audiovisual Self-Supervision**, Abhinav Shukla et.al., Paper: [http://arxiv.org/abs/2007.04134](http://arxiv.org/abs/2007.04134)
- 2023-11-03, **Learning Separable Hidden Unit Contributions for Speaker-Adaptive Lip-Reading**, Songtao Luo et.al., Paper: [http://arxiv.org/abs/2310.05058](http://arxiv.org/abs/2310.05058), Code: **[https://github.com/jinchiniao/LSHUC](https://github.com/jinchiniao/LSHUC)**
- 2023-11-30, **Learning One-Shot 4D Head Avatar Synthesis using Synthetic Data**, Yu Deng et.al., Paper: [http://arxiv.org/abs/2311.18729](http://arxiv.org/abs/2311.18729)
- 2023-07-26, **Learning Landmarks Motion from Speech for Speaker-Agnostic 3D Talking Heads Generation**, Federico Nocentini et.al., Paper: [http://arxiv.org/abs/2306.01415](http://arxiv.org/abs/2306.01415), Code: **[https://github.com/fedenoce/s2l-s2d](https://github.com/fedenoce/s2l-s2d)**
- 2024-02-27, **Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis**, Zicheng Zhang et.al., Paper: [http://arxiv.org/abs/2402.17364](http://arxiv.org/abs/2402.17364), Code: **[https://github.com/zhangzc21/dyntet](https://github.com/zhangzc21/dyntet)**
- 2022-07-24, **Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head Synthesis**, Shuai Shen et.al., Paper: [http://arxiv.org/abs/2207.11770](http://arxiv.org/abs/2207.11770), Code: **[https://github.com/sstzal/DFRF](https://github.com/sstzal/DFRF)**
- 2023-12-19, **Learning Dense Correspondence for NeRF-Based Face Reenactment**, Songlin Yang et.al., Paper: [http://arxiv.org/abs/2312.10422](http://arxiv.org/abs/2312.10422)
- 2023-01-15, **Learning Audio-Driven Viseme Dynamics for 3D Face Animation**, Linchao Bao et.al., Paper: [http://arxiv.org/abs/2301.06059](http://arxiv.org/abs/2301.06059)
- 2021-04-29, **Learned Spatial Representations for Few-shot Talking-Head Synthesis**, Moustafa Meshry et.al., Paper: [http://arxiv.org/abs/2104.14557](http://arxiv.org/abs/2104.14557)
- 2018-07-26, **Learnable PINs: Cross-Modal Embeddings for Person Identity**, Arsha Nagrani et.al., Paper: [http://arxiv.org/abs/1805.00833](http://arxiv.org/abs/1805.00833)
- 2023-08-30, **Laughing Matters: Introducing Laughing-Face Generation using Diffusion Models**, Antoni Bigata Casademunt et.al., Paper: [http://arxiv.org/abs/2305.08854](http://arxiv.org/abs/2305.08854), Code: **[https://github.com/antonibigata/Laughing-Matters](https://github.com/antonibigata/Laughing-Matters)**
- 2023-11-02, **LaughTalk: Expressive 3D Talking Head Generation with Laughter**, Kim Sung-Bin et.al., Paper: [http://arxiv.org/abs/2311.00994](http://arxiv.org/abs/2311.00994)
- 2020-11-06, **Large-scale multilingual audio visual dubbing**, Yi Yang et.al., Paper: [http://arxiv.org/abs/2011.03530](http://arxiv.org/abs/2011.03530)
- 2023-05-17, **LPMM: Intuitive Pose Control for Neural Talking-Head Model via Landmark-Parameter Morphable Model**, Kwangho Lee et.al., Paper: [http://arxiv.org/abs/2305.10456](http://arxiv.org/abs/2305.10456)
- 2021-04-07, **LI-Net: Large-Pose Identity-Preserving Face Reenactment Network**, Jin Liu et.al., Paper: [http://arxiv.org/abs/2104.02850](http://arxiv.org/abs/2104.02850)
- 2021-08-23, **KoDF: A Large-scale Korean DeepFake Detection Dataset**, Patrick Kwon et.al., Paper: [http://arxiv.org/abs/2103.10094](http://arxiv.org/abs/2103.10094)
- 2024-01-11, **Jump Cut Smoothing for Talking Heads**, Xiaojuan Wang et.al., Paper: [http://arxiv.org/abs/2401.04718](http://arxiv.org/abs/2401.04718)
- 2021-12-07, **Joint Audio-Text Model for Expressive Speech-Driven 3D Facial Animation**, Yingruo Fan et.al., Paper: [http://arxiv.org/abs/2112.02214](http://arxiv.org/abs/2112.02214)
- 2020-11-21, **Iterative Text-based Editing of Talking-heads Using Neural Retargeting**, Xinwei Yao et.al., Paper: [http://arxiv.org/abs/2011.10688](http://arxiv.org/abs/2011.10688)
- 2021-10-22, **Invertible Frowns: Video-to-Video Facial Emotion Translation**, Ian Magnusson et.al., Paper: [http://arxiv.org/abs/2109.08061](http://arxiv.org/abs/2109.08061)
- 2020-10-12, **Intuitive Facial Animation Editing Based On A Generative RNN Framework**, Eloïse Berson et.al., Paper: [http://arxiv.org/abs/2010.05655](http://arxiv.org/abs/2010.05655)
- 2023-07-05, **Interactive Conversational Head Generation**, Mohan Zhou et.al., Paper: [http://arxiv.org/abs/2307.02090](http://arxiv.org/abs/2307.02090)
- 2021-10-16, **Intelligent Video Editing: Incorporating Modern Talking Face Generation Algorithms in a Video Editor**, Anchit Gupta et.al., Paper: [http://arxiv.org/abs/2110.08580](http://arxiv.org/abs/2110.08580)
- 2023-06-05, **Instruct-Video2Avatar: Video-to-Avatar Generation with Instructions**, Shaoxu Li et.al., Paper: [http://arxiv.org/abs/2306.02903](http://arxiv.org/abs/2306.02903), Code: **[https://github.com/lsx0101/instruct-video2avatar](https://github.com/lsx0101/instruct-video2avatar)**
- 2023-08-16, **Instruct-NeuralTalker: Editing Audio-Driven Talking Radiance Fields with Instructions**, Yuqi Sun et.al., Paper: [http://arxiv.org/abs/2306.10813](http://arxiv.org/abs/2306.10813)
- 2021-12-19, **Initiative Defense against Facial Manipulation**, Qidong Huang et.al., Paper: [http://arxiv.org/abs/2112.10098](http://arxiv.org/abs/2112.10098), Code: **[https://github.com/shikiw/initiative-defense-for-deepfake](https://github.com/shikiw/initiative-defense-for-deepfake)**
- 2018-11-16, **Influence of visual cues on head and eye movements during listening tasks in multi-talker audiovisual environments with animated characters**, Maartje M. E. Hendrikse et.al., Paper: [http://arxiv.org/abs/1812.02088](http://arxiv.org/abs/1812.02088)
- 2023-03-09, **Improving Few-Shot Learning for Talking Face System with TTS Data Augmentation**, Qi Chen et.al., Paper: [http://arxiv.org/abs/2303.05322](http://arxiv.org/abs/2303.05322), Code: **[https://github.com/moon0316/t2a](https://github.com/moon0316/t2a)**
- 2024-01-26, **Implicit Neural Representation for Physics-driven Actuated Soft Bodies**, Lingchen Yang et.al., Paper: [http://arxiv.org/abs/2401.14861](http://arxiv.org/abs/2401.14861)
- 2023-04-21, **Implicit Neural Head Synthesis via Controllable Local Deformation Fields**, Chuhan Chen et.al., Paper: [http://arxiv.org/abs/2304.11113](http://arxiv.org/abs/2304.11113)
- 2023-08-18, **Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation**, Fa-Ting Hong et.al., Paper: [http://arxiv.org/abs/2307.09906](http://arxiv.org/abs/2307.09906), Code: **[https://github.com/harlanhong/iccv2023-mcnet](https://github.com/harlanhong/iccv2023-mcnet)**
- 2022-12-30, **Imitator: Personalized Speech-driven 3D Facial Animation**, Balamurugan Thambiraja et.al., Paper: [http://arxiv.org/abs/2301.00023](http://arxiv.org/abs/2301.00023)
- 2021-10-30, **Imitating Arbitrary Talking Style for Realistic Audio-DrivenTalking Face Synthesis**, Haozhe Wu et.al., Paper: [http://arxiv.org/abs/2111.00203](http://arxiv.org/abs/2111.00203), Code: **[https://github.com/wuhaozhe/style_avatar](https://github.com/wuhaozhe/style_avatar)**
- 2023-05-15, **Identity-Preserving Talking Face Generation with Landmark and Appearance Priors**, Weizhi Zhong et.al., Paper: [http://arxiv.org/abs/2305.08293](http://arxiv.org/abs/2305.08293), Code: **[https://github.com/Weizhi-Zhong/IP_LAP](https://github.com/Weizhi-Zhong/IP_LAP)**
- 2020-05-25, **Identity-Preserving Realistic Talking Face Generation**, Sanjana Sinha et.al., Paper: [http://arxiv.org/abs/2005.12318](http://arxiv.org/abs/2005.12318)
- 2023-05-17, **INCLG: Inpainting for Non-Cleft Lip Generation with a Multi-Task Image Processing Network**, Shuang Chen et.al., Paper: [http://arxiv.org/abs/2305.10589](http://arxiv.org/abs/2305.10589)
- 2020-01-17, **ICface: Interpretable and Controllable Face Reenactment Using GANs**, Soumya Tripathy et.al., Paper: [http://arxiv.org/abs/1904.01909](http://arxiv.org/abs/1904.01909)
- 2023-07-20, **HyperReenact: One-Shot Reenactment via Jointly Learning to Refine and Retarget Faces**, Stella Bounareli et.al., Paper: [http://arxiv.org/abs/2307.10797](http://arxiv.org/abs/2307.10797), Code: **[https://github.com/stelabou/hyperreenact](https://github.com/stelabou/hyperreenact)**
- 2023-10-15, **HyperLips: Hyper Control Lips with High Resolution Decoder for Talking Face Generation**, Yaosen Chen et.al., Paper: [http://arxiv.org/abs/2310.05720](http://arxiv.org/abs/2310.05720), Code: **[https://github.com/semchan/HyperLips](https://github.com/semchan/HyperLips)**
- 2023-05-31, **High-fidelity Generalized Emotional Talking Face Generation with Multi-modal Emotion Space Learning**, Chao Xu et.al., Paper: [http://arxiv.org/abs/2305.02572](http://arxiv.org/abs/2305.02572)
- 2023-03-04, **High-fidelity Facial Avatar Reconstruction from Monocular Video with Generative Priors**, Yunpeng Bai et.al., Paper: [http://arxiv.org/abs/2211.15064](http://arxiv.org/abs/2211.15064)
- 2023-11-02, **High-Fidelity and Freely Controllable Talking Head Video Generation**, Yue Gao et.al., Paper: [http://arxiv.org/abs/2304.10168](http://arxiv.org/abs/2304.10168)
- 2020-03-26, **High-Accuracy Facial Depth Models derived from 3D Synthetic Data**, Faisal Khan et.al., Paper: [http://arxiv.org/abs/2003.06211](http://arxiv.org/abs/2003.06211)
- 2023-07-19, **Hierarchical Semantic Perceptual Listener Head Video Generation: A High-performance Pipeline**, Zhigang Chang et.al., Paper: [http://arxiv.org/abs/2307.09821](http://arxiv.org/abs/2307.09821)
- 2019-05-09, **Hierarchical Cross-Modal Talking Face Generationwith Dynamic Pixel-Wise Loss**, Lele Chen et.al., Paper: [http://arxiv.org/abs/1905.03820](http://arxiv.org/abs/1905.03820), Code: **[https://github.com/lelechen63/ATVGnet](https://github.com/lelechen63/ATVGnet)**
- 2021-08-23, **HeadGAN: One-shot Neural Head Synthesis and Editing**, Michail Christos Doukas et.al., Paper: [http://arxiv.org/abs/2012.08261](http://arxiv.org/abs/2012.08261)
- 2020-05-22, **Head2Head: Video-based Neural Head Synthesis**, Mohammad Rami Koujan et.al., Paper: [http://arxiv.org/abs/2005.10954](http://arxiv.org/abs/2005.10954)
- 2023-09-14, **HDTR-Net: A Real-Time High-Definition Teeth Restoration Network for Arbitrary Talking Face Generation Methods**, Yongyuan Li et.al., Paper: [http://arxiv.org/abs/2309.07495](http://arxiv.org/abs/2309.07495), Code: **[https://github.com/yylgoodlucky/hdtr](https://github.com/yylgoodlucky/hdtr)**
- 2023-11-29, **Gotcha: Real-Time Video Deepfake Detection via Challenge-Response**, Govind Mittal et.al., Paper: [http://arxiv.org/abs/2210.06186](http://arxiv.org/abs/2210.06186)
- 2023-10-08, **GestSync: Determining who is speaking without a talking head**, Sindhu B Hegde et.al., Paper: [http://arxiv.org/abs/2310.05304](http://arxiv.org/abs/2310.05304), Code: **[https://github.com/Sindhu-Hegde/gestsync](https://github.com/Sindhu-Hegde/gestsync)**
- 2020-08-23, **Geometry-guided Dense Perspective Network for Speech-Driven Facial Animation**, Jingying Liu et.al., Paper: [http://arxiv.org/abs/2008.10004](http://arxiv.org/abs/2008.10004)
- 2018-04-23, **Generating Talking Face Landmarks from Speech**, Sefik Emre Eskimez et.al., Paper: [http://arxiv.org/abs/1803.09803](http://arxiv.org/abs/1803.09803)
- 2023-07-04, **Generating Animatable 3D Cartoon Faces from Single Portraits**, Chuanyu Pan et.al., Paper: [http://arxiv.org/abs/2307.01468](http://arxiv.org/abs/2307.01468)
- 2023-01-31, **GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face Synthesis**, Zhenhui Ye et.al., Paper: [http://arxiv.org/abs/2301.13430](http://arxiv.org/abs/2301.13430)
- 2023-05-01, **GeneFace++: Generalized and Stable Real-Time Audio-Driven 3D Talking Face Generation**, Zhenhui Ye et.al., Paper: [http://arxiv.org/abs/2305.00787](http://arxiv.org/abs/2305.00787)
- 2023-10-19, **Gemino: Practical and Robust Neural Compression for Video Conferencing**, Vibhaalakshmi Sivaraman et.al., Paper: [http://arxiv.org/abs/2209.10507](http://arxiv.org/abs/2209.10507)
- 2023-12-19, **Gaussian3Diff: 3D Gaussian Diffusion for 3D Full Head Synthesis and Editing**, Yushi Lan et.al., Paper: [http://arxiv.org/abs/2312.03763](http://arxiv.org/abs/2312.03763)
- 2023-12-12, **GSmoothFace: Generalized Smooth Talking Face Generation via Fine Grained 3D Face Guidance**, Haiming Zhang et.al., Paper: [http://arxiv.org/abs/2312.07385](http://arxiv.org/abs/2312.07385)
- 2023-12-12, **GMTalker: Gaussian Mixture based Emotional talking video Portraits**, Yibo Xia et.al., Paper: [http://arxiv.org/abs/2312.07669](http://arxiv.org/abs/2312.07669)
- 2018-08-28, **GANimation: Anatomically-aware Facial Animation from a Single Image**, Albert Pumarola et.al., Paper: [http://arxiv.org/abs/1807.09251](http://arxiv.org/abs/1807.09251), Code: **[https://github.com/albertpumarola/GANimation](https://github.com/albertpumarola/GANimation)**
- 2023-11-26, **GAIA: Zero-shot Talking Avatar Generation**, Tianyu He et.al., Paper: [http://arxiv.org/abs/2311.15230](http://arxiv.org/abs/2311.15230)
- 2024-03-02, **G4G:A Generic Framework for High Fidelity Talking Face Generation with Fine-grained Intra-modal Alignment**, Juan Zhang et.al., Paper: [http://arxiv.org/abs/2402.18122](http://arxiv.org/abs/2402.18122)
- 2023-08-30, **From Pixels to Portraits: A Comprehensive Survey of Talking Head Generation Techniques and Applications**, Shreyank N Gowda et.al., Paper: [http://arxiv.org/abs/2308.16041](http://arxiv.org/abs/2308.16041)
- 2024-01-07, **Freetalker: Controllable Speech and Text-Driven Gesture Generation Based on Diffusion Models for Enhanced Speaker Naturalness**, Sicheng Yang et.al., Paper: [http://arxiv.org/abs/2401.03476](http://arxiv.org/abs/2401.03476)
- 2022-08-03, **Free-HeadGAN: Neural Talking Head Synthesis with Explicit Gaze Control**, Michail Christos Doukas et.al., Paper: [http://arxiv.org/abs/2208.02210](http://arxiv.org/abs/2208.02210)
- 2021-10-12, **Fine-grained Identity Preserving Landmark Synthesis for Face Reenactment**, Haichao Zhang et.al., Paper: [http://arxiv.org/abs/2110.04708](http://arxiv.org/abs/2110.04708)
- 2022-10-06, **Finding Directions in GAN's Latent Space for Neural Face Reenactment**, Stella Bounareli et.al., Paper: [http://arxiv.org/abs/2202.00046](http://arxiv.org/abs/2202.00046), Code: **[https://github.com/stelabou/stylegan_directions_face_reenactment](https://github.com/stelabou/stylegan_directions_face_reenactment)**
- 2019-10-28, **Few-shot Video-to-Video Synthesis**, Ting-Chun Wang et.al., Paper: [http://arxiv.org/abs/1910.12713](http://arxiv.org/abs/1910.12713)
- 2019-09-25, **Few-Shot Adversarial Learning of Realistic Neural Talking Head Models**, Egor Zakharov et.al., Paper: [http://arxiv.org/abs/1905.08233](http://arxiv.org/abs/1905.08233)
- 2022-07-13, **FastLTS: Non-Autoregressive End-to-End Unconstrained Lip-to-Speech Synthesis**, Yongqi Wang et.al., Paper: [http://arxiv.org/abs/2207.03800](http://arxiv.org/abs/2207.03800)
- 2024-01-19, **Fast Registration of Photorealistic Avatars for VR Facial Animation**, Chaitanya Patel et.al., Paper: [http://arxiv.org/abs/2401.11002](http://arxiv.org/abs/2401.11002)
- 2022-04-25, **Fast Facial Landmark Detection and Applications: A Survey**, Kostiantyn Khabarlak et.al., Paper: [http://arxiv.org/abs/2101.10808](http://arxiv.org/abs/2101.10808)
- 2022-03-01, **FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset**, Hasam Khalid et.al., Paper: [http://arxiv.org/abs/2108.05080](http://arxiv.org/abs/2108.05080), Code: **[https://github.com/dash-lab/fakeavceleb](https://github.com/dash-lab/fakeavceleb)**
- 2022-09-29, **Facial Landmark Predictions with Applications to Metaverse**, Qiao Han et.al., Paper: [http://arxiv.org/abs/2209.14698](http://arxiv.org/abs/2209.14698), Code: **[https://github.com/sweatybridge/text-to-anime](https://github.com/sweatybridge/text-to-anime)**
- 2020-11-02, **Facial Keypoint Sequence Generation from Audio**, Prateek Manocha et.al., Paper: [http://arxiv.org/abs/2011.01114](http://arxiv.org/abs/2011.01114)
- 2023-03-09, **FaceXHuBERT: Text-less Speech-driven E(X)pressive 3D Facial Animation Synthesis Using Self-Supervised Speech Representation Learning**, Kazi Injamamul Haque et.al., Paper: [http://arxiv.org/abs/2303.05416](http://arxiv.org/abs/2303.05416), Code: **[https://github.com/galib360/facexhubert](https://github.com/galib360/facexhubert)**
- 2022-03-17, **FaceFormer: Speech-Driven 3D Facial Animation with Transformers**, Yingruo Fan et.al., Paper: [http://arxiv.org/abs/2112.05329](http://arxiv.org/abs/2112.05329), Code: **[https://github.com/EvelynFan/FaceFormer](https://github.com/EvelynFan/FaceFormer)**
- 2023-09-20, **FaceDiffuser: Speech-Driven 3D Facial Animation Synthesis Using Diffusion**, Stefan Stan et.al., Paper: [http://arxiv.org/abs/2309.11306](http://arxiv.org/abs/2309.11306), Code: **[https://github.com/uuembodiedsocialai/FaceDiffuser](https://github.com/uuembodiedsocialai/FaceDiffuser)**
- 2024-03-04, **FaceChain-ImagineID: Freely Crafting High-Fidelity Diverse Talking Faces from Disentangled Audio**, Chao Xu et.al., Paper: [http://arxiv.org/abs/2403.01901](http://arxiv.org/abs/2403.01901), Code: **[https://github.com/modelscope/facechain](https://github.com/modelscope/facechain)**
- 2022-06-09, **Face-Dubbing++: Lip-Synchronous, Voice Preserving Translation of Videos**, Alexander Waibel et.al., Paper: [http://arxiv.org/abs/2206.04523](http://arxiv.org/abs/2206.04523)
- 2023-04-06, **Face Animation with an Attribute-Guided Diffusion Model**, Bohan Zeng et.al., Paper: [http://arxiv.org/abs/2304.03199](http://arxiv.org/abs/2304.03199), Code: **[https://github.com/zengbohan0217/fadm](https://github.com/zengbohan0217/fadm)**
- 2020-05-13, **FaR-GAN for One-Shot Face Reenactment**, Hanxiang Hao et.al., Paper: [http://arxiv.org/abs/2005.06402](http://arxiv.org/abs/2005.06402)
- 2023-07-08, **FTFDNet: Learning to Detect Talking Face Video Manipulation with Tri-Modality Interaction**, Ganglai Wang et.al., Paper: [http://arxiv.org/abs/2307.03990](http://arxiv.org/abs/2307.03990)
- 2023-12-09, **FT2TF: First-Person Statement Text-To-Talking Face Generation**, Xingjian Diao et.al., Paper: [http://arxiv.org/abs/2312.05430](http://arxiv.org/abs/2312.05430)
- 2022-02-25, **FSGANv2: Improved Subject Agnostic Face Swapping and Reenactment**, Yuval Nirkin et.al., Paper: [http://arxiv.org/abs/2202.12972](http://arxiv.org/abs/2202.12972)
- 2019-08-16, **FSGAN: Subject Agnostic Face Swapping and Reenactment**, Yuval Nirkin et.al., Paper: [http://arxiv.org/abs/1908.05932](http://arxiv.org/abs/1908.05932), Code: **[https://github.com/YuvalNirkin/fsgan](https://github.com/YuvalNirkin/fsgan)**
- 2020-05-16, **FReeNet: Multi-Identity Face Reenactment**, Jiangning Zhang et.al., Paper: [http://arxiv.org/abs/1905.11805](http://arxiv.org/abs/1905.11805)
- 2023-03-31, **FONT: Flow-guided One-shot Talking Head Generation with Natural Head Motions**, Jin Liu et.al., Paper: [http://arxiv.org/abs/2303.17789](http://arxiv.org/abs/2303.17789)
- 2022-09-21, **FNeVR: Neural Volume Rendering for Face Animation**, Bohan Zeng et.al., Paper: [http://arxiv.org/abs/2209.10340](http://arxiv.org/abs/2209.10340), Code: **[https://github.com/zengbohan0217/FNeVR](https://github.com/zengbohan0217/FNeVR)**
- 2019-11-21, **FLNet: Landmark Driven Fetching and Learning Network for Faithful Talking Facial Animation Synthesis**, Kuangxiao Gu et.al., Paper: [http://arxiv.org/abs/1911.09224](http://arxiv.org/abs/1911.09224)
- 2019-04-02, **FEAFA: A Well-Annotated Dataset for Facial Expression Analysis and 3D Facial Animation**, Yanfu Yan et.al., Paper: [http://arxiv.org/abs/1904.01509](http://arxiv.org/abs/1904.01509)
- 2021-11-04, **FEAFA+: An Extended Well-Annotated Dataset for Facial Expression Analysis and 3D Facial Animation**, Wei Gan et.al., Paper: [http://arxiv.org/abs/2111.02751](http://arxiv.org/abs/2111.02751)
- 2023-07-18, **FACTS: Facial Animation Creation using the Transfer of Styles**, Jack Saunders et.al., Paper: [http://arxiv.org/abs/2307.09480](http://arxiv.org/abs/2307.09480)
- 2021-08-18, **FACIAL: Synthesizing Dynamic Talking Face with Implicit Attribute Learning**, Chenxu Zhang et.al., Paper: [http://arxiv.org/abs/2108.07938](http://arxiv.org/abs/2108.07938), Code: **[https://github.com/zhangchenxu528/FACIAL](https://github.com/zhangchenxu528/FACIAL)**
- 2020-11-09, **FACEGAN: Facial Attribute Controllable rEenactment GAN**, Soumya Tripathy et.al., Paper: [http://arxiv.org/abs/2011.04439](http://arxiv.org/abs/2011.04439)
- 2023-12-20, **FAAC: Facial Animation Generation with Anchor Frame and Conditional Control for Superior Fidelity and Editability**, Linze Li et.al., Paper: [http://arxiv.org/abs/2312.03775](http://arxiv.org/abs/2312.03775)
- 2022-08-17, **Extreme-scale Talking-Face Video Upsampling with Audio-Visual Priors**, Sindhu B Hegde et.al., Paper: [http://arxiv.org/abs/2208.08118](http://arxiv.org/abs/2208.08118), Code: **[https://github.com/Sindhu-Hegde/video-super-resolver](https://github.com/Sindhu-Hegde/video-super-resolver)**
- 2022-11-30, **Extracting Semantic Knowledge from GANs with Unsupervised Learning**, Jianjin Xu et.al., Paper: [http://arxiv.org/abs/2211.16710](http://arxiv.org/abs/2211.16710)
- 2023-02-14, **Expressive Talking Head Video Encoding in StyleGAN2 Latent-Space**, Trevine Oorloff et.al., Paper: [http://arxiv.org/abs/2203.14512](http://arxiv.org/abs/2203.14512), Code: **[https://github.com/trevineoorloff/Encode-in-Style](https://github.com/trevineoorloff/Encode-in-Style)**
- 2024-01-04, **Expressive Speech-driven Facial Animation with controllable emotions**, Yutong Chen et.al., Paper: [http://arxiv.org/abs/2301.02008](http://arxiv.org/abs/2301.02008), Code: **[https://github.com/on1262/facialanimation](https://github.com/on1262/facialanimation)**
- 2024-01-18, **Exposing Lip-syncing Deepfakes from Mouth Inconsistencies**, Soumyya Kanti Datta et.al., Paper: [http://arxiv.org/abs/2401.10113](http://arxiv.org/abs/2401.10113)
- 2024-01-16, **Exploring Phonetic Context-Aware Lip-Sync For Talking Face Generation**, Se Jin Park et.al., Paper: [http://arxiv.org/abs/2305.19556](http://arxiv.org/abs/2305.19556)
- 2023-09-11, **ExpCLIP: Bridging Text and Facial Expressions via Semantic Alignment**, Yicheng Zhong et.al., Paper: [http://arxiv.org/abs/2308.14448](http://arxiv.org/abs/2308.14448)
- 2021-04-07, **Everything's Talkin': Pareidolia Face Reenactment**, Linsen Song et.al., Paper: [http://arxiv.org/abs/2104.03061](http://arxiv.org/abs/2104.03061), Code: **[https://github.com/Linsen13/EverythingTalking](https://github.com/Linsen13/EverythingTalking)**
- 2021-03-03, **Estimating Uniqueness of I-Vector Representation of Human Voice**, Erkam Sinan Tandogan et.al., Paper: [http://arxiv.org/abs/2008.11985](http://arxiv.org/abs/2008.11985)
- 2018-07-19, **End-to-End Speech-Driven Facial Animation with Temporal GANs**, Konstantinos Vougioukas et.al., Paper: [http://arxiv.org/abs/1805.09313](http://arxiv.org/abs/1805.09313)
- 2021-03-19, **End-to-End Lip Synchronisation Based on Pattern Classification**, You Jin Kim et.al., Paper: [http://arxiv.org/abs/2005.08606](http://arxiv.org/abs/2005.08606)
- 2022-03-30, **End to End Lip Synchronization with a Temporal AutoEncoder**, Yoav Shalev et.al., Paper: [http://arxiv.org/abs/2203.16224](http://arxiv.org/abs/2203.16224), Code: **[https://github.com/itsyoavshalev/end-to-end-lip-synchronization-with-a-temporal-autoencoder](https://github.com/itsyoavshalev/end-to-end-lip-synchronization-with-a-temporal-autoencoder)**
- 2023-03-26, **Emotionally Enhanced Talking Face Generation**, Sahil Goyal et.al., Paper: [http://arxiv.org/abs/2303.11548](http://arxiv.org/abs/2303.11548), Code: **[https://github.com/sahilg06/EmoGen](https://github.com/sahilg06/EmoGen)**
- 2023-06-06, **Emotional Talking Head Generation based on Memory-Sharing and Attention-Augmented Networks**, Jianrong Wang et.al., Paper: [http://arxiv.org/abs/2306.03594](http://arxiv.org/abs/2306.03594)
- 2023-09-26, **Emotional Speech-Driven Animation with Content-Emotion Disentanglement**, Radek Daněček et.al., Paper: [http://arxiv.org/abs/2306.08990](http://arxiv.org/abs/2306.08990)
- 2022-05-02, **Emotion-Controllable Generalized Talking Face Generation**, Sanjana Sinha et.al., Paper: [http://arxiv.org/abs/2205.01155](http://arxiv.org/abs/2205.01155)
- 2021-10-26, **Emotion recognition in talking-face videos using persistent entropy and neural networks**, Eduardo Paluzo-Hidalgo et.al., Paper: [http://arxiv.org/abs/2110.13571](http://arxiv.org/abs/2110.13571), Code: **[https://github.com/cimagroup/audiovisual-emotionrecognitionusingtda](https://github.com/cimagroup/audiovisual-emotionrecognitionusingtda)**
- 2019-08-11, **Emotion Dependent Facial Animation from Affective Speech**, Rizwan Sadiq et.al., Paper: [http://arxiv.org/abs/1908.03904](http://arxiv.org/abs/1908.03904)
- 2024-01-16, **EmoTalker: Emotionally Editable Talking Face Generation via Diffusion Model**, Bingyuan Zhang et.al., Paper: [http://arxiv.org/abs/2401.08049](http://arxiv.org/abs/2401.08049)
- 2023-08-25, **EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation**, Ziqiao Peng et.al., Paper: [http://arxiv.org/abs/2303.11089](http://arxiv.org/abs/2303.11089), Code: **[https://github.com/psyai-net/EmoTalk_release](https://github.com/psyai-net/EmoTalk_release)**
- 2024-02-02, **EmoSpeaker: One-shot Fine-grained Emotion-Controlled Talking Face Generation**, Guanwen Feng et.al., Paper: [http://arxiv.org/abs/2402.01422](http://arxiv.org/abs/2402.01422)
- 2019-10-09, **EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos**, Haipeng Zeng et.al., Paper: [http://arxiv.org/abs/1907.12918](http://arxiv.org/abs/1907.12918)
- 2021-07-07, **Egocentric Videoconferencing**, Mohamed Elgharib et.al., Paper: [http://arxiv.org/abs/2107.03109](http://arxiv.org/abs/2107.03109)
- 2022-03-16, **Efficient conditioned face animation using frontally-viewed embedding**, Maxime Oquab et.al., Paper: [http://arxiv.org/abs/2203.08765](http://arxiv.org/abs/2203.08765)
- 2023-08-24, **Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis**, Jiahe Li et.al., Paper: [http://arxiv.org/abs/2307.09323](http://arxiv.org/abs/2307.09323), Code: **[https://github.com/fictionarry/er-nerf](https://github.com/fictionarry/er-nerf)**
- 2023-10-12, **Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation**, Yuan Gan et.al., Paper: [http://arxiv.org/abs/2309.04946](http://arxiv.org/abs/2309.04946), Code: **[https://github.com/yuangan/eat_code](https://github.com/yuangan/eat_code)**
- 2024-02-27, **EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions**, Linrui Tian et.al., Paper: [http://arxiv.org/abs/2402.17485](http://arxiv.org/abs/2402.17485)
- 2024-01-08, **EFHQ: Multi-purpose ExtremePose-Face-HQ dataset**, Trung Tuan Dao et.al., Paper: [http://arxiv.org/abs/2312.17205](http://arxiv.org/abs/2312.17205)
- 2022-09-23, **EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model**, Xinya Ji et.al., Paper: [http://arxiv.org/abs/2205.15278](http://arxiv.org/abs/2205.15278)
- 2018-08-19, **Dynamic Temporal Alignment of Speech to Lips**, Tavi Halperin et.al., Paper: [http://arxiv.org/abs/1808.06250](http://arxiv.org/abs/1808.06250), Code: **[https://github.com/tavihalperin/AV-sync](https://github.com/tavihalperin/AV-sync)**
- 2022-04-13, **Dynamic Neural Textures: Generating Talking-Face Videos with Continuously Controllable Expressions**, Zipeng Ye et.al., Paper: [http://arxiv.org/abs/2204.06180](http://arxiv.org/abs/2204.06180)
- 2020-10-05, **Dynamic Facial Asset and Rig Generation from a Single Scan**, Jiaman Li et.al., Paper: [http://arxiv.org/abs/2010.00560](http://arxiv.org/abs/2010.00560)
- 2022-12-23, **Dubbing in Practice: A Large Scale Study of Human Localization With Insights for Automatic Dubbing**, William Brannon et.al., Paper: [http://arxiv.org/abs/2212.12137](http://arxiv.org/abs/2212.12137)
- 2024-01-11, **Dubbing for Everyone: Data-Efficient Visual Dubbing using Neural Rendering Priors**, Jack Saunders et.al., Paper: [http://arxiv.org/abs/2401.06126](http://arxiv.org/abs/2401.06126)
- 2023-11-13, **DualTalker: A Cross-Modal Dual Learning Approach for Speech-Driven 3D Facial Animation**, Guinan Su et.al., Paper: [http://arxiv.org/abs/2311.04766](http://arxiv.org/abs/2311.04766)
- 2020-09-12, **DualLip: A System for Joint Lip Reading and Generation**, Weicong Chen et.al., Paper: [http://arxiv.org/abs/2009.05784](http://arxiv.org/abs/2009.05784)
- 2023-12-15, **DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models**, Yifeng Ma et.al., Paper: [http://arxiv.org/abs/2312.09767](http://arxiv.org/abs/2312.09767)
- 2023-04-01, **DreamFace: Progressive Generation of Animatable 3D Faces under Text Guidance**, Longwen Zhang et.al., Paper: [http://arxiv.org/abs/2304.03117](http://arxiv.org/abs/2304.03117)
- 2023-03-26, **Distributed Solution of the Inverse Rig Problem in Blendshape Facial Animation**, Stevo Racković et.al., Paper: [http://arxiv.org/abs/2303.06370](http://arxiv.org/abs/2303.06370)
- 2020-05-04, **Disentangled Speech Embeddings using Cross-modal Self-supervision**, Arsha Nagrani et.al., Paper: [http://arxiv.org/abs/2002.08742](http://arxiv.org/abs/2002.08742)
- 2023-03-14, **DisCoHead: Audio-and-Video-Driven Talking Head Generation by Disentangled Control of Head Pose and Facial Expressions**, Geumbyeol Hwang et.al., Paper: [http://arxiv.org/abs/2303.07697](http://arxiv.org/abs/2303.07697), Code: **[https://github.com/deepbrainai-research/koeba](https://github.com/deepbrainai-research/koeba)**
- 2023-12-02, **DiffusionTalker: Personalization and Acceleration for Speech-Driven 3D Face Diffuser**, Peng Chen et.al., Paper: [http://arxiv.org/abs/2311.16565](http://arxiv.org/abs/2311.16565)
- 2023-07-29, **Diffused Heads: Diffusion Models Beat GANs on Talking-Face Generation**, Michał Stypułkowski et.al., Paper: [http://arxiv.org/abs/2301.03396](http://arxiv.org/abs/2301.03396)
- 2023-09-14, **DiffTalker: Co-driven audio-image diffusion for talking faces via intermediate landmarks**, Zipeng Qi et.al., Paper: [http://arxiv.org/abs/2309.07509](http://arxiv.org/abs/2309.07509)
- 2023-04-20, **DiffTalk: Crafting Diffusion Models for Generalized Audio-Driven Portraits Animation**, Shuai Shen et.al., Paper: [http://arxiv.org/abs/2301.03786](http://arxiv.org/abs/2301.03786), Code: **[https://github.com/sstzal/DiffTalk](https://github.com/sstzal/DiffTalk)**
- 2024-02-08, **DiffSpeaker: Speech-Driven 3D Facial Animation with Diffusion Transformer**, Zhiyuan Ma et.al., Paper: [http://arxiv.org/abs/2402.05712](http://arxiv.org/abs/2402.05712), Code: **[https://github.com/theericma/diffspeaker](https://github.com/theericma/diffspeaker)**
- 2023-09-30, **DiffPoseTalk: Speech-Driven Stylistic 3D Facial Animation and Head Pose Generation via Diffusion Models**, Zhiyao Sun et.al., Paper: [http://arxiv.org/abs/2310.00434](http://arxiv.org/abs/2310.00434)
- 2024-01-12, **DiffDub: Person-generic Visual Dubbing Using Inpainting Renderer with Diffusion Auto-encoder**, Tao Liu et.al., Paper: [http://arxiv.org/abs/2311.01811](http://arxiv.org/abs/2311.01811)
- 2023-08-18, **Diff2Lip: Audio Conditioned Diffusion Models for Lip-Synchronization**, Soumik Mukhopadhyay et.al., Paper: [http://arxiv.org/abs/2308.09716](http://arxiv.org/abs/2308.09716), Code: **[https://github.com/soumik-kanad/diff2lip](https://github.com/soumik-kanad/diff2lip)**
- 2023-08-12, **DialogueNeRF: Towards Realistic Avatar Face-to-Face Conversation Video Generation**, Yichao Yan et.al., Paper: [http://arxiv.org/abs/2203.07931](http://arxiv.org/abs/2203.07931)
- 2023-12-11, **DiT-Head: High-Resolution Talking Head Synthesis using Diffusion Transformers**, Aaron Mir et.al., Paper: [http://arxiv.org/abs/2312.06400](http://arxiv.org/abs/2312.06400)
- 2021-09-17, **Detection of GAN-synthesized street videos**, Omran Alamayreh et.al., Paper: [http://arxiv.org/abs/2109.04991](http://arxiv.org/abs/2109.04991)
- 2019-10-16, **Designing Style Matching Conversational Agents**, Deepali Aneja et.al., Paper: [http://arxiv.org/abs/1910.07514](http://arxiv.org/abs/1910.07514)
- 2022-03-15, **Depth-Aware Generative Adversarial Network for Talking Head Video Generation**, Fa-Ting Hong et.al., Paper: [http://arxiv.org/abs/2203.06605](http://arxiv.org/abs/2203.06605), Code: **[https://github.com/harlanhong/cvpr2022-dagan](https://github.com/harlanhong/cvpr2022-dagan)**
- 2020-07-20, **Deformable Style Transfer**, Sunnie S. Y. Kim et.al., Paper: [http://arxiv.org/abs/2003.11038](http://arxiv.org/abs/2003.11038), Code: **[https://github.com/sunniesuhyoung/DST](https://github.com/sunniesuhyoung/DST)**
- 2018-12-20, **DeepFakes: a New Threat to Face Recognition? Assessment and Detection**, Pavel Korshunov et.al., Paper: [http://arxiv.org/abs/1812.08685](http://arxiv.org/abs/1812.08685)
- 2021-08-18, **DeepFake MNIST+: A DeepFake Facial Animation Dataset**, Jiajun Huang et.al., Paper: [http://arxiv.org/abs/2108.07949](http://arxiv.org/abs/2108.07949), Code: **[https://github.com/huangjiadidi/DeepFakeMnist](https://github.com/huangjiadidi/DeepFakeMnist)**
- 2023-02-27, **Deep Visual Forced Alignment: Learning to Align Transcription with Talking Face Video**, Minsu Kim et.al., Paper: [http://arxiv.org/abs/2303.08670](http://arxiv.org/abs/2303.08670)
- 2018-05-29, **Deep Video Portraits**, Hyeongwoo Kim et.al., Paper: [http://arxiv.org/abs/1805.11714](http://arxiv.org/abs/1805.11714)
- 2023-08-21, **Deep Person Generation: A Survey from the Perspective of Face, Pose and Cloth Synthesis**, Tong Sha et.al., Paper: [http://arxiv.org/abs/2109.02081](http://arxiv.org/abs/2109.02081)
- 2020-08-02, **Deep Multi-modality Soft-decoding of Very Low Bit-rate Face Videos**, Yanhui Guo et.al., Paper: [http://arxiv.org/abs/2008.01652](http://arxiv.org/abs/2008.01652)
- 2018-12-22, **Deep Audio-Visual Speech Recognition**, Triantafyllos Afouras et.al., Paper: [http://arxiv.org/abs/1809.02108](http://arxiv.org/abs/1809.02108)
- 2019-07-24, **Data-Driven Physical Face Inversion**, Yeara Kozlov et.al., Paper: [http://arxiv.org/abs/1907.10402](http://arxiv.org/abs/1907.10402)
- 2023-01-23, **Data standardization for robust lip sync**, Chun Wang et.al., Paper: [http://arxiv.org/abs/2202.06198](http://arxiv.org/abs/2202.06198)
- 2020-05-11, **Dancing to the Partisan Beat: A First Analysis of Political Communication on TikTok**, Juan Carlos Medina Serrano et.al., Paper: [http://arxiv.org/abs/2004.05478](http://arxiv.org/abs/2004.05478), Code: **[https://github.com/JuanCarlosCSE/TikTok](https://github.com/JuanCarlosCSE/TikTok)**
- 2023-12-10, **DaGAN++: Depth-Aware Generative Adversarial Network for Talking Head Video Generation**, Fa-Ting Hong et.al., Paper: [http://arxiv.org/abs/2305.06225](http://arxiv.org/abs/2305.06225), Code: **[https://github.com/harlanhong/cvpr2022-dagan](https://github.com/harlanhong/cvpr2022-dagan)**
- 2023-09-14, **DT-NeRF: Decomposed Triplane-Hash Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis**, Yaoyu Su et.al., Paper: [http://arxiv.org/abs/2309.07752](http://arxiv.org/abs/2309.07752)
- 2023-12-21, **DREAM-Talk: Diffusion-based Realistic Emotional Audio-driven Method for Single Image Talking Face Generation**, Chenxu Zhang et.al., Paper: [http://arxiv.org/abs/2312.13578](http://arxiv.org/abs/2312.13578)
- 2023-03-01, **DPE: Disentanglement of Pose and Expression for General Video Portrait Editing**, Youxin Pang et.al., Paper: [http://arxiv.org/abs/2301.06281](http://arxiv.org/abs/2301.06281), Code: **[https://github.com/Carlyx/DPE](https://github.com/Carlyx/DPE)**
- 2023-03-07, **DINet: Deformation Inpainting Network for Realistic Face Visually Dubbing on High Resolution Video**, Zhimeng Zhang et.al., Paper: [http://arxiv.org/abs/2303.03988](http://arxiv.org/abs/2303.03988), Code: **[https://github.com/MRzzm/DINet](https://github.com/MRzzm/DINet)**
- 2022-01-03, **DFA-NeRF: Personalized Talking Head Generation via Disentangled Face Attributes Neural Rendering**, Shunyu Yao et.al., Paper: [http://arxiv.org/abs/2201.00791](http://arxiv.org/abs/2201.00791)
- 2023-09-12, **DF-TransFusion: Multimodal Deepfake Detection via Lip-Audio Cross-Attention and Facial Self-Attention**, Aaditya Kharel et.al., Paper: [http://arxiv.org/abs/2309.06511](http://arxiv.org/abs/2309.06511)
- 2023-08-23, **DF-3DFace: One-to-Many Speech Synchronized 3D Face Animation with Diffusion**, Se Jin Park et.al., Paper: [http://arxiv.org/abs/2310.05934](http://arxiv.org/abs/2310.05934)
- 2024-03-01, **DAE-Talker: High Fidelity Speech-Driven Talking Face Generation with Diffusion Autoencoder**, Chenpeng Du et.al., Paper: [http://arxiv.org/abs/2303.17550](http://arxiv.org/abs/2303.17550)
- 2023-03-05, **Cyber Vaccine for Deepfake Immunity**, Ching-Chun Chang et.al., Paper: [http://arxiv.org/abs/2303.02659](http://arxiv.org/abs/2303.02659)
- 2022-06-29, **Cut Inner Layers: A Structured Pruning Strategy for Efficient U-Net GANs**, Bo-Kyeong Kim et.al., Paper: [http://arxiv.org/abs/2206.14658](http://arxiv.org/abs/2206.14658)
- 2023-10-17, **CorrTalk: Correlation Between Hierarchical Speech and Facial Activity Variances for 3D Animation**, Zhaojie Chu et.al., Paper: [http://arxiv.org/abs/2310.11295](http://arxiv.org/abs/2310.11295)
- 2023-04-27, **Controllable One-Shot Face Video Synthesis With Semantic Aware Prior**, Kangning Liu et.al., Paper: [http://arxiv.org/abs/2304.14471](http://arxiv.org/abs/2304.14471)
- 2023-11-28, **Continuously Controllable Facial Expression Editing in Talking Face Videos**, Zhiyao Sun et.al., Paper: [http://arxiv.org/abs/2209.08289](http://arxiv.org/abs/2209.08289)
- 2024-02-28, **Context-aware Talking Face Video Generation**, Meidai Xuanyuan et.al., Paper: [http://arxiv.org/abs/2402.18092](http://arxiv.org/abs/2402.18092)
- 2023-09-20, **Context-Aware Talking-Head Video Editing**, Songlin Yang et.al., Paper: [http://arxiv.org/abs/2308.00462](http://arxiv.org/abs/2308.00462)
- 2022-10-07, **Compressing Video Calls using Synthetic Talking Heads**, Madhav Agarwal et.al., Paper: [http://arxiv.org/abs/2210.03692](http://arxiv.org/abs/2210.03692)
- 2023-04-03, **CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior**, Jinbo Xing et.al., Paper: [http://arxiv.org/abs/2301.02379](http://arxiv.org/abs/2301.02379), Code: **[https://github.com/Doubiiu/CodeTalker](https://github.com/Doubiiu/CodeTalker)**
- 2023-10-12, **CleftGAN: Adapting A Style-Based Generative Adversarial Network To Create Images Depicting Cleft Lip Deformity**, Abdullah Hayajneh et.al., Paper: [http://arxiv.org/abs/2310.07969](http://arxiv.org/abs/2310.07969), Code: **[https://github.com/abdullah-tamu/CleftGAN](https://github.com/abdullah-tamu/CleftGAN)**
- 2023-11-12, **ChatAnything: Facetime Chat with LLM-Enhanced Personas**, Yilin Zhao et.al., Paper: [http://arxiv.org/abs/2311.06772](http://arxiv.org/abs/2311.06772)
- 2019-05-08, **Capture, Learning, and Synthesis of 3D Speaking Styles**, Daniel Cudeiro et.al., Paper: [http://arxiv.org/abs/1905.03079](http://arxiv.org/abs/1905.03079), Code: **[https://github.com/TimoBolkart/voca](https://github.com/TimoBolkart/voca)**
- 2023-05-23, **CPNet: Exploiting CLIP-based Attention Condenser and Probability Map Guidance for High-fidelity Talking Face Generation**, Jingning Xu et.al., Paper: [http://arxiv.org/abs/2305.13962](http://arxiv.org/abs/2305.13962)
- 2023-11-15, **CP-EB: Talking Face Generation with Controllable Pose and Eye Blinking Embedding**, Jianzong Wang et.al., Paper: [http://arxiv.org/abs/2311.08673](http://arxiv.org/abs/2311.08673)
- 2024-02-21, **Bring Your Own Character: A Holistic Solution for Automatic Facial Animation Generation of Customized Characters**, Zechen Bai et.al., Paper: [http://arxiv.org/abs/2402.13724](http://arxiv.org/abs/2402.13724)
- 2023-10-31, **Breathing Life into Faces: Speech-driven 3D Facial Animation with Natural Head Pose and Detailed Shape**, Wei Zhao et.al., Paper: [http://arxiv.org/abs/2310.20240](http://arxiv.org/abs/2310.20240)
- 2021-11-02, **BiosecurID: a multimodal biometric database**, Julian Fierrez et.al., Paper: [http://arxiv.org/abs/2111.03472](http://arxiv.org/abs/2111.03472)
- 2021-07-27, **Beyond Voice Identity Conversion: Manipulating Voice Attributes by Adversarial Learning of Structured Disentangled Representations**, Laurent Benaroya et.al., Paper: [http://arxiv.org/abs/2107.12346](http://arxiv.org/abs/2107.12346)
- 2023-11-28, **BakedAvatar: Baking Neural Fields for Real-Time Head Avatar Synthesis**, Hao-Bin Duan et.al., Paper: [http://arxiv.org/abs/2311.05521](http://arxiv.org/abs/2311.05521), Code: **[https://github.com/buaavrcg/BakedAvatar](https://github.com/buaavrcg/BakedAvatar)**
- 2023-09-12, **Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head Videos**, Ekta Prashnani et.al., Paper: [http://arxiv.org/abs/2305.03713](http://arxiv.org/abs/2305.03713)
- 2023-04-17, **Autoregressive GAN for Semantic Unconditional Head Motion Generation**, Louis Airale et.al., Paper: [http://arxiv.org/abs/2211.00987](http://arxiv.org/abs/2211.00987), Code: **[https://github.com/louisbearing/unconditionalheadmotion](https://github.com/louisbearing/unconditionalheadmotion)**
- 2022-09-19, **AutoLV: Automatic Lecture Video Generator**, Wenbin Wang et.al., Paper: [http://arxiv.org/abs/2209.08795](http://arxiv.org/abs/2209.08795)
- 2021-08-30, **Audiovisual Speech Synthesis using Tacotron2**, Ahmed Hussen Abdelaziz et.al., Paper: [http://arxiv.org/abs/2008.00620](http://arxiv.org/abs/2008.00620)
- 2021-02-18, **AudioVisual Speech Synthesis: A brief literature review**, Efthymios Georgiou et.al., Paper: [http://arxiv.org/abs/2103.03927](http://arxiv.org/abs/2103.03927)
- 2023-04-25, **AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head**, Rongjie Huang et.al., Paper: [http://arxiv.org/abs/2304.12995](http://arxiv.org/abs/2304.12995), Code: **[https://github.com/aigc-audio/audiogpt](https://github.com/aigc-audio/audiogpt)**
- 2021-07-20, **Audio2Head: Audio-driven One-shot Talking-head Generation with Natural Head Motion**, Suzhen Wang et.al., Paper: [http://arxiv.org/abs/2107.09293](http://arxiv.org/abs/2107.09293), Code: **[https://github.com/wangsuzhen/Audio2Head](https://github.com/wangsuzhen/Audio2Head)**
- 2019-05-27, **Audio2Face: Generating Speech/Face Animation from Single Audio with Attention-Based Bidirectional LSTM Networks**, Guanzhong Tian et.al., Paper: [http://arxiv.org/abs/1905.11142](http://arxiv.org/abs/1905.11142)
- 2020-03-05, **Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose**, Ran Yi et.al., Paper: [http://arxiv.org/abs/2002.10137](http://arxiv.org/abs/2002.10137), Code: **[https://github.com/yiranran/Audio-driven-TalkingFace-HeadPose](https://github.com/yiranran/Audio-driven-TalkingFace-HeadPose)**
- 2023-12-11, **Audio-driven Talking Face Generation by Overcoming Unintended Information Flow**, Dogucan Yaman et.al., Paper: [http://arxiv.org/abs/2307.09368](http://arxiv.org/abs/2307.09368)
- 2023-05-18, **Audio-Visual Person-of-Interest DeepFake Detection**, Davide Cozzolino et.al., Paper: [http://arxiv.org/abs/2204.03083](http://arxiv.org/abs/2204.03083), Code: **[https://github.com/grip-unina/poi-forensics](https://github.com/grip-unina/poi-forensics)**
- 2022-10-06, **Audio-Visual Face Reenactment**, Madhav Agarwal et.al., Paper: [http://arxiv.org/abs/2210.02755](http://arxiv.org/abs/2210.02755), Code: **[https://github.com/mdv3101/AVFR-Gan](https://github.com/mdv3101/AVFR-Gan)**
- 2023-09-15, **Audio-Visual Active Speaker Extraction for Sparsely Overlapped Multi-talker Speech**, Junjie Li et.al., Paper: [http://arxiv.org/abs/2309.08408](http://arxiv.org/abs/2309.08408), Code: **[https://github.com/mrjunjieli/activeextract](https://github.com/mrjunjieli/activeextract)**
- 2022-01-16, **Audio-Driven Talking Face Video Generation with Dynamic Convolution Kernels**, Zipeng Ye et.al., Paper: [http://arxiv.org/abs/2201.05986](http://arxiv.org/abs/2201.05986)
- 2023-04-18, **Audio-Driven Talking Face Generation with Diverse yet Realistic Facial Animations**, Rongliang Wu et.al., Paper: [http://arxiv.org/abs/2304.08945](http://arxiv.org/abs/2304.08945)
- 2021-05-20, **Audio-Driven Emotional Video Portraits**, Xinya Ji et.al., Paper: [http://arxiv.org/abs/2104.07452](http://arxiv.org/abs/2104.07452)
- 2023-06-20, **Audio-Driven 3D Facial Animation from In-the-Wild Videos**, Liying Lu et.al., Paper: [http://arxiv.org/abs/2306.11541](http://arxiv.org/abs/2306.11541)
- 2020-08-11, **Audio- and Gaze-driven Facial Animation of Codec Avatars**, Alexander Richard et.al., Paper: [http://arxiv.org/abs/2008.05023](http://arxiv.org/abs/2008.05023)
- 2023-12-15, **Attention-Based VR Facial Animation with Visual Mouth Camera Guidance for Immersive Telepresence Avatars**, Andre Rochow et.al., Paper: [http://arxiv.org/abs/2312.09750](http://arxiv.org/abs/2312.09750)
- 2022-03-08, **Attention-Based Lip Audio-Visual Synthesis for Talking Face Generation in the Wild**, Ganglai Wang et.al., Paper: [http://arxiv.org/abs/2203.03984](http://arxiv.org/abs/2203.03984)
- 2020-05-13, **Arbitrary Talking Face Generation via Attentional Audio-Visual Coherence Learning**, Hao Zhu et.al., Paper: [http://arxiv.org/abs/1812.06589](http://arxiv.org/abs/1812.06589)
- 2021-08-11, **AnyoneNet: Synchronized Speech and Talking Head Generation for Arbitrary Person**, Xinsheng Wang et.al., Paper: [http://arxiv.org/abs/2108.04325](http://arxiv.org/abs/2108.04325)
- 2019-03-13, **Animating an Autonomous 3D Talking Avatar**, Dominik Borer et.al., Paper: [http://arxiv.org/abs/1903.05448](http://arxiv.org/abs/1903.05448)
- 2019-10-02, **Animating Face using Disentangled Audio Representations**, Gaurav Mittal et.al., Paper: [http://arxiv.org/abs/1910.00726](http://arxiv.org/abs/1910.00726)
- 2020-09-20, **An Improved Approach of Intention Discovery with Machine Learning for POMDP-based Dialogue Management**, Ruturaj Raval et.al., Paper: [http://arxiv.org/abs/2009.09354](http://arxiv.org/abs/2009.09354)
- 2024-01-27, **An Implicit Physical Face Model Driven by Expression and Style**, Lingchen Yang et.al., Paper: [http://arxiv.org/abs/2401.15414](http://arxiv.org/abs/2401.15414)
- 2022-03-10, **An Audio-Visual Attention Based Multimodal Network for Fake Talking Face Videos Detection**, Ganglai Wang et.al., Paper: [http://arxiv.org/abs/2203.05178](http://arxiv.org/abs/2203.05178)
- 2023-05-18, **An Android Robot Head as Embodied Conversational Agent**, Marcel Heisler et.al., Paper: [http://arxiv.org/abs/2305.10945](http://arxiv.org/abs/2305.10945)
- 2022-12-28, **All's well that FID's well? Result quality and metric scores in GAN models for lip-sychronization tasks**, Carina Geldhauser et.al., Paper: [http://arxiv.org/abs/2212.13810](http://arxiv.org/abs/2212.13810)
- 2020-11-30, **Adaptive Compact Attention For Few-shot Video-to-video Translation**, Risheng Huang et.al., Paper: [http://arxiv.org/abs/2011.14695](http://arxiv.org/abs/2011.14695)
- 2024-01-08, **AdaMesh: Personalized Facial Expressions and Head Poses for Adaptive Speech-Driven 3D Facial Animation**, Liyang Chen et.al., Paper: [http://arxiv.org/abs/2310.07236](http://arxiv.org/abs/2310.07236)
- 2023-08-02, **Ada-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis**, Zhenhui Ye et.al., Paper: [http://arxiv.org/abs/2306.03504](http://arxiv.org/abs/2306.03504)
- 2020-03-30, **ActGAN: Flexible and Efficient One-shot Face Reenactment**, Ivan Kosarevych et.al., Paper: [http://arxiv.org/abs/2003.13840](http://arxiv.org/abs/2003.13840)
- 2021-09-20, **Accurate, Interpretable, and Fast Animation: An Iterative, Sparse, and Nonconvex Approach**, Stevo Rackovic et.al., Paper: [http://arxiv.org/abs/2109.08356](http://arxiv.org/abs/2109.08356)
- 2023-03-27, **Accurate and Interpretable Solution of the Inverse Rig for Realistic Blendshape Models with Quadratic Corrective Terms**, Stevo Racković et.al., Paper: [http://arxiv.org/abs/2302.04843](http://arxiv.org/abs/2302.04843)
- 2024-02-25, **AVI-Talking: Learning Audio-Visual Instructions for Expressive 3D Talking Face Generation**, Yasheng Sun et.al., Paper: [http://arxiv.org/abs/2402.16124](http://arxiv.org/abs/2402.16124)
- 2020-10-25, **APB2FaceV2: Real-Time Audio-Guided Multi-Face Reenactment**, Jiangning Zhang et.al., Paper: [http://arxiv.org/abs/2010.13017](http://arxiv.org/abs/2010.13017), Code: **[https://github.com/zhangzjn/APB2FaceV2](https://github.com/zhangzjn/APB2FaceV2)**
- 2020-04-30, **APB2Face: Audio-guided face reenactment with auxiliary pose and blink signals**, Jiangning Zhang et.al., Paper: [http://arxiv.org/abs/2004.14569](http://arxiv.org/abs/2004.14569)
- 2023-12-18, **AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head Synthesis**, Dongze Li et.al., Paper: [http://arxiv.org/abs/2312.10921](http://arxiv.org/abs/2312.10921)
- 2021-08-19, **AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis**, Yudong Guo et.al., Paper: [http://arxiv.org/abs/2103.11078](http://arxiv.org/abs/2103.11078), Code: **[https://github.com/YudongGuo/AD-NeRF](https://github.com/YudongGuo/AD-NeRF)**
- 2019-07-23, **A system for efficient 3D printed stop-motion face animation**, Rinat Abdrashitov et.al., Paper: [http://arxiv.org/abs/1907.10163](http://arxiv.org/abs/1907.10163)
- 2023-04-28, **A Unified Compression Framework for Efficient Speech-Driven Talking-Face Generation**, Bo-Kyeong Kim et.al., Paper: [http://arxiv.org/abs/2304.00471](http://arxiv.org/abs/2304.00471)
- 2023-08-17, **A Survey on Deep Multi-modal Learning for Body Language Recognition and Generation**, Li Liu et.al., Paper: [http://arxiv.org/abs/2308.08849](http://arxiv.org/abs/2308.08849), Code: **[https://github.com/wentaol86/awesome-body-language](https://github.com/wentaol86/awesome-body-language)**
- 2020-07-18, **A Robust Interactive Facial Animation Editing System**, Eloïse Berson et.al., Paper: [http://arxiv.org/abs/2007.09367](http://arxiv.org/abs/2007.09367)
- 2022-05-02, **A Novel Speech-Driven Lip-Sync Model with CNN and LSTM**, Xiaohong Li et.al., Paper: [http://arxiv.org/abs/2205.00916](http://arxiv.org/abs/2205.00916)
- 2021-05-05, **A Neural Lip-Sync Framework for Synthesizing Photorealistic Virtual News Anchors**, Ruobing Zheng et.al., Paper: [http://arxiv.org/abs/2002.08700](http://arxiv.org/abs/2002.08700)
- 2023-03-27, **A Majorization-Minimization Based Method for Nonconvex Inverse Rig Problems in Facial Animation: Algorithm Derivation**, Stevo Racković et.al., Paper: [http://arxiv.org/abs/2205.04289](http://arxiv.org/abs/2205.04289)
- 2020-08-23, **A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild**, K R Prajwal et.al., Paper: [http://arxiv.org/abs/2008.10010](http://arxiv.org/abs/2008.10010), Code: **[https://github.com/Rudrabha/Wav2Lip](https://github.com/Rudrabha/Wav2Lip)**
- 2022-10-07, **A Keypoint Based Enhancement Method for Audio Driven Free View Talking Head Synthesis**, Yichen Han et.al., Paper: [http://arxiv.org/abs/2210.03335](http://arxiv.org/abs/2210.03335)
- 2022-07-27, **A Hybrid Deep Animation Codec for Low-bitrate Video Conferencing**, Goluck Konuko et.al., Paper: [http://arxiv.org/abs/2207.13530](http://arxiv.org/abs/2207.13530)
- 2019-10-15, **A High-Fidelity Open Embodied Avatar with Lip Syncing and Expression Capabilities**, Deepali Aneja et.al., Paper: [http://arxiv.org/abs/1909.08766](http://arxiv.org/abs/1909.08766), Code: **[https://github.com/danmcduff/AvatarSim](https://github.com/danmcduff/AvatarSim)**
- 2022-08-01, **A Feasibility Study on Image Inpainting for Non-cleft Lip Generation from Patients with Cleft Lip**, Shuang Chen et.al., Paper: [http://arxiv.org/abs/2208.01149](http://arxiv.org/abs/2208.01149), Code: **[https://github.com/chrischen1023/nclg-mt](https://github.com/chrischen1023/nclg-mt)**
- 2023-07-04, **A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony in Talking Head Generation**, Louis Airale et.al., Paper: [http://arxiv.org/abs/2307.03270](http://arxiv.org/abs/2307.03270), Code: **[https://github.com/louisbearing/hmo-audio](https://github.com/louisbearing/hmo-audio)**
- 2023-04-06, **4D Agnostic Real-Time Facial Animation Pipeline for Desktop Scenarios**, Wei Chen et.al., Paper: [http://arxiv.org/abs/2304.02814](http://arxiv.org/abs/2304.02814)
- 2023-12-01, **3DiFACE: Diffusion-based Speech-driven 3D Facial Animation and Editing**, Balamurugan Thambiraja et.al., Paper: [http://arxiv.org/abs/2312.00870](http://arxiv.org/abs/2312.00870)
- 2021-04-25, **3D-TalkEmo: Learning to Synthesize 3D Emotional Talking Head**, Qianyun Wang et.al., Paper: [http://arxiv.org/abs/2104.12051](http://arxiv.org/abs/2104.12051)
- 2023-11-05, **3D-Aware Talking-Head Video Motion Transfer**, Haomiao Ni et.al., Paper: [http://arxiv.org/abs/2311.02549](http://arxiv.org/abs/2311.02549)
- 2019-08-29, **3D Face Pose and Animation Tracking via Eigen-Decomposition based Bayesian Approach**, Ngoc-Trung Tran et.al., Paper: [http://arxiv.org/abs/1908.11039](http://arxiv.org/abs/1908.11039)
- 2020-08-29, **"It took me almost 30 minutes to practice this". Performance and Production Practices in Dance Challenge Videos on TikTok**, Daniel Klug et.al., Paper: [http://arxiv.org/abs/2008.13040](http://arxiv.org/abs/2008.13040)

<p align=right>(<a href=#updated-on-20240312>back to top</a>)</p>

## Image Animation

- 2023-04-12, **VidStyleODE: Disentangled Video Editing via StyleGAN and NeuralODEs**, Moayed Haji Ali et.al., Paper: [http://arxiv.org/abs/2304.06020](http://arxiv.org/abs/2304.06020)
- 2015-03-16, **Use of Effective Audio in E-learning Courseware**, Kisor Ray et.al., Paper: [http://arxiv.org/abs/1503.04837](http://arxiv.org/abs/1503.04837)
- 2020-12-01, **Ultra-low bitrate video conferencing using deep image animation**, Goluck Konuko et.al., Paper: [http://arxiv.org/abs/2012.00346](http://arxiv.org/abs/2012.00346)
- 2010-01-04, **Tutoring System for Dance Learning**, Rajkumar Kannan et.al., Paper: [http://arxiv.org/abs/1001.0440](http://arxiv.org/abs/1001.0440)
- 2024-03-05, **Tuning-Free Noise Rectification for High Fidelity Image-to-Video Generation**, Weijie Li et.al., Paper: [http://arxiv.org/abs/2403.02827](http://arxiv.org/abs/2403.02827)
- 2022-03-29, **Thin-Plate Spline Motion Model for Image Animation**, Jian Zhao et.al., Paper: [http://arxiv.org/abs/2203.14367](http://arxiv.org/abs/2203.14367), Code: **[https://github.com/yoyo-nb/thin-plate-spline-motion-model](https://github.com/yoyo-nb/thin-plate-spline-motion-model)**
- 2023-09-26, **Text-Guided Synthesis of Eulerian Cinemagraphs**, Aniruddha Mahapatra et.al., Paper: [http://arxiv.org/abs/2307.03190](http://arxiv.org/abs/2307.03190), Code: **[https://github.com/text2cinemagraph/text2cinemagraph](https://github.com/text2cinemagraph/text2cinemagraph)**
- 2021-09-03, **Sparse to Dense Motion Transfer for Face Image Animation**, Ruiqi Zhao et.al., Paper: [http://arxiv.org/abs/2109.00471](http://arxiv.org/abs/2109.00471)
- 2022-07-19, **Single Stage Virtual Try-on via Deformable Attention Flows**, Shuai Bai et.al., Paper: [http://arxiv.org/abs/2207.09161](http://arxiv.org/abs/2207.09161), Code: **[https://github.com/OFA-Sys/DAFlow](https://github.com/OFA-Sys/DAFlow)**
- 2021-04-07, **Single Source One Shot Reenactment using Weighted motion From Paired Feature Points**, Soumya Tripathy et.al., Paper: [http://arxiv.org/abs/2104.03117](http://arxiv.org/abs/2104.03117)
- 2018-01-31, **RAPTOR I: Time-dependent radiative transfer in arbitrary spacetimes**, Thomas Bronzwaer et.al., Paper: [http://arxiv.org/abs/1801.10452](http://arxiv.org/abs/1801.10452)
- 2021-03-22, **PriorityCut: Occlusion-guided Regularization for Warp-based Image Animation**, Wai Ting Cheung et.al., Paper: [http://arxiv.org/abs/2103.11600](http://arxiv.org/abs/2103.11600)
- 2023-07-09, **Predictive Coding For Animation-Based Video Compression**, Goluck Konuko et.al., Paper: [http://arxiv.org/abs/2307.04187](http://arxiv.org/abs/2307.04187)
- 2023-12-21, **PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models**, Yiming Zhang et.al., Paper: [http://arxiv.org/abs/2312.13964](http://arxiv.org/abs/2312.13964), Code: **[https://github.com/open-mmlab/PIA](https://github.com/open-mmlab/PIA)**
- 2022-04-05, **Neural Fields in Visual Computing and Beyond**, Yiheng Xie et.al., Paper: [http://arxiv.org/abs/2111.11426](http://arxiv.org/abs/2111.11426)
- 2022-11-30, **NeRFInvertor: High Fidelity NeRF-GAN Inversion for Single-shot Real Image Animation**, Yu Yin et.al., Paper: [http://arxiv.org/abs/2211.17235](http://arxiv.org/abs/2211.17235)
- 2015-02-04, **Multimedia-Video for Learning**, Kah Hean Chua et.al., Paper: [http://arxiv.org/abs/1502.01090](http://arxiv.org/abs/1502.01090)
- 2021-12-19, **Move As You Like: Image Animation in E-Commerce Scenario**, Borun Xu et.al., Paper: [http://arxiv.org/abs/2112.13647](http://arxiv.org/abs/2112.13647)
- 2023-11-30, **Motion-Conditioned Image Animation for Video Editing**, Wilson Yan et.al., Paper: [http://arxiv.org/abs/2311.18827](http://arxiv.org/abs/2311.18827)
- 2022-09-28, **Motion Transformer for Unsupervised Image Animation**, Jiale Tao et.al., Paper: [http://arxiv.org/abs/2209.14024](http://arxiv.org/abs/2209.14024), Code: **[https://github.com/jialetao/motrans](https://github.com/jialetao/motrans)**
- 2024-01-03, **Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions**, David Junhao Zhang et.al., Paper: [http://arxiv.org/abs/2401.01827](http://arxiv.org/abs/2401.01827), Code: **[https://github.com/salesforce/lavis](https://github.com/salesforce/lavis)**
- 2013-01-25, **Measurements of Martian Dust Devil Winds with HiRISE**, David S. Choi et.al., Paper: [http://arxiv.org/abs/1301.6130](http://arxiv.org/abs/1301.6130)
- 2023-11-27, **MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model**, Zhongcong Xu et.al., Paper: [http://arxiv.org/abs/2311.16498](http://arxiv.org/abs/2311.16498)
- 2023-12-05, **LivePhoto: Real Image Animation with Text-guided Motion Control**, Xi Chen et.al., Paper: [http://arxiv.org/abs/2312.02928](http://arxiv.org/abs/2312.02928)
- 2022-03-17, **Latent Image Animator: Learning to Animate Images via Latent Space Navigation**, Yaohui Wang et.al., Paper: [http://arxiv.org/abs/2203.09043](http://arxiv.org/abs/2203.09043)
- 2023-10-11, **LEO: Generative Latent Image Animator for Human Video Synthesis**, Yaohui Wang et.al., Paper: [http://arxiv.org/abs/2305.03989](http://arxiv.org/abs/2305.03989), Code: **[https://github.com/wyhsirius/LEO](https://github.com/wyhsirius/LEO)**
- 2023-10-16, **LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation**, Ruiqi Wu et.al., Paper: [http://arxiv.org/abs/2310.10769](http://arxiv.org/abs/2310.10769), Code: **[https://github.com/RQ-Wu/LAMP](https://github.com/RQ-Wu/LAMP)**
- 2022-07-08, **Jointly Harnessing Prior Structures and Temporal Consistency for Sign Language Video Generation**, Yucheng Suo et.al., Paper: [http://arxiv.org/abs/2207.03714](http://arxiv.org/abs/2207.03714)
- 2021-10-26, **Incremental Learning for Animal Pose Estimation using RBF k-DPP**, Gaurav Kumar Nayak et.al., Paper: [http://arxiv.org/abs/2110.13598](http://arxiv.org/abs/2110.13598)
- 2022-10-04, **Implicit Warping for Animation with Image Sets**, Arun Mallya et.al., Paper: [http://arxiv.org/abs/2210.01794](http://arxiv.org/abs/2210.01794)
- 2022-03-29, **Image Animation with Perturbed Masks**, Yoav Shalev et.al., Paper: [http://arxiv.org/abs/2011.06922](http://arxiv.org/abs/2011.06922), Code: **[https://github.com/itsyoavshalev/Image-Animation-with-Perturbed-Masks](https://github.com/itsyoavshalev/Image-Animation-with-Perturbed-Masks)**
- 2021-12-21, **Image Animation with Keypoint Mask**, Or Toledano et.al., Paper: [http://arxiv.org/abs/2112.10457](http://arxiv.org/abs/2112.10457), Code: **[https://github.com/or-toledano/animation-with-keypoint-mask](https://github.com/or-toledano/animation-with-keypoint-mask)**
- 2016-06-23, **Gender and Interest Targeting for Sponsored Post Advertising at Tumblr**, Mihajlo Grbovic et.al., Paper: [http://arxiv.org/abs/1606.07189](http://arxiv.org/abs/1606.07189)
- 2020-10-01, **First Order Motion Model for Image Animation**, Aliaksandr Siarohin et.al., Paper: [http://arxiv.org/abs/2003.00196](http://arxiv.org/abs/2003.00196), Code: **[https://github.com/AliaksandrSiarohin/first-order-model](https://github.com/AliaksandrSiarohin/first-order-model)**
- 2023-11-27, **DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors**, Jinbo Xing et.al., Paper: [http://arxiv.org/abs/2310.12190](http://arxiv.org/abs/2310.12190), Code: **[https://github.com/Doubiiu/DynamiCrafter](https://github.com/Doubiiu/DynamiCrafter)**
- 2023-02-02, **Dreamix: Video Diffusion Models are General Video Editors**, Eyal Molad et.al., Paper: [http://arxiv.org/abs/2302.01329](http://arxiv.org/abs/2302.01329)
- 2023-11-19, **Differential Motion Evolution for Fine-Grained Motion Deformation in Unsupervised Image Animation**, Peirong Liu et.al., Paper: [http://arxiv.org/abs/2110.04658](http://arxiv.org/abs/2110.04658)
- 2021-08-18, **DeepFake MNIST+: A DeepFake Facial Animation Dataset**, Jiajun Huang et.al., Paper: [http://arxiv.org/abs/2108.07949](http://arxiv.org/abs/2108.07949), Code: **[https://github.com/huangjiadidi/DeepFakeMnist](https://github.com/huangjiadidi/DeepFakeMnist)**
- 2020-08-27, **Deep Spatial Transformation for Pose-Guided Person Image Generation and Animation**, Yurui Ren et.al., Paper: [http://arxiv.org/abs/2008.12606](http://arxiv.org/abs/2008.12606), Code: **[https://github.com/RenYurui/Global-Flow-Local-Attention](https://github.com/RenYurui/Global-Flow-Local-Attention)**
- 2023-01-14, **Continuous odor profile monitoring to study olfactory navigation in small animals**, Kevin S. Chen et.al., Paper: [http://arxiv.org/abs/2301.05905](http://arxiv.org/abs/2301.05905)
- 2024-01-17, **Continuous Piecewise-Affine Based Motion Model for Image Animation**, Hexiang Wang et.al., Paper: [http://arxiv.org/abs/2401.09146](http://arxiv.org/abs/2401.09146), Code: **[https://github.com/devilpg/aaai2024-cpabmm](https://github.com/devilpg/aaai2024-cpabmm)**
- 2022-06-11, **Bayesian Statistics Guided Label Refurbishment Mechanism: Mitigating Label Noise in Medical Image Classification**, Mengdi Gao et.al., Paper: [http://arxiv.org/abs/2106.12284](http://arxiv.org/abs/2106.12284), Code: **[https://github.com/neugmd/blrm](https://github.com/neugmd/blrm)**
- 2023-09-25, **Automatic Animation of Hair Blowing in Still Portrait Photos**, Wenpeng Xiao et.al., Paper: [http://arxiv.org/abs/2309.14207](http://arxiv.org/abs/2309.14207)
- 2019-08-30, **Animating Arbitrary Objects via Deep Motion Transfer**, Aliaksandr Siarohin et.al., Paper: [http://arxiv.org/abs/1812.08861](http://arxiv.org/abs/1812.08861), Code: **[https://github.com/AliaksandrSiarohin/monkey-net](https://github.com/AliaksandrSiarohin/monkey-net)**
- 2023-12-06, **AnimateZero: Video Diffusion Models are Zero-Shot Image Animators**, Jiwen Yu et.al., Paper: [http://arxiv.org/abs/2312.03793](http://arxiv.org/abs/2312.03793), Code: **[https://github.com/vvictoryuki/animatezero](https://github.com/vvictoryuki/animatezero)**
- 2023-07-10, **AnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning**, Yuwei Guo et.al., Paper: [http://arxiv.org/abs/2307.04725](http://arxiv.org/abs/2307.04725), Code: **[https://github.com/guoyww/animatediff](https://github.com/guoyww/animatediff)**
- 2023-12-04, **AnimateAnything: Fine-Grained Open Domain Image Animation with Motion Guidance**, Zuozhuo Dai et.al., Paper: [http://arxiv.org/abs/2311.12886](http://arxiv.org/abs/2311.12886), Code: **[https://github.com/alibaba/animate-anything](https://github.com/alibaba/animate-anything)**
- 2021-06-23, **Analisis Kualitas Layanan Website E-Commerce Bukalapak Terhadap Kepuasan Pengguna Mahasiswa Universitas Bina Darma Menggunakan Metode Webqual 4.0**, Adellia et.al., Paper: [http://arxiv.org/abs/2106.15342](http://arxiv.org/abs/2106.15342)
- 2021-12-17, **AI-Empowered Persuasive Video Generation: A Survey**, Chang Liu et.al., Paper: [http://arxiv.org/abs/2112.09401](http://arxiv.org/abs/2112.09401)
- 2018-06-24, **A Design of FPGA Based Small Animal PET Real Time Digital Signal Processing and Correction Logic**, Jiaming Lu et.al., Paper: [http://arxiv.org/abs/1806.09117](http://arxiv.org/abs/1806.09117)
- 2018-10-09, **3D model silhouette-based tracking in depth images for puppet suit dynamic video-mapping**, Guillaume Caron et.al., Paper: [http://arxiv.org/abs/1810.03956](http://arxiv.org/abs/1810.03956)
- 2022-03-25, **3D GAN Inversion for Controllable Portrait Image Animation**, Connor Z. Lin et.al., Paper: [http://arxiv.org/abs/2203.13441](http://arxiv.org/abs/2203.13441)
- 2023-03-10, **3D Cinemagraphy from a Single Image**, Xingyi Li et.al., Paper: [http://arxiv.org/abs/2303.05724](http://arxiv.org/abs/2303.05724)

<p align=right>(<a href=#updated-on-20240312>back to top</a>)</p>

## TTS

- 2023-08-29, **a unified front-end framework for english text-to-speech synthesis**, Zelin Ying et.al., Paper: [http://arxiv.org/abs/2305.10666](http://arxiv.org/abs/2305.10666)
- 2023-04-24, **Zero-shot text-to-speech synthesis conditioned using self-supervised speech representation model**, Kenichi Fujita et.al., Paper: [http://arxiv.org/abs/2304.11976](http://arxiv.org/abs/2304.11976)
- 2023-12-22, **ZMM-TTS: Zero-shot Multilingual and Multispeaker Speech Synthesis Conditioned on Self-supervised Discrete Speech Representations**, Cheng Gong et.al., Paper: [http://arxiv.org/abs/2312.14398](http://arxiv.org/abs/2312.14398)
- 2023-05-23, **ZET-Speech: Zero-shot adaptive Emotion-controllable Text-to-Speech Synthesis with Diffusion and Style-based Models**, Minki Kang et.al., Paper: [http://arxiv.org/abs/2305.13831](http://arxiv.org/abs/2305.13831)
- 2023-05-31, **XPhoneBERT: A Pre-trained Multilingual Model for Phoneme Representations for Text-to-Speech**, Linh The Nguyen et.al., Paper: [http://arxiv.org/abs/2305.19709](http://arxiv.org/abs/2305.19709), Code: **[https://github.com/vinairesearch/xphonebert](https://github.com/vinairesearch/xphonebert)**
- 2023-08-08, **WonderFlow: Narration-Centric Design of Animated Data Videos**, Yun Wang et.al., Paper: [http://arxiv.org/abs/2308.04040](http://arxiv.org/abs/2308.04040)
- 2022-07-20, **When Is TTS Augmentation Through a Pivot Language Useful?**, Nathaniel Robinson et.al., Paper: [http://arxiv.org/abs/2207.09889](http://arxiv.org/abs/2207.09889), Code: **[https://github.com/n8rob/multilingual_tts_augmentation](https://github.com/n8rob/multilingual_tts_augmentation)**
- 2023-03-24, **Wave-U-Net Discriminator: Fast and Lightweight Discriminator for Generative Adversarial Network-Based Speech Synthesis**, Takuhiro Kaneko et.al., Paper: [http://arxiv.org/abs/2303.13909](http://arxiv.org/abs/2303.13909)
- 2023-11-29, **Vulnerability of Automatic Identity Recognition to Audio-Visual Deepfakes**, Pavel Korshunov et.al., Paper: [http://arxiv.org/abs/2311.17655](http://arxiv.org/abs/2311.17655)
- 2023-10-19, **Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale**, Matthew Le et.al., Paper: [http://arxiv.org/abs/2306.15687](http://arxiv.org/abs/2306.15687)
- 2023-09-24, **VoiceLDM: Text-to-Speech with Environmental Context**, Yeonghyeon Lee et.al., Paper: [http://arxiv.org/abs/2309.13664](http://arxiv.org/abs/2309.13664)
- 2024-01-16, **VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching**, Yiwei Guo et.al., Paper: [http://arxiv.org/abs/2309.05027](http://arxiv.org/abs/2309.05027)
- 2022-08-21, **Visualising Model Training via Vowel Space for Text-To-Speech Systems**, Binu Abeysinghe et.al., Paper: [http://arxiv.org/abs/2208.09775](http://arxiv.org/abs/2208.09775), Code: **[https://github.com/babe269/performant](https://github.com/babe269/performant)**
- 2023-06-21, **Visual-Aware Text-to-Speech**, Mohan Zhou et.al., Paper: [http://arxiv.org/abs/2306.12020](http://arxiv.org/abs/2306.12020)
- 2023-03-15, **Virtuoso: Massive Multilingual Speech-Text Joint Semi-Supervised Learning for Text-To-Speech**, Takaaki Saeki et.al., Paper: [http://arxiv.org/abs/2210.15447](http://arxiv.org/abs/2210.15447)
- 2023-05-25, **VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation**, Tianrui Wang et.al., Paper: [http://arxiv.org/abs/2305.16107](http://arxiv.org/abs/2305.16107)
- 2023-05-22, **ViT-TTS: Visual Text-to-Speech with Scalable Diffusion Transformer**, Huadai Liu et.al., Paper: [http://arxiv.org/abs/2305.12708](http://arxiv.org/abs/2305.12708)
- 2023-10-12, **Vec-Tok Speech: speech vectorization and tokenization for neural speech generation**, Xinfa Zhu et.al., Paper: [http://arxiv.org/abs/2310.07246](http://arxiv.org/abs/2310.07246), Code: **[https://github.com/bakerbunker/vectok](https://github.com/bakerbunker/vectok)**
- 2023-02-27, **Varianceflow: High-Quality and Controllable Text-to-Speech using Variance Information via Normalizing Flow**, Yoonhyung Lee et.al., Paper: [http://arxiv.org/abs/2302.13458](http://arxiv.org/abs/2302.13458)
- 2023-07-31, **VITS2: Improving Quality and Efficiency of Single-Stage Text-to-Speech with Adversarial Learning and Architecture Design**, Jungil Kong et.al., Paper: [http://arxiv.org/abs/2307.16430](http://arxiv.org/abs/2307.16430)
- 2023-06-08, **VIFS: An End-to-End Variational Inference for Foley Sound Synthesis**, Junhyeok Lee et.al., Paper: [http://arxiv.org/abs/2306.05004](http://arxiv.org/abs/2306.05004), Code: **[https://github.com/junjun3518/vifs](https://github.com/junjun3518/vifs)**
- 2024-01-30, **VALL-T: Decoder-Only Generative Transducer for Robust and Decoding-Controllable Text-to-Speech**, Chenpeng Du et.al., Paper: [http://arxiv.org/abs/2401.14321](http://arxiv.org/abs/2401.14321)
- 2023-05-21, **VAKTA-SETU: A Speech-to-Speech Machine Translation Service in Select Indic Languages**, Shivam Mhaskar et.al., Paper: [http://arxiv.org/abs/2305.12518](http://arxiv.org/abs/2305.12518)
- 2023-03-01, **UzbekTagger: The rule-based POS tagger for Uzbek language**, Maksud Sharipov et.al., Paper: [http://arxiv.org/abs/2301.12711](http://arxiv.org/abs/2301.12711)
- 2023-11-18, **Utilizing Speech Emotion Recognition and Recommender Systems for Negative Emotion Handling in Therapy Chatbots**, Farideh Majidi et.al., Paper: [http://arxiv.org/abs/2311.11116](http://arxiv.org/abs/2311.11116)
- 2024-01-03, **Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic Token Prediction**, Minchan Kim et.al., Paper: [http://arxiv.org/abs/2401.01498](http://arxiv.org/abs/2401.01498)
- 2022-09-14, **Using Rater and System Metadata to Explain Variance in the VoiceMOS Challenge 2022 Dataset**, Michael Chinen et.al., Paper: [http://arxiv.org/abs/2209.06358](http://arxiv.org/abs/2209.06358)
- 2023-01-06, **Using External Off-Policy Speech-To-Text Mappings in Contextual End-To-End Automated Speech Recognition**, David M. Chan et.al., Paper: [http://arxiv.org/abs/2301.02736](http://arxiv.org/abs/2301.02736)
- 2023-03-28, **Unsupervised Pre-Training For Data-Efficient Text-to-Speech On Low Resource Languages**, Seongyeon Park et.al., Paper: [http://arxiv.org/abs/2303.15669](http://arxiv.org/abs/2303.15669), Code: **[https://github.com/cnaigithub/SpeechDewarping](https://github.com/cnaigithub/SpeechDewarping)**
- 2023-01-26, **Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a Case Study**, Massa Baali et.al., Paper: [http://arxiv.org/abs/2301.09099](http://arxiv.org/abs/2301.09099), Code: **[https://github.com/espnet/espnet/tree/master/egs2/qasr_tts/tts1](https://github.com/espnet/espnet/tree/master/egs2/qasr_tts/tts1)**
- 2023-06-28, **UnitSpeech: Speaker-adaptive Speech Synthesis with Untranscribed Data**, Heeseung Kim et.al., Paper: [http://arxiv.org/abs/2306.16083](http://arxiv.org/abs/2306.16083), Code: **[https://github.com/gmltmd789/UnitSpeech](https://github.com/gmltmd789/UnitSpeech)**
- 2023-05-24, **Unit-based Speech-to-Speech Translation Without Parallel Data**, Anuj Diwan et.al., Paper: [http://arxiv.org/abs/2305.15405](http://arxiv.org/abs/2305.15405), Code: **[https://github.com/ajd12342/unit-speech-translation](https://github.com/ajd12342/unit-speech-translation)**
- 2023-01-10, **UnifySpeech: A Unified Framework for Zero-shot Text-to-Speech and Voice Conversion**, Haogeng Liu et.al., Paper: [http://arxiv.org/abs/2301.03801](http://arxiv.org/abs/2301.03801)
- 2022-07-04, **Unify and Conquer: How Phonetic Feature Representation Affects Polyglot Text-To-Speech (TTS)**, Ariadna Sanchez et.al., Paper: [http://arxiv.org/abs/2207.01547](http://arxiv.org/abs/2207.01547)
- 2024-01-09, **Unified speech and gesture synthesis using flow matching**, Shivam Mehta et.al., Paper: [http://arxiv.org/abs/2310.05181](http://arxiv.org/abs/2310.05181)
- 2024-02-08, **Unified Speech-Text Pretraining for Spoken Dialog Modeling**, Heeseung Kim et.al., Paper: [http://arxiv.org/abs/2402.05706](http://arxiv.org/abs/2402.05706)
- 2022-12-06, **UniSyn: An End-to-End Unified Model for Text-to-Speech and Singing Voice Synthesis**, Yi Lei et.al., Paper: [http://arxiv.org/abs/2212.01546](http://arxiv.org/abs/2212.01546)
- 2023-05-19, **UniFLG: Unified Facial Landmark Generator from Text or Speech**, Kentaro Mitsui et.al., Paper: [http://arxiv.org/abs/2302.14337](http://arxiv.org/abs/2302.14337)
- 2024-01-18, **UniCATS: A Unified Context-Aware Text-to-Speech Framework with Contextual VQ-Diffusion and Vocoding**, Chenpeng Du et.al., Paper: [http://arxiv.org/abs/2306.07547](http://arxiv.org/abs/2306.07547)
- 2023-05-22, **U-DiT TTS: U-Diffusion Vision Transformer for Text-to-Speech**, Xin Jing et.al., Paper: [http://arxiv.org/abs/2305.13195](http://arxiv.org/abs/2305.13195)
- 2022-07-25, **Transplantation of Conversational Speaking Style with Interjections in Sequence-to-Sequence Speech Synthesis**, Raul Fernandez et.al., Paper: [http://arxiv.org/abs/2207.12262](http://arxiv.org/abs/2207.12262)
- 2024-01-07, **Transfer the linguistic representations from TTS to accent conversion with non-parallel data**, Xi Chen et.al., Paper: [http://arxiv.org/abs/2401.03538](http://arxiv.org/abs/2401.03538)
- 2023-11-08, **Transduce and Speak: Neural Transducer for Text-to-Speech with Semantic Token Prediction**, Minchan Kim et.al., Paper: [http://arxiv.org/abs/2311.02898](http://arxiv.org/abs/2311.02898)
- 2022-08-28, **Training Text-To-Speech Systems From Synthetic Data: A Practical Approach For Accent Transfer Tasks**, Lev Finkelstein et.al., Paper: [http://arxiv.org/abs/2208.13183](http://arxiv.org/abs/2208.13183)
- 2022-10-28, **Towards zero-shot Text-based voice editing using acoustic context conditioning, utterance embeddings, and reference encoders**, Jason Fong et.al., Paper: [http://arxiv.org/abs/2210.16045](http://arxiv.org/abs/2210.16045)
- 2023-10-02, **Towards human-like spoken dialogue generation between AI agents from written dialogue**, Kentaro Mitsui et.al., Paper: [http://arxiv.org/abs/2310.01088](http://arxiv.org/abs/2310.01088)
- 2023-08-08, **Towards an AI to Win Ghana's National Science and Maths Quiz**, George Boateng et.al., Paper: [http://arxiv.org/abs/2308.04333](http://arxiv.org/abs/2308.04333), Code: **[https://github.com/nsmq-ai/nsmqai](https://github.com/nsmq-ai/nsmqai)**
- 2023-08-31, **Towards Spontaneous Style Modeling with Semi-supervised Pre-training for Conversational Text-to-Speech Synthesis**, Weiqin Li et.al., Paper: [http://arxiv.org/abs/2308.16593](http://arxiv.org/abs/2308.16593)
- 2023-05-30, **Towards Selection of Text-to-speech Data to Augment ASR Training**, Shuo Liu et.al., Paper: [http://arxiv.org/abs/2306.00998](http://arxiv.org/abs/2306.00998)
- 2023-06-02, **Towards Robust FastSpeech 2 by Modelling Residual Multimodality**, Fabian Kögel et.al., Paper: [http://arxiv.org/abs/2306.01442](http://arxiv.org/abs/2306.01442), Code: **[https://github.com/sony/ai-research-code](https://github.com/sony/ai-research-code)**
- 2022-10-17, **Towards Relation Extraction From Speech**, Tongtong Wu et.al., Paper: [http://arxiv.org/abs/2210.08759](http://arxiv.org/abs/2210.08759), Code: **[https://github.com/wutong8023/speechre](https://github.com/wutong8023/speechre)**
- 2022-08-15, **Towards Parametric Speech Synthesis Using Gaussian-Markov Model of Spectral Envelope and Wavelet-Based Decomposition of F0**, Mohammed Salah Al-Radhi et.al., Paper: [http://arxiv.org/abs/2208.07122](http://arxiv.org/abs/2208.07122)
- 2022-10-04, **Towards MOOCs for Lipreading: Using Synthetic Talking Heads to Train Humans in Lipreading at Scale**, Aditya Agarwal et.al., Paper: [http://arxiv.org/abs/2208.09796](http://arxiv.org/abs/2208.09796)
- 2023-09-20, **Towards Joint Modeling of Dialogue Response and Speech Synthesis based on Large Language Model**, Xinyu Zhou et.al., Paper: [http://arxiv.org/abs/2309.11000](http://arxiv.org/abs/2309.11000), Code: **[https://github.com/XinyuZhou2000/Spoken_Dialogue](https://github.com/XinyuZhou2000/Spoken_Dialogue)**
- 2023-06-14, **Towards Building Voice-based Conversational Recommender Systems: Datasets, Potential Solutions, and Prospects**, Xinghua Qu et.al., Paper: [http://arxiv.org/abs/2306.08219](http://arxiv.org/abs/2306.08219), Code: **[https://github.com/hyllll/vcrs](https://github.com/hyllll/vcrs)**
- 2023-02-17, **Towards Building Text-To-Speech Systems for the Next Billion Users**, Gokul Karthik Kumar et.al., Paper: [http://arxiv.org/abs/2211.09536](http://arxiv.org/abs/2211.09536), Code: **[https://github.com/gokulkarthik/text2speech](https://github.com/gokulkarthik/text2speech)**
- 2024-03-02, **Towards Accurate Lip-to-Speech Synthesis in-the-Wild**, Sindhu Hegde et.al., Paper: [http://arxiv.org/abs/2403.01087](http://arxiv.org/abs/2403.01087)
- 2023-01-31, **Time out of Mind: Generating Rate of Speech conditioned on emotion and speaker**, Navjot Kaur et.al., Paper: [http://arxiv.org/abs/2301.12331](http://arxiv.org/abs/2301.12331)
- 2023-10-07, **The VoiceMOS Challenge 2023: Zero-shot Subjective Speech Quality Prediction for Multiple Domains**, Erica Cooper et.al., Paper: [http://arxiv.org/abs/2310.02640](http://arxiv.org/abs/2310.02640)
- 2023-07-10, **The NPU-MSXF Speech-to-Speech Translation System for IWSLT 2023 Speech-to-Speech Translation Task**, Kun Song et.al., Paper: [http://arxiv.org/abs/2307.04630](http://arxiv.org/abs/2307.04630)
- 2022-10-26, **The NPU-ASLP System for The ISCSLP 2022 Magichub Code-Swiching ASR Challenge**, Yuhao Liang et.al., Paper: [http://arxiv.org/abs/2210.14448](http://arxiv.org/abs/2210.14448)
- 2023-09-21, **The Impact of Silence on Speech Anti-Spoofing**, Yuxiang Zhang et.al., Paper: [http://arxiv.org/abs/2309.11827](http://arxiv.org/abs/2309.11827)
- 2023-09-01, **The FruitShell French synthesis system at the Blizzard 2023 Challenge**, Xin Qi et.al., Paper: [http://arxiv.org/abs/2309.00223](http://arxiv.org/abs/2309.00223)
- 2023-06-01, **The Effects of Input Type and Pronunciation Dictionary Usage in Transfer Learning for Low-Resource Text-to-Speech**, Phat Do et.al., Paper: [http://arxiv.org/abs/2306.00535](http://arxiv.org/abs/2306.00535)
- 2023-09-01, **The DeepZen Speech Synthesis System for Blizzard Challenge 2023**, Christophe Veaux et.al., Paper: [http://arxiv.org/abs/2308.15945](http://arxiv.org/abs/2308.15945)
- 2023-08-28, **TextrolSpeech: A Text Style Control Speech Corpus With Codec Language Text-to-Speech Models**, Shengpeng Ji et.al., Paper: [http://arxiv.org/abs/2308.14430](http://arxiv.org/abs/2308.14430)
- 2022-10-26, **Text-to-speech synthesis from dark data with evaluation-in-the-loop data selection**, Kentaro Seki et.al., Paper: [http://arxiv.org/abs/2210.14850](http://arxiv.org/abs/2210.14850)
- 2022-12-16, **Text-to-speech synthesis based on latent variable conversion using diffusion probabilistic model and variational autoencoder**, Yusuke Yasuda et.al., Paper: [http://arxiv.org/abs/2212.08329](http://arxiv.org/abs/2212.08329)
- 2023-08-12, **Text-to-Video: a Two-stage Framework for Zero-shot Identity-agnostic Talking-head Generation**, Zhichao Wang et.al., Paper: [http://arxiv.org/abs/2308.06457](http://arxiv.org/abs/2308.06457), Code: **[https://github.com/zhichaowang970201/text-to-video](https://github.com/zhichaowang970201/text-to-video)**
- 2023-05-31, **Text-to-Speech Pipeline for Swiss German -- A comparison**, Tobias Bollinger et.al., Paper: [http://arxiv.org/abs/2305.19750](http://arxiv.org/abs/2305.19750)
- 2023-03-09, **Text-to-ECG: 12-Lead Electrocardiogram Synthesis conditioned on Clinical Text Reports**, Hyunseung Chung et.al., Paper: [http://arxiv.org/abs/2303.09395](http://arxiv.org/abs/2303.09395), Code: **[https://github.com/tclife/text_to_ecg](https://github.com/tclife/text_to_ecg)**
- 2022-07-13, **Text-driven Emotional Style Control and Cross-speaker Style Transfer in Neural TTS**, Yookyung Shin et.al., Paper: [http://arxiv.org/abs/2207.06000](http://arxiv.org/abs/2207.06000)
- 2024-01-25, **Text to speech synthesis**, Harini s et.al., Paper: [http://arxiv.org/abs/2401.13891](http://arxiv.org/abs/2401.13891)
- 2023-03-27, **Text is All You Need: Personalizing ASR Models using Controllable Speech Synthesis**, Karren Yang et.al., Paper: [http://arxiv.org/abs/2303.14885](http://arxiv.org/abs/2303.14885)
- 2023-05-22, **Text Generation with Speech Synthesis for ASR Data Augmentation**, Zhuangqun Huang et.al., Paper: [http://arxiv.org/abs/2305.16333](http://arxiv.org/abs/2305.16333)
- 2022-10-20, **Text Enhancement for Paragraph Processing in End-to-End Code-switching TTS**, Chunyu Qiang et.al., Paper: [http://arxiv.org/abs/2210.11429](http://arxiv.org/abs/2210.11429)
- 2022-11-01, **Technology Pipeline for Large Scale Cross-Lingual Dubbing of Lecture Videos into Multiple Indian Languages**, Anusha Prakash et.al., Paper: [http://arxiv.org/abs/2211.01338](http://arxiv.org/abs/2211.01338)
- 2022-06-30, **TTS-by-TTS 2: Data-selective augmentation for neural speech synthesis using ranking support vector machine with variational autoencoder**, Eunwoo Song et.al., Paper: [http://arxiv.org/abs/2206.14984](http://arxiv.org/abs/2206.14984)
- 2022-12-20, **TTS-Guided Training for Accent Conversion Without Parallel Data**, Yi Zhou et.al., Paper: [http://arxiv.org/abs/2212.10204](http://arxiv.org/abs/2212.10204)
- 2022-09-16, **TIMIT-TTS: a Text-to-Speech Dataset for Multimodal Synthetic Media Detection**, Davide Salvi et.al., Paper: [http://arxiv.org/abs/2209.08000](http://arxiv.org/abs/2209.08000)
- 2023-09-29, **Synthetic Speech Detection Based on Temporal Consistency and Distribution of Speaker Features**, Yuxiang Zhang et.al., Paper: [http://arxiv.org/abs/2309.16954](http://arxiv.org/abs/2309.16954)
- 2023-11-08, **Synthetic Speaking Children -- Why We Need Them and How to Make Them**, Muhammad Ali Farooq et.al., Paper: [http://arxiv.org/abs/2311.06307](http://arxiv.org/abs/2311.06307)
- 2023-07-19, **Synthesizing Speech Test Cases with Text-to-Speech? An Empirical Study on the False Alarms in Automated Speech Recognition Testing**, Julia Kaiwen Lau et.al., Paper: [http://arxiv.org/abs/2305.17445](http://arxiv.org/abs/2305.17445), Code: **[https://github.com/julianyonghao/fainasrtest](https://github.com/julianyonghao/fainasrtest)**
- 2022-12-29, **StyleTTS-VC: One-Shot Voice Conversion by Knowledge Transfer from Style-Based TTS Models**, Yinghao Aaron Li et.al., Paper: [http://arxiv.org/abs/2212.14227](http://arxiv.org/abs/2212.14227), Code: **[https://github.com/yl4579/StyleTTS-VC](https://github.com/yl4579/StyleTTS-VC)**
- 2023-11-20, **StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models**, Yinghao Aaron Li et.al., Paper: [http://arxiv.org/abs/2306.07691](http://arxiv.org/abs/2306.07691)
- 2023-07-25, **StyleS2ST: Zero-shot Style Transfer for Direct Speech-to-speech Translation**, Kun Song et.al., Paper: [http://arxiv.org/abs/2305.17732](http://arxiv.org/abs/2305.17732)
- 2023-10-27, **Style Description based Text-to-Speech with Conditional Prosodic Layer Normalization based Diffusion GAN**, Neeraj Kumar et.al., Paper: [http://arxiv.org/abs/2310.18169](http://arxiv.org/abs/2310.18169)
- 2022-11-04, **Stutter-TTS: Controlled Synthesis and Improved Recognition of Stuttered Speech**, Xin Zhang et.al., Paper: [http://arxiv.org/abs/2211.09731](http://arxiv.org/abs/2211.09731)
- 2022-10-31, **Structured State Space Decoder for Speech Recognition and Synthesis**, Koichi Miyazaki et.al., Paper: [http://arxiv.org/abs/2210.17098](http://arxiv.org/abs/2210.17098)
- 2023-05-28, **Stochastic Pitch Prediction Improves the Diversity and Naturalness of Speech in Glow-TTS**, Sewade Ogun et.al., Paper: [http://arxiv.org/abs/2305.17724](http://arxiv.org/abs/2305.17724), Code: **[https://github.com/ogunlao/glowtts_stdp](https://github.com/ogunlao/glowtts_stdp)**
- 2023-08-14, **SpeechX: Neural Codec Language Model as a Versatile Speech Transformer**, Xiaofei Wang et.al., Paper: [http://arxiv.org/abs/2308.06873](http://arxiv.org/abs/2308.06873)
- 2024-01-23, **SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models**, Xin Zhang et.al., Paper: [http://arxiv.org/abs/2308.16692](http://arxiv.org/abs/2308.16692), Code: **[https://github.com/0nutation/uslm](https://github.com/0nutation/uslm)**
- 2022-12-08, **SpeechLMScore: Evaluating speech generation using speech language model**, Soumi Maiti et.al., Paper: [http://arxiv.org/abs/2212.04559](http://arxiv.org/abs/2212.04559), Code: **[https://github.com/espnet/espnet](https://github.com/espnet/espnet)**
- 2024-01-25, **SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation**, Dong Zhang et.al., Paper: [http://arxiv.org/abs/2401.13527](http://arxiv.org/abs/2401.13527), Code: **[https://github.com/0nutation/speechgpt](https://github.com/0nutation/speechgpt)**
- 2022-12-28, **Speech Synthesis with Mixed Emotions**, Kun Zhou et.al., Paper: [http://arxiv.org/abs/2208.05890](http://arxiv.org/abs/2208.05890)
- 2022-12-16, **Speech Aware Dialog System Technology Challenge (DSTC11)**, Hagen Soltau et.al., Paper: [http://arxiv.org/abs/2212.08704](http://arxiv.org/abs/2212.08704)
- 2022-07-11, **Speaker consistency loss and step-wise optimization for semi-supervised joint training of TTS and ASR using unpaired text data**, Naoki Makishima et.al., Paper: [http://arxiv.org/abs/2207.04659](http://arxiv.org/abs/2207.04659)
- 2023-02-07, **Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision**, Eugene Kharitonov et.al., Paper: [http://arxiv.org/abs/2302.03540](http://arxiv.org/abs/2302.03540)
- 2023-09-20, **Speak While You Think: Streaming Speech Synthesis During Text Generation**, Avihu Dekel et.al., Paper: [http://arxiv.org/abs/2309.11210](http://arxiv.org/abs/2309.11210)
- 2023-03-07, **Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling**, Ziqiang Zhang et.al., Paper: [http://arxiv.org/abs/2303.03926](http://arxiv.org/abs/2303.03926)
- 2023-09-22, **Sparks of Large Audio Models: A Survey and Outlook**, Siddique Latif et.al., Paper: [http://arxiv.org/abs/2308.12792](http://arxiv.org/abs/2308.12792)
- 2023-04-26, **Source-Filter-Based Generative Adversarial Neural Vocoder for High Fidelity Speech Synthesis**, Ye-Xin Lu et.al., Paper: [http://arxiv.org/abs/2304.13270](http://arxiv.org/abs/2304.13270)
- 2024-01-31, **Singing Voice Data Scaling-up: An Introduction to ACE-Opencpop and KiSing-v2**, Jiatong Shi et.al., Paper: [http://arxiv.org/abs/2401.17619](http://arxiv.org/abs/2401.17619), Code: **[https://github.com/espnet/espnet](https://github.com/espnet/espnet)**
- 2022-06-29, **Simple and Effective Multi-sentence TTS with Expressive and Coherent Prosody**, Peter Makarov et.al., Paper: [http://arxiv.org/abs/2206.14643](http://arxiv.org/abs/2206.14643)
- 2023-05-29, **Semi-supervised learning for continuous emotional intensity controllable speech synthesis with disentangled representations**, Yoori Oh et.al., Paper: [http://arxiv.org/abs/2211.06160](http://arxiv.org/abs/2211.06160)
- 2022-10-25, **Semi-Supervised Learning Based on Reference Model for Low-resource TTS**, Xulong Zhang et.al., Paper: [http://arxiv.org/abs/2210.14723](http://arxiv.org/abs/2210.14723)
- 2024-01-11, **Self-Attention and Hybrid Features for Replay and Deep-Fake Audio Detection**, Lian Huang et.al., Paper: [http://arxiv.org/abs/2401.05614](http://arxiv.org/abs/2401.05614)
- 2023-10-25, **SeamlessM4T: Massively Multilingual & Multimodal Machine Translation**, Seamless Communication et.al., Paper: [http://arxiv.org/abs/2308.11596](http://arxiv.org/abs/2308.11596), Code: **[https://github.com/facebookresearch/seamless_communication](https://github.com/facebookresearch/seamless_communication)**
- 2023-12-06, **Schrodinger Bridges Beat Diffusion Models on Text-to-Speech Synthesis**, Zehua Chen et.al., Paper: [http://arxiv.org/abs/2312.03491](http://arxiv.org/abs/2312.03491)
- 2023-05-30, **STT4SG-350: A Speech Corpus for All Swiss German Dialect Regions**, Michel Plüss et.al., Paper: [http://arxiv.org/abs/2305.18855](http://arxiv.org/abs/2305.18855)
- 2023-06-01, **SQuId: Measuring Speech Naturalness in Many Languages**, Thibault Sellam et.al., Paper: [http://arxiv.org/abs/2210.06324](http://arxiv.org/abs/2210.06324)
- 2022-11-14, **SNIPER Training: Variable Sparsity Rate Training For Text-To-Speech**, Perry Lam et.al., Paper: [http://arxiv.org/abs/2211.07283](http://arxiv.org/abs/2211.07283)
- 2022-11-30, **SNAC: Speaker-normalized affine coupling layer in flow-based architecture for zero-shot multi-speaker text-to-speech**, Byoung Jin Choi et.al., Paper: [http://arxiv.org/abs/2211.16866](http://arxiv.org/abs/2211.16866)
- 2023-07-18, **SLMGAN: Exploiting Speech Language Model Representations for Unsupervised Zero-Shot Voice Conversion in GANs**, Yinghao Aaron Li et.al., Paper: [http://arxiv.org/abs/2307.09435](http://arxiv.org/abs/2307.09435)
- 2023-07-20, **SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer**, Daegyeom Kim et.al., Paper: [http://arxiv.org/abs/2307.10550](http://arxiv.org/abs/2307.10550), Code: **[https://github.com/0913ktg/sc_vall-e](https://github.com/0913ktg/sc_vall-e)**
- 2022-07-13, **SATTS: Speaker Attractor Text to Speech, Learning to Speak by Learning to Separate**, Nabarun Goswami et.al., Paper: [http://arxiv.org/abs/2207.06011](http://arxiv.org/abs/2207.06011)
- 2023-04-23, **SAR: Self-Supervised Anti-Distortion Representation for End-To-End Speech Model**, Jianzong Wang et.al., Paper: [http://arxiv.org/abs/2304.11547](http://arxiv.org/abs/2304.11547)
- 2022-09-08, **SANIP: Shopping Assistant and Navigation for the visually impaired**, Shubham Deshmukh et.al., Paper: [http://arxiv.org/abs/2209.03570](http://arxiv.org/abs/2209.03570)
- 2023-08-02, **SALTTS: Leveraging Self-Supervised Speech Representations for improved Text-to-Speech Synthesis**, Ramanan Sivaguru et.al., Paper: [http://arxiv.org/abs/2308.01018](http://arxiv.org/abs/2308.01018)
- 2022-11-02, **Robust MelGAN: A robust universal neural vocoder for high-fidelity TTS**, Kun Song et.al., Paper: [http://arxiv.org/abs/2210.17349](http://arxiv.org/abs/2210.17349)
- 2023-06-05, **Rhythm-controllable Attention with High Robustness for Long Sentence Speech Synthesis**, Dengfeng Ke et.al., Paper: [http://arxiv.org/abs/2306.02593](http://arxiv.org/abs/2306.02593)
- 2023-05-30, **Resource-Efficient Fine-Tuning Strategies for Automatic MOS Prediction in Text-to-Speech for Low-Resource Languages**, Phat Do et.al., Paper: [http://arxiv.org/abs/2305.19396](http://arxiv.org/abs/2305.19396)
- 2022-10-28, **Residual Adapters for Few-Shot Text-to-Speech Speaker Adaptation**, Nobuyuki Morioka et.al., Paper: [http://arxiv.org/abs/2210.15868](http://arxiv.org/abs/2210.15868)
- 2022-12-30, **ResGrad: Residual Denoising Diffusion Probabilistic Models for Text to Speech**, Zehua Chen et.al., Paper: [http://arxiv.org/abs/2212.14518](http://arxiv.org/abs/2212.14518)
- 2023-09-04, **Rep2wav: Noise Robust text-to-speech Using self-supervised representations**, Qiushi Zhu et.al., Paper: [http://arxiv.org/abs/2308.14553](http://arxiv.org/abs/2308.14553)
- 2022-12-21, **ReVISE: Self-Supervised Speech Resynthesis with Visual Input for Universal and Generalized Speech Enhancement**, Wei-Ning Hsu et.al., Paper: [http://arxiv.org/abs/2212.11377](http://arxiv.org/abs/2212.11377)
- 2024-01-31, **ReFlow-TTS: A Rectified Flow Model for High-fidelity Text-to-Speech**, Wenhao Guan et.al., Paper: [http://arxiv.org/abs/2309.17056](http://arxiv.org/abs/2309.17056)
- 2023-12-02, **Rapid Speaker Adaptation in Low Resource Text to Speech Systems using Synthetic Data and Transfer learning**, Raviraj Joshi et.al., Paper: [http://arxiv.org/abs/2312.01107](http://arxiv.org/abs/2312.01107)
- 2022-12-15, **RWEN-TTS: Relation-aware Word Encoding Network for Natural Text-to-Speech Synthesis**, Shinhyeok Oh et.al., Paper: [http://arxiv.org/abs/2212.07939](http://arxiv.org/abs/2212.07939), Code: **[https://github.com/shinhyeokoh/rwen](https://github.com/shinhyeokoh/rwen)**
- 2022-06-30, **R-MelNet: Reduced Mel-Spectral Modeling for Neural TTS**, Kyle Kastner et.al., Paper: [http://arxiv.org/abs/2206.15276](http://arxiv.org/abs/2206.15276)
- 2023-02-23, **QuickVC: Any-to-many Voice Conversion Using Inverse Short-time Fourier Transform for Faster Conversion**, Houjian Guo et.al., Paper: [http://arxiv.org/abs/2302.08296](http://arxiv.org/abs/2302.08296), Code: **[https://github.com/quickvc/quickvoice-conversion](https://github.com/quickvc/quickvoice-conversion)**
- 2023-08-31, **QS-TTS: Towards Semi-Supervised Text-to-Speech Synthesis via Vector-Quantized Self-Supervised Speech Representation Learning**, Haohan Guo et.al., Paper: [http://arxiv.org/abs/2309.00126](http://arxiv.org/abs/2309.00126)
- 2023-03-14, **QI-TTS: Questioning Intonation Control for Emotional Speech Synthesis**, Haobin Tang et.al., Paper: [http://arxiv.org/abs/2303.07682](http://arxiv.org/abs/2303.07682)
- 2023-08-28, **Pruning Self-Attention for Zero-Shot Multi-Speaker Text-to-Speech**, Hyungchan Yoon et.al., Paper: [http://arxiv.org/abs/2308.14909](http://arxiv.org/abs/2308.14909)
- 2023-10-10, **Prosody Analysis of Audiobooks**, Charuta Pethe et.al., Paper: [http://arxiv.org/abs/2310.06930](http://arxiv.org/abs/2310.06930)
- 2022-11-22, **PromptTTS: Controllable Text-to-Speech with Text Descriptions**, Zhifang Guo et.al., Paper: [http://arxiv.org/abs/2211.12171](http://arxiv.org/abs/2211.12171)
- 2023-12-27, **PromptTTS++: Controlling Speaker Identity in Prompt-Based Text-to-Speech Using Natural Language Descriptions**, Reo Shimizu et.al., Paper: [http://arxiv.org/abs/2309.08140](http://arxiv.org/abs/2309.08140)
- 2023-10-12, **PromptTTS 2: Describing and Generating Voices with Text Prompt**, Yichong Leng et.al., Paper: [http://arxiv.org/abs/2309.02285](http://arxiv.org/abs/2309.02285)
- 2023-06-01, **PromptStyle: Controllable Style Transfer for Text-to-Speech with Natural Language Descriptions**, Guanghou Liu et.al., Paper: [http://arxiv.org/abs/2305.19522](http://arxiv.org/abs/2305.19522)
- 2022-12-14, **Probing Deep Speaker Embeddings for Speaker-related Tasks**, Zifeng Zhao et.al., Paper: [http://arxiv.org/abs/2212.07068](http://arxiv.org/abs/2212.07068)
- 2022-07-13, **ProDiff: Progressive Fast Diffusion Model For High-Quality Text-to-Speech**, Rongjie Huang et.al., Paper: [http://arxiv.org/abs/2207.06389](http://arxiv.org/abs/2207.06389), Code: **[https://github.com/Rongjiehuang/ProDiff](https://github.com/Rongjiehuang/ProDiff)**
- 2022-10-13, **Pre-Avatar: An Automatic Presentation Generation Framework Leveraging Talking Avatar**, Aolan Sun et.al., Paper: [http://arxiv.org/abs/2210.06877](http://arxiv.org/abs/2210.06877)
- 2023-01-20, **Phoneme-Level BERT for Enhanced Prosody of Text-to-Speech with Grapheme Predictions**, Yinghao Aaron Li et.al., Paper: [http://arxiv.org/abs/2301.08810](http://arxiv.org/abs/2301.08810)
- 2023-02-22, **Period VITS: Variational Inference with Explicit Pitch Modeling for End-to-end Emotional Speech Synthesis**, Yuma Shirahata et.al., Paper: [http://arxiv.org/abs/2210.15964](http://arxiv.org/abs/2210.15964)
- 2023-06-13, **PauseSpeech: Natural Speech Synthesis via Pre-trained Language Model and Pause-based Prosody Modeling**, Ji-Sang Hwang et.al., Paper: [http://arxiv.org/abs/2306.07489](http://arxiv.org/abs/2306.07489)
- 2023-12-17, **ParrotTTS: Text-to-Speech synthesis by exploiting self-supervised representations**, Neil Shah et.al., Paper: [http://arxiv.org/abs/2303.01261](http://arxiv.org/abs/2303.01261)
- 2023-05-18, **Parameter-Efficient Learning for Text-to-Speech Accent Adaptation**, Li-Jen Yang et.al., Paper: [http://arxiv.org/abs/2305.11320](http://arxiv.org/abs/2305.11320), Code: **[https://github.com/TTS-Research/PEL-TTS](https://github.com/TTS-Research/PEL-TTS)**
- 2022-11-06, **Parallel Attention Forcing for Machine Translation**, Qingyun Dou et.al., Paper: [http://arxiv.org/abs/2211.03237](http://arxiv.org/abs/2211.03237)
- 2023-06-06, **PITS: Variational Pitch Inference without Fundamental Frequency for End-to-End Pitch-controllable TTS**, Junhyeok Lee et.al., Paper: [http://arxiv.org/abs/2302.12391](http://arxiv.org/abs/2302.12391), Code: **[https://github.com/anonymous-pits/pits](https://github.com/anonymous-pits/pits)**
- 2024-02-01, **PAM: Prompting Audio-Language Models for Audio Quality Assessment**, Soham Deshmukh et.al., Paper: [http://arxiv.org/abs/2402.00282](http://arxiv.org/abs/2402.00282), Code: **[https://github.com/soham97/pam](https://github.com/soham97/pam)**
- 2023-05-29, **OverFlow: Putting flows on top of neural transducers for better TTS**, Shivam Mehta et.al., Paper: [http://arxiv.org/abs/2211.06892](http://arxiv.org/abs/2211.06892), Code: **[https://github.com/coqui-ai/TTS](https://github.com/coqui-ai/TTS)**
- 2023-07-11, **On the Use of Self-Supervised Speech Representations in Spontaneous Speech Synthesis**, Siyang Wang et.al., Paper: [http://arxiv.org/abs/2307.05132](http://arxiv.org/abs/2307.05132)
- 2024-02-19, **On the Semantic Latent Space of Diffusion-Based Text-to-Speech Models**, Miri Varshavsky-Hassid et.al., Paper: [http://arxiv.org/abs/2402.12423](http://arxiv.org/abs/2402.12423)
- 2023-10-12, **On the Relevance of Phoneme Duration Variability of Synthesized Training Data for Automatic Speech Recognition**, Nick Rossenbach et.al., Paper: [http://arxiv.org/abs/2310.08132](http://arxiv.org/abs/2310.08132)
- 2023-01-26, **On granularity of prosodic representations in expressive text-to-speech**, Mikolaj Babianski et.al., Paper: [http://arxiv.org/abs/2301.11446](http://arxiv.org/abs/2301.11446)
- 2024-01-01, **Normalization of Lithuanian Text Using Regular Expressions**, Pijus Kasparaitis et.al., Paper: [http://arxiv.org/abs/2312.17660](http://arxiv.org/abs/2312.17660)
- 2022-11-04, **NoreSpeech: Knowledge Distillation based Conditional Diffusion Model for Noise-robust Expressive TTS**, Dongchao Yang et.al., Paper: [http://arxiv.org/abs/2211.02448](http://arxiv.org/abs/2211.02448)
- 2022-09-07, **Non-Standard Vietnamese Word Detection and Normalization for Text-to-Speech**, Huu-Tien Dang et.al., Paper: [http://arxiv.org/abs/2209.02971](http://arxiv.org/abs/2209.02971)
- 2024-01-10, **Noise-robust zero-shot text-to-speech synthesis conditioned on self-supervised speech-representation model with adapters**, Kenichi Fujita et.al., Paper: [http://arxiv.org/abs/2401.05111](http://arxiv.org/abs/2401.05111)
- 2023-11-11, **NewsGPT: ChatGPT Integration for Robot-Reporter**, Abdelhadi Hireche et.al., Paper: [http://arxiv.org/abs/2311.06640](http://arxiv.org/abs/2311.06640), Code: **[https://github.com/aeh1707/NewsGPT_Pepper](https://github.com/aeh1707/NewsGPT_Pepper)**
- 2023-12-11, **Neural Text to Articulate Talk: Deep Text to Audiovisual Speech Synthesis achieving both Auditory and Photo-realism**, Georgios Milis et.al., Paper: [http://arxiv.org/abs/2312.06613](http://arxiv.org/abs/2312.06613), Code: **[https://github.com/g-milis/NEUTART](https://github.com/g-milis/NEUTART)**
- 2023-01-05, **Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers**, Chengyi Wang et.al., Paper: [http://arxiv.org/abs/2301.02111](http://arxiv.org/abs/2301.02111), Code: **[https://github.com/microsoft/unilm](https://github.com/microsoft/unilm)**
- 2024-03-05, **NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and Diffusion Models**, Zeqian Ju et.al., Paper: [http://arxiv.org/abs/2403.03100](http://arxiv.org/abs/2403.03100)
- 2023-05-30, **NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers**, Kai Shen et.al., Paper: [http://arxiv.org/abs/2304.09116](http://arxiv.org/abs/2304.09116)
- 2024-02-02, **Natural language guidance of high-fidelity text-to-speech with synthetic annotations**, Dan Lyth et.al., Paper: [http://arxiv.org/abs/2402.01912](http://arxiv.org/abs/2402.01912)
- 2022-11-17, **NANSY++: Unified Voice Synthesis with Neural Analysis and Synthesis**, Hyeong-Seok Choi et.al., Paper: [http://arxiv.org/abs/2211.09407](http://arxiv.org/abs/2211.09407)
- 2024-01-28, **MunTTS: A Text-to-Speech System for Mundari**, Varun Gumma et.al., Paper: [http://arxiv.org/abs/2401.15579](http://arxiv.org/abs/2401.15579)
- 2023-07-31, **Multilingual context-based pronunciation learning for Text-to-Speech**, Giulia Comini et.al., Paper: [http://arxiv.org/abs/2307.16709](http://arxiv.org/abs/2307.16709)
- 2023-05-25, **Multilingual Text-to-Speech Synthesis for Turkic Languages Using Transliteration**, Rustem Yeshpanov et.al., Paper: [http://arxiv.org/abs/2305.15749](http://arxiv.org/abs/2305.15749), Code: **[https://github.com/is2ai/turkictts](https://github.com/is2ai/turkictts)**
- 2024-01-12, **Multi-Task Learning for Front-End Text Processing in TTS**, Wonjune Kang et.al., Paper: [http://arxiv.org/abs/2401.06321](http://arxiv.org/abs/2401.06321), Code: **[https://github.com/facebookresearch/llama-hd-dataset](https://github.com/facebookresearch/llama-hd-dataset)**
- 2022-09-26, **Multi-Task Adversarial Training Algorithm for Multi-Speaker Neural Text-to-Speech**, Yusuke Nakai et.al., Paper: [http://arxiv.org/abs/2209.12549](http://arxiv.org/abs/2209.12549)
- 2022-11-22, **Multi-Speaker Multi-Style Speech Synthesis with Timbre and Style Disentanglement**, Wei Song et.al., Paper: [http://arxiv.org/abs/2211.00967](http://arxiv.org/abs/2211.00967)
- 2023-04-25, **Multi-Speaker Multi-Lingual VQTTS System for LIMMITS 2023 Challenge**, Chenpeng Du et.al., Paper: [http://arxiv.org/abs/2304.13121](http://arxiv.org/abs/2304.13121)
- 2023-09-11, **Multi-Modal Automatic Prosody Annotation with Contrastive Pretraining of SSWP**, Jinzuomu Zhong et.al., Paper: [http://arxiv.org/abs/2309.05423](http://arxiv.org/abs/2309.05423), Code: **[https://github.com/jzmzhong/Automatic-Prosody-Annotator-with-SSWP-CLAP](https://github.com/jzmzhong/Automatic-Prosody-Annotator-with-SSWP-CLAP)**
- 2023-08-31, **Multi-GradSpeech: Towards Diffusion-based Multi-Speaker Text-to-speech Using Consistent Diffusion Models**, Heyang Xue et.al., Paper: [http://arxiv.org/abs/2308.10428](http://arxiv.org/abs/2308.10428)
- 2023-09-12, **MuLanTTS: The Microsoft Speech Synthesis System for Blizzard Challenge 2023**, Zhihang Xu et.al., Paper: [http://arxiv.org/abs/2309.02743](http://arxiv.org/abs/2309.02743)
- 2023-01-11, **Modelling low-resource accents without accent-specific TTS frontend**, Georgi Tinchev et.al., Paper: [http://arxiv.org/abs/2301.04606](http://arxiv.org/abs/2301.04606)
- 2024-02-14, **MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech**, Shengpeng Ji et.al., Paper: [http://arxiv.org/abs/2402.09378](http://arxiv.org/abs/2402.09378)
- 2022-09-22, **MnTTS: An Open-Source Mongolian Text-to-Speech Synthesis Dataset and Accompanied Baseline**, Yifan Hu et.al., Paper: [http://arxiv.org/abs/2209.10848](http://arxiv.org/abs/2209.10848), Code: **[https://github.com/walker-hyf/mntts](https://github.com/walker-hyf/mntts)**
- 2022-12-11, **MnTTS2: An Open-Source Multi-Speaker Mongolian Text-to-Speech Synthesis Dataset**, Kailin Liang et.al., Paper: [http://arxiv.org/abs/2301.00657](http://arxiv.org/abs/2301.00657), Code: **[https://github.com/ssmlkl/mntts2](https://github.com/ssmlkl/mntts2)**
- 2022-07-04, **Mix and Match: An Empirical Study on Training Corpus Composition for Polyglot Text-To-Speech (TTS)**, Ziyao Zhang et.al., Paper: [http://arxiv.org/abs/2207.01507](http://arxiv.org/abs/2207.01507)
- 2023-12-18, **Minimally-Supervised Speech Synthesis with Conditional Diffusion Model and Language Model: A Comparative Study of Semantic Coding**, Chunyu Qiang et.al., Paper: [http://arxiv.org/abs/2307.15484](http://arxiv.org/abs/2307.15484)
- 2023-08-14, **Miipher: A Robust Speech Restoration Model Integrating Self-Supervised Speech and Text Representations**, Yuma Koizumi et.al., Paper: [http://arxiv.org/abs/2303.01664](http://arxiv.org/abs/2303.01664)
- 2023-06-06, **Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias**, Ziyue Jiang et.al., Paper: [http://arxiv.org/abs/2306.03509](http://arxiv.org/abs/2306.03509)
- 2023-09-28, **Mega-TTS 2: Zero-Shot Text-to-Speech with Arbitrary Length Speech Prompts**, Ziyue Jiang et.al., Paper: [http://arxiv.org/abs/2307.07218](http://arxiv.org/abs/2307.07218)
- 2024-01-23, **Maximizing Data Efficiency for Cross-Lingual TTS Adaptation by Self-Supervised Representation Mixing and Embedding Initialization**, Wei-Ping Huang et.al., Paper: [http://arxiv.org/abs/2402.01692](http://arxiv.org/abs/2402.01692)
- 2023-08-03, **Many-to-Many Spoken Language Translation via Unified Speech and Text Representation Learning with Unit-to-Unit Translation**, Minsu Kim et.al., Paper: [http://arxiv.org/abs/2308.01831](http://arxiv.org/abs/2308.01831), Code: **[https://github.com/choijeongsoo/utut](https://github.com/choijeongsoo/utut)**
- 2023-05-19, **Making More of Little Data: Improving Low-Resource Automatic Speech Recognition Using Data Augmentation**, Martijn Bartelds et.al., Paper: [http://arxiv.org/abs/2305.10951](http://arxiv.org/abs/2305.10951), Code: **[https://github.com/bartelds/asr-augmentation](https://github.com/bartelds/asr-augmentation)**
- 2024-03-04, **Making Flow-Matching-Based Zero-Shot Text-to-Speech Laugh as You Like**, Naoyuki Kanda et.al., Paper: [http://arxiv.org/abs/2402.07383](http://arxiv.org/abs/2402.07383)
- 2023-05-30, **Make-A-Voice: Unified Voice Synthesis With Discrete Representation**, Rongjie Huang et.al., Paper: [http://arxiv.org/abs/2305.19269](http://arxiv.org/abs/2305.19269)
- 2023-05-19, **MParrotTTS: Multilingual Multi-speaker Text to Speech Synthesis in Low Resource Setting**, Neil Shah et.al., Paper: [http://arxiv.org/abs/2305.11926](http://arxiv.org/abs/2305.11926)
- 2024-01-31, **MM-TTS: Multi-modal Prompt based Style Transfer for Expressive Text-to-Speech Synthesis**, Wenhao Guan et.al., Paper: [http://arxiv.org/abs/2312.10687](http://arxiv.org/abs/2312.10687)
- 2024-02-28, **MLAAD: The Multi-Language Audio Anti-Spoofing Dataset**, Nicolas M. Müller et.al., Paper: [http://arxiv.org/abs/2401.09512](http://arxiv.org/abs/2401.09512)
- 2023-07-29, **METTS: Multilingual Emotional Text-to-Speech by Cross-speaker and Cross-lingual Emotion Transfer**, Xinfa Zhu et.al., Paper: [http://arxiv.org/abs/2307.15951](http://arxiv.org/abs/2307.15951)
- 2024-01-15, **MCMChaos: Improvising Rap Music with MCMC Methods and Chaos Theory**, Robert G. Kimelman et.al., Paper: [http://arxiv.org/abs/2401.07967](http://arxiv.org/abs/2401.07967)
- 2023-02-15, **MAC: A unified framework boosting low resource automatic speech recognition**, Zeping Min et.al., Paper: [http://arxiv.org/abs/2302.03498](http://arxiv.org/abs/2302.03498)
- 2023-05-03, **M2-CTTS: End-to-End Multi-scale Multi-modal Conversational Text-to-Speech Synthesis**, Jinlong Xue et.al., Paper: [http://arxiv.org/abs/2305.02269](http://arxiv.org/abs/2305.02269)
- 2022-07-29, **Low-data? No problem: low-resource, language-agnostic conversational text-to-speech via F0-conditioned data augmentation**, Giulia Comini et.al., Paper: [http://arxiv.org/abs/2207.14607](http://arxiv.org/abs/2207.14607)
- 2023-06-16, **Low-Resource Text-to-Speech Using Specific Data and Noise Augmentation**, Kishor Kayyar Lakshminarayana et.al., Paper: [http://arxiv.org/abs/2306.10152](http://arxiv.org/abs/2306.10152)
- 2023-09-29, **Low-Resource Self-Supervised Learning with SSL-Enhanced TTS**, Po-chun Hsu et.al., Paper: [http://arxiv.org/abs/2309.17020](http://arxiv.org/abs/2309.17020)
- 2022-10-21, **Low-Resource Multilingual and Zero-Shot Multispeaker TTS**, Florian Lux et.al., Paper: [http://arxiv.org/abs/2210.12223](http://arxiv.org/abs/2210.12223), Code: **[https://github.com/digitalphonetics/ims-toucan](https://github.com/digitalphonetics/ims-toucan)**
- 2023-01-04, **Low-Resource Mongolian Speech Synthesis Based on Automatic Prosody Annotation**, Xin Yuan et.al., Paper: [http://arxiv.org/abs/2211.09365](http://arxiv.org/abs/2211.09365)
- 2022-12-07, **Low-Resource End-to-end Sanskrit TTS using Tacotron2, WaveGlow and Transfer Learning**, Ankur Debnath et.al., Paper: [http://arxiv.org/abs/2212.03558](http://arxiv.org/abs/2212.03558)
- 2023-03-02, **LiteG2P: A fast, light and high accuracy model for grapheme-to-phoneme conversion**, Chunfeng Wang et.al., Paper: [http://arxiv.org/abs/2303.01086](http://arxiv.org/abs/2303.01086)
- 2023-02-21, **Lightweight and High-Fidelity End-to-End Text-to-Speech with Multi-Band Generation and Inverse Short-Time Fourier Transform**, Masaya Kawamura et.al., Paper: [http://arxiv.org/abs/2210.15975](http://arxiv.org/abs/2210.15975), Code: **[https://github.com/masayakawamura/mb-istft-vits](https://github.com/masayakawamura/mb-istft-vits)**
- 2023-08-31, **LightGrad: Lightweight Diffusion Probabilistic Model for Text-to-Speech**, Jie Chen et.al., Paper: [http://arxiv.org/abs/2308.16569](http://arxiv.org/abs/2308.16569)
- 2023-05-30, **LibriTTS-R: A Restored Multi-Speaker Text-to-Speech Corpus**, Yuma Koizumi et.al., Paper: [http://arxiv.org/abs/2305.18802](http://arxiv.org/abs/2305.18802)
- 2023-09-19, **Leveraging Speech PTM, Text LLM, and Emotional TTS for Speech Emotion Recognition**, Ziyang Ma et.al., Paper: [http://arxiv.org/abs/2309.10294](http://arxiv.org/abs/2309.10294)
- 2023-03-02, **Leveraging Large Text Corpora for End-to-End Speech Summarization**, Kohei Matsuura et.al., Paper: [http://arxiv.org/abs/2303.00978](http://arxiv.org/abs/2303.00978)
- 2023-08-04, **Let's Give a Voice to Conversational Agents in Virtual Reality**, Michele Yin et.al., Paper: [http://arxiv.org/abs/2308.02665](http://arxiv.org/abs/2308.02665), Code: **[https://github.com/sislab-unitn/Let-s-Give-a-Voice-to-Conversational-Agents-in-VR](https://github.com/sislab-unitn/Let-s-Give-a-Voice-to-Conversational-Agents-in-VR)**
- 2023-05-27, **Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining**, Takaaki Saeki et.al., Paper: [http://arxiv.org/abs/2301.12596](http://arxiv.org/abs/2301.12596), Code: **[https://github.com/takaaki-saeki/zm-text-tts](https://github.com/takaaki-saeki/zm-text-tts)**
- 2023-04-04, **Learning to Dub Movies via Hierarchical Prosody Models**, Gaoxiang Cong et.al., Paper: [http://arxiv.org/abs/2212.04054](http://arxiv.org/abs/2212.04054), Code: **[https://github.com/galaxycong/hpmdubbing](https://github.com/galaxycong/hpmdubbing)**
- 2023-12-18, **Learning Speech Representation From Contrastive Token-Acoustic Pretraining**, Chunyu Qiang et.al., Paper: [http://arxiv.org/abs/2309.00424](http://arxiv.org/abs/2309.00424)
- 2023-06-09, **Learning Emotional Representations from Imbalanced Speech Data for Speech Emotion Recognition and Emotional Text-to-Speech**, Shijun Wang et.al., Paper: [http://arxiv.org/abs/2306.05709](http://arxiv.org/abs/2306.05709)
- 2024-02-06, **Learning Arousal-Valence Representation from Categorical Emotion Labels of Speech**, Enting Zhou et.al., Paper: [http://arxiv.org/abs/2311.14816](http://arxiv.org/abs/2311.14816), Code: **[https://github.com/ETZET/SpeechEmotionAVLearning](https://github.com/ETZET/SpeechEmotionAVLearning)**
- 2022-10-17, **LeVoice ASR Systems for the ISCSLP 2022 Intelligent Cockpit Speech Recognition Challenge**, Yan Jia et.al., Paper: [http://arxiv.org/abs/2210.07749](http://arxiv.org/abs/2210.07749)
- 2023-10-11, **LauraGPT: Listen, Attend, Understand, and Regenerate Audio with GPT**, Jiaming Wang et.al., Paper: [http://arxiv.org/abs/2310.04673](http://arxiv.org/abs/2310.04673)
- 2023-05-26, **Laughter Synthesis using Pseudo Phonetic Tokens with a Large-scale In-the-wild Laughter Corpus**, Detai Xin et.al., Paper: [http://arxiv.org/abs/2305.12442](http://arxiv.org/abs/2305.12442), Code: **[https://github.com/aria-k-alethia/laughter-synthesis](https://github.com/aria-k-alethia/laughter-synthesis)**
- 2023-06-05, **Latent Optimal Paths by Gumbel Propagation for Variational Bayesian Dynamic Programming**, Xinlei Niu et.al., Paper: [http://arxiv.org/abs/2306.02568](http://arxiv.org/abs/2306.02568), Code: **[https://github.com/Berthaniu/LatentOptimalPathsBayesianDP](https://github.com/Berthaniu/LatentOptimalPathsBayesianDP)**
- 2024-01-22, **Latent Filling: Latent Space Data Augmentation for Zero-shot Speech Synthesis**, Jae-Sung Bae et.al., Paper: [http://arxiv.org/abs/2310.03538](http://arxiv.org/abs/2310.03538)
- 2023-09-07, **Large-Scale Automatic Audiobook Creation**, Brendan Walsh et.al., Paper: [http://arxiv.org/abs/2309.03926](http://arxiv.org/abs/2309.03926)
- 2022-07-01, **Language Model-Based Emotion Prediction Methods for Emotional Speech Synthesis Systems**, Hyun-Wook Yoon et.al., Paper: [http://arxiv.org/abs/2206.15067](http://arxiv.org/abs/2206.15067)
- 2022-07-11, **LIP: Lightweight Intelligent Preprocessor for meaningful text-to-speech**, Harshvardhan Anand et.al., Paper: [http://arxiv.org/abs/2207.07118](http://arxiv.org/abs/2207.07118)
- 2024-02-05, **LAraBench: Benchmarking Arabic AI with Large Language Models**, Ahmed Abdelali et.al., Paper: [http://arxiv.org/abs/2305.14982](http://arxiv.org/abs/2305.14982)
- 2023-10-09, **JVNV: A Corpus of Japanese Emotional Speech with Verbal Content and Nonverbal Expressions**, Detai Xin et.al., Paper: [http://arxiv.org/abs/2310.06072](http://arxiv.org/abs/2310.06072)
- 2022-12-16, **Investigation of Japanese PnG BERT language model in text-to-speech synthesis for pitch accent language**, Yusuke Yasuda et.al., Paper: [http://arxiv.org/abs/2212.08321](http://arxiv.org/abs/2212.08321)
- 2023-05-07, **Investigating Content-Aware Neural Text-To-Speech MOS Prediction Using Prosodic and Linguistic Features**, Alexandra Vioni et.al., Paper: [http://arxiv.org/abs/2211.00342](http://arxiv.org/abs/2211.00342)
- 2023-07-11, **Interpretable Style Transfer for Text-to-Speech with ControlVAE and Diffusion Bridge**, Wenhao Guan et.al., Paper: [http://arxiv.org/abs/2306.04301](http://arxiv.org/abs/2306.04301)
- 2023-06-25, **InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt**, Dongchao Yang et.al., Paper: [http://arxiv.org/abs/2301.13662](http://arxiv.org/abs/2301.13662), Code: **[https://github.com/yangdongchao/academicodec](https://github.com/yangdongchao/academicodec)**
- 2024-01-03, **Incremental FastPitch: Chunk-based High Quality Text to Speech**, Muyang Du et.al., Paper: [http://arxiv.org/abs/2401.01755](http://arxiv.org/abs/2401.01755)
- 2023-06-28, **Improving the quality of neural TTS using long-form content and multi-speaker multi-style modeling**, Tuomo Raitio et.al., Paper: [http://arxiv.org/abs/2212.10075](http://arxiv.org/abs/2212.10075)
- 2023-07-31, **Improving grapheme-to-phoneme conversion by learning pronunciations from speech recordings**, Manuel Sam Ribeiro et.al., Paper: [http://arxiv.org/abs/2307.16643](http://arxiv.org/abs/2307.16643)
- 2023-11-16, **Improving fairness for spoken language understanding in atypical speech with Text-to-Speech**, Helin Wang et.al., Paper: [http://arxiv.org/abs/2311.10149](http://arxiv.org/abs/2311.10149), Code: **[https://github.com/wanghelin1997/aty-tts](https://github.com/wanghelin1997/aty-tts)**
- 2023-07-30, **Improving TTS for Shanghainese: Addressing Tone Sandhi via Word Segmentation**, Yuanhao Chen et.al., Paper: [http://arxiv.org/abs/2307.16199](http://arxiv.org/abs/2307.16199), Code: **[https://github.com/edward-martyr/shanghainese-tts](https://github.com/edward-martyr/shanghainese-tts)**
- 2022-10-26, **Improving Speech-to-Speech Translation Through Unlabeled Text**, Xuan-Phi Nguyen et.al., Paper: [http://arxiv.org/abs/2210.14514](http://arxiv.org/abs/2210.14514)
- 2022-11-04, **Improving Speech Prosody of Audiobook Text-to-Speech Synthesis with Acoustic and Textual Contexts**, Detai Xin et.al., Paper: [http://arxiv.org/abs/2211.02336](http://arxiv.org/abs/2211.02336)
- 2023-08-31, **Improving Mandarin Prosodic Structure Prediction with Multi-level Contextual Information**, Jie Chen et.al., Paper: [http://arxiv.org/abs/2308.16577](http://arxiv.org/abs/2308.16577)
- 2023-09-22, **Improving Language Model-Based Zero-Shot Text-to-Speech Synthesis with Multi-Scale Acoustic Prompts**, Shun Lei et.al., Paper: [http://arxiv.org/abs/2309.11977](http://arxiv.org/abs/2309.11977)
- 2023-03-09, **Improving Few-Shot Learning for Talking Face System with TTS Data Augmentation**, Qi Chen et.al., Paper: [http://arxiv.org/abs/2303.05322](http://arxiv.org/abs/2303.05322), Code: **[https://github.com/moon0316/t2a](https://github.com/moon0316/t2a)**
- 2022-06-29, **Improving Deliberation by Text-Only and Semi-Supervised Training**, Ke Hu et.al., Paper: [http://arxiv.org/abs/2206.14716](http://arxiv.org/abs/2206.14716)
- 2022-09-02, **Improving Contextual Recognition of Rare Words with an Alternate Spelling Prediction Model**, Jennifer Drexler Fox et.al., Paper: [http://arxiv.org/abs/2209.01250](http://arxiv.org/abs/2209.01250)
- 2023-06-14, **Improving Code-Switching and Named Entity Recognition in ASR with Speech Editing based Data Augmentation**, Zheng Liang et.al., Paper: [http://arxiv.org/abs/2306.08588](http://arxiv.org/abs/2306.08588)
- 2023-11-07, **Improved Child Text-to-Speech Synthesis through Fastpitch-based Transfer Learning**, Rishabh Jain et.al., Paper: [http://arxiv.org/abs/2311.04313](http://arxiv.org/abs/2311.04313), Code: **[https://github.com/c3imaging/child_tts_fastpitch](https://github.com/c3imaging/child_tts_fastpitch)**
- 2023-02-27, **Imaginary Voice: Face-styled Diffusion Model for Text-to-Speech**, Jiyoung Lee et.al., Paper: [http://arxiv.org/abs/2302.13700](http://arxiv.org/abs/2302.13700), Code: **[https://github.com/naver-ai/facetts](https://github.com/naver-ai/facetts)**
- 2022-11-23, **IMaSC -- ICFOSS Malayalam Speech Corpus**, Deepa P Gopinath et.al., Paper: [http://arxiv.org/abs/2211.12796](http://arxiv.org/abs/2211.12796)
- 2022-07-12, **Huqariq: A Multilingual Speech Corpus of Native Languages of Peru for Speech Recognition**, Rodolfo Zevallos et.al., Paper: [http://arxiv.org/abs/2207.05498](http://arxiv.org/abs/2207.05498)
- 2023-06-29, **High-Quality Automatic Voice Over with Accurate Alignment: Supervision through Self-Supervised Discrete Speech Units**, Junchen Lu et.al., Paper: [http://arxiv.org/abs/2306.17005](http://arxiv.org/abs/2306.17005)
- 2023-12-18, **High-Fidelity Speech Synthesis with Minimal Supervision: All Using Diffusion Models**, Chunyu Qiang et.al., Paper: [http://arxiv.org/abs/2309.15512](http://arxiv.org/abs/2309.15512)
- 2023-11-27, **HierSpeech++: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis**, Sang-Hoon Lee et.al., Paper: [http://arxiv.org/abs/2311.12454](http://arxiv.org/abs/2311.12454), Code: **[https://github.com/sh-lee-prml/hierspeechpp](https://github.com/sh-lee-prml/hierspeechpp)**
- 2023-10-07, **HiGNN-TTS: Hierarchical Prosody Modeling with Graph Neural Networks for Expressive Long-form TTS**, Dake Guo et.al., Paper: [http://arxiv.org/abs/2309.13907](http://arxiv.org/abs/2309.13907)
- 2023-09-17, **HiFi-WaveGAN: Generative Adversarial Network with Auxiliary Spectrogram-Phase Loss for High-Fidelity Singing Voice Generation**, Chunhui Wang et.al., Paper: [http://arxiv.org/abs/2210.12740](http://arxiv.org/abs/2210.12740)
- 2022-12-22, **HMM-based data augmentation for E2E systems for building conversational speech synthesis systems**, Ishika Gupta et.al., Paper: [http://arxiv.org/abs/2212.11982](http://arxiv.org/abs/2212.11982)
- 2023-09-15, **HM-Conformer: A Conformer-based audio deepfake detection system with hierarchical pooling and multi-level classification token aggregation methods**, Hyun-seo Shin et.al., Paper: [http://arxiv.org/abs/2309.08208](http://arxiv.org/abs/2309.08208)
- 2023-12-07, **Guided Flows for Generative Modeling and Decision Making**, Qinqing Zheng et.al., Paper: [http://arxiv.org/abs/2311.13443](http://arxiv.org/abs/2311.13443)
- 2023-03-14, **Grad-StyleSpeech: Any-speaker Adaptive Text-to-Speech Synthesis with Diffusion Models**, Minki Kang et.al., Paper: [http://arxiv.org/abs/2211.09383](http://arxiv.org/abs/2211.09383)
- 2022-07-05, **Glow-WaveGAN 2: High-quality Zero-shot Text-to-speech Synthesis and Any-to-any Voice Conversion**, Yi Lei et.al., Paper: [http://arxiv.org/abs/2207.01832](http://arxiv.org/abs/2207.01832)
- 2023-10-25, **Generative Pre-training for Speech with Flow Matching**, Alexander H. Liu et.al., Paper: [http://arxiv.org/abs/2310.16338](http://arxiv.org/abs/2310.16338)
- 2023-01-10, **Generative Emotional AI for Speech Emotion Recognition: The Case for Synthetic Emotional Speech Augmentation**, Abdullah Shahid et.al., Paper: [http://arxiv.org/abs/2301.03751](http://arxiv.org/abs/2301.03751)
- 2023-10-14, **Generative Adversarial Training for Text-to-Speech Synthesis Based on Raw Phonetic Input and Explicit Prosody Modelling**, Tiberiu Boros et.al., Paper: [http://arxiv.org/abs/2310.09636](http://arxiv.org/abs/2310.09636), Code: **[https://github.com/tiberiu44/TTS-Cube](https://github.com/tiberiu44/TTS-Cube)**
- 2023-02-08, **Generating Synthetic Speech from SpokenVocab for Speech Translation**, Jinming Zhao et.al., Paper: [http://arxiv.org/abs/2210.08174](http://arxiv.org/abs/2210.08174), Code: **[https://github.com/mingzi151/spokenvocab](https://github.com/mingzi151/spokenvocab)**
- 2023-06-11, **Generating Multilingual Gender-Ambiguous Text-to-Speech Voices**, Konstantinos Markopoulos et.al., Paper: [http://arxiv.org/abs/2211.00375](http://arxiv.org/abs/2211.00375)
- 2023-08-24, **Generalizable Zero-Shot Speaker Adaptive Speech Synthesis with Disentangled Representations**, Wenbin Wang et.al., Paper: [http://arxiv.org/abs/2308.13007](http://arxiv.org/abs/2308.13007)
- 2023-06-27, **GenerTTS: Pronunciation Disentanglement for Timbre and Style Generalization in Cross-Lingual Text-to-Speech**, Yahuan Cong et.al., Paper: [http://arxiv.org/abs/2306.15304](http://arxiv.org/abs/2306.15304)
- 2023-09-11, **GRASS: Unified Generation Model for Speech-to-Semantic Tasks**, Aobo Xia et.al., Paper: [http://arxiv.org/abs/2309.02780](http://arxiv.org/abs/2309.02780)
- 2023-10-07, **FunCodec: A Fundamental, Reproducible and Integrable Open-source Toolkit for Neural Speech Codec**, Zhihao Du et.al., Paper: [http://arxiv.org/abs/2309.07405](http://arxiv.org/abs/2309.07405), Code: **[https://github.com/alibaba-damo-academy/funcodec](https://github.com/alibaba-damo-academy/funcodec)**
- 2024-02-01, **Frame-Wise Breath Detection with Self-Training: An Exploration of Enhancing Breath Naturalness in Text-to-Speech**, Dong Yang et.al., Paper: [http://arxiv.org/abs/2402.00288](http://arxiv.org/abs/2402.00288)
- 2023-03-08, **FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model**, Ruiqing Xue et.al., Paper: [http://arxiv.org/abs/2303.02939](http://arxiv.org/abs/2303.02939)
- 2023-03-11, **Fine-grained Emotional Control of Text-To-Speech: Learning To Rank Inter- And Intra-Class Emotion Intensities**, Shijun Wang et.al., Paper: [http://arxiv.org/abs/2303.01508](http://arxiv.org/abs/2303.01508)
- 2024-03-11, **Fewer-token Neural Speech Codec with Time-invariant Codes**, Yong Ren et.al., Paper: [http://arxiv.org/abs/2310.00014](http://arxiv.org/abs/2310.00014)
- 2023-09-16, **FastGraphTTS: An Ultrafast Syntax-Aware Speech Synthesis Framework**, Jianzong Wang et.al., Paper: [http://arxiv.org/abs/2309.08837](http://arxiv.org/abs/2309.08837)
- 2023-05-18, **FastFit: Towards Real-Time Iterative Neural Vocoder by Replacing U-Net Encoder With Multiple STFTs**, Won Jang et.al., Paper: [http://arxiv.org/abs/2305.10823](http://arxiv.org/abs/2305.10823)
- 2023-02-13, **Fast and small footprint Hybrid HMM-HiFiGAN based system for speech synthesis in Indian languages**, Sudhanshu Srivastava et.al., Paper: [http://arxiv.org/abs/2302.06227](http://arxiv.org/abs/2302.06227)
- 2022-09-29, **Facial Landmark Predictions with Applications to Metaverse**, Qiao Han et.al., Paper: [http://arxiv.org/abs/2209.14698](http://arxiv.org/abs/2209.14698), Code: **[https://github.com/sweatybridge/text-to-anime](https://github.com/sweatybridge/text-to-anime)**
- 2023-09-25, **Face-StyleSpeech: Improved Face-to-Voice latent mapping for Natural Zero-shot Speech Synthesis from a Face Image**, Minki Kang et.al., Paper: [http://arxiv.org/abs/2311.05844](http://arxiv.org/abs/2311.05844)
- 2022-10-27, **FCTalker: Fine and Coarse Grained Context Modeling for Expressive Conversational Speech Synthesis**, Yifan Hu et.al., Paper: [http://arxiv.org/abs/2210.15360](http://arxiv.org/abs/2210.15360), Code: **[https://github.com/walker-hyf/fctalker](https://github.com/walker-hyf/fctalker)**
- 2023-12-19, **External Knowledge Augmented Polyphone Disambiguation Using Large Language Model**, Chen Li et.al., Paper: [http://arxiv.org/abs/2312.11920](http://arxiv.org/abs/2312.11920)
- 2024-02-29, **Extending Multilingual Speech Synthesis to 100+ Languages without Transcribed Data**, Takaaki Saeki et.al., Paper: [http://arxiv.org/abs/2402.18932](http://arxiv.org/abs/2402.18932)
- 2022-06-28, **Expressive, Variable, and Controllable Duration Modelling in TTS**, Ammar Abbas et.al., Paper: [http://arxiv.org/abs/2206.14165](http://arxiv.org/abs/2206.14165)
- 2023-09-02, **Expressive paragraph text-to-speech synthesis with multi-step variational autoencoder**, Xuyuan Li et.al., Paper: [http://arxiv.org/abs/2308.13365](http://arxiv.org/abs/2308.13365)
- 2023-11-02, **Expressive TTS Driven by Natural Language Prompts Using Few Human Annotations**, Hanglei Zhang et.al., Paper: [http://arxiv.org/abs/2311.01260](http://arxiv.org/abs/2311.01260)
- 2023-06-21, **Expressive Machine Dubbing Through Phrase-level Cross-lingual Prosody Transfer**, Jakub Swiatkowski et.al., Paper: [http://arxiv.org/abs/2306.11662](http://arxiv.org/abs/2306.11662)
- 2023-09-19, **Exploring Speech Enhancement for Low-resource Speech Synthesis**, Zhaoheng Ni et.al., Paper: [http://arxiv.org/abs/2309.10795](http://arxiv.org/abs/2309.10795)
- 2022-10-27, **Explicit Intensity Control for Accented Text-to-speech**, Rui Liu et.al., Paper: [http://arxiv.org/abs/2210.15364](http://arxiv.org/abs/2210.15364)
- 2023-05-25, **Evaluating and reducing the distance between synthetic and real speech distributions**, Christoph Minixhofer et.al., Paper: [http://arxiv.org/abs/2211.16049](http://arxiv.org/abs/2211.16049)
- 2024-01-07, **Evaluating and Personalizing User-Perceived Quality of Text-to-Speech Voices for Delivering Mindfulness Meditation with Different Physical Embodiments**, Zhonghao Shi et.al., Paper: [http://arxiv.org/abs/2401.03581](http://arxiv.org/abs/2401.03581)
- 2023-10-01, **Evaluating Speech Synthesis by Training Recognizers on Synthetic Speech**, Dareen Alharthi et.al., Paper: [http://arxiv.org/abs/2310.00706](http://arxiv.org/abs/2310.00706)
- 2023-03-02, **Evaluating Parameter-Efficient Transfer Learning Approaches on SURE Benchmark for Speech Understanding**, Yingting Li et.al., Paper: [http://arxiv.org/abs/2303.03267](http://arxiv.org/abs/2303.03267), Code: **[https://github.com/declare-lab/speech-adapters](https://github.com/declare-lab/speech-adapters)**
- 2023-04-03, **Ensemble prosody prediction for expressive speech synthesis**, Tian Huey Teh et.al., Paper: [http://arxiv.org/abs/2304.00714](http://arxiv.org/abs/2304.00714)
- 2024-02-05, **Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations**, Álvaro Martín-Cortinas et.al., Paper: [http://arxiv.org/abs/2402.03407](http://arxiv.org/abs/2402.03407)
- 2023-04-10, **Enhancing Speech-to-Speech Translation with Multiple TTS Targets**, Jiatong Shi et.al., Paper: [http://arxiv.org/abs/2304.04618](http://arxiv.org/abs/2304.04618)
- 2022-07-12, **End-to-end speech recognition modeling from de-identified data**, Martin Flechl et.al., Paper: [http://arxiv.org/abs/2207.05469](http://arxiv.org/abs/2207.05469)
- 2024-01-11, **End to end Hindi to English speech conversion using Bark, mBART and a finetuned XLSR Wav2Vec2**, Aniket Tathe et.al., Paper: [http://arxiv.org/abs/2401.06183](http://arxiv.org/abs/2401.06183)
- 2024-02-16, **Empowering Communication: Speech Technology for Indian and Western Accents through AI-powered Speech Synthesis**, Vinotha R et.al., Paper: [http://arxiv.org/abs/2401.11771](http://arxiv.org/abs/2401.11771)
- 2023-02-21, **Emphasizing Unseen Words: New Vocabulary Acquisition for End-to-End Speech Recognition**, Leyuan Qu et.al., Paper: [http://arxiv.org/abs/2302.09723](http://arxiv.org/abs/2302.09723)
- 2023-09-21, **Emotion-Aware Prosodic Phrasing for Expressive Text-to-Speech**, Rui Liu et.al., Paper: [http://arxiv.org/abs/2309.11724](http://arxiv.org/abs/2309.11724), Code: **[https://github.com/ai-s2-lab/emopp](https://github.com/ai-s2-lab/emopp)**
- 2023-06-28, **EmoSpeech: Guiding FastSpeech2 Towards Emotional Text to Speech**, Daria Diatlova et.al., Paper: [http://arxiv.org/abs/2307.00024](http://arxiv.org/abs/2307.00024), Code: **[https://github.com/deepvk/emospeech](https://github.com/deepvk/emospeech)**
- 2023-06-01, **EmoMix: Emotion Mixing via Diffusion Models for Emotional Speech Synthesis**, Haobin Tang et.al., Paper: [http://arxiv.org/abs/2306.00648](http://arxiv.org/abs/2306.00648)
- 2023-02-16, **EmoDiff: Intensity Controllable Emotional Text-to-Speech with Soft-Label Guidance**, Yiwei Guo et.al., Paper: [http://arxiv.org/abs/2211.09496](http://arxiv.org/abs/2211.09496)
- 2023-04-16, **Efficiently Trained Low-Resource Mongolian Text-to-Speech System Based On FullConv-TTS**, Ziqi Liang et.al., Paper: [http://arxiv.org/abs/2211.01948](http://arxiv.org/abs/2211.01948)
- 2023-05-23, **EfficientSpeech: An On-Device Text to Speech Model**, Rowel Atienza et.al., Paper: [http://arxiv.org/abs/2305.13905](http://arxiv.org/abs/2305.13905), Code: **[https://github.com/roatienza/efficientspeech](https://github.com/roatienza/efficientspeech)**
- 2024-02-22, **Efficient data selection employing Semantic Similarity-based Graph Structures for model training**, Roxana Petcu et.al., Paper: [http://arxiv.org/abs/2402.14888](http://arxiv.org/abs/2402.14888)
- 2022-12-05, **Efficient Incremental Text-to-Speech on GPUs**, Muyang Du et.al., Paper: [http://arxiv.org/abs/2211.13939](http://arxiv.org/abs/2211.13939)
- 2022-12-04, **ERNIE-SAT: Speech and Text Joint Pretraining for Cross-Lingual Multi-Speaker Text-to-Speech**, Xiaoran Fan et.al., Paper: [http://arxiv.org/abs/2211.03545](http://arxiv.org/abs/2211.03545), Code: **[https://github.com/PaddlePaddle/PaddleSpeech](https://github.com/PaddlePaddle/PaddleSpeech)**
- 2022-09-22, **EPIC TTS Models: Empirical Pruning Investigations Characterizing Text-To-Speech Models**, Perry Lam et.al., Paper: [http://arxiv.org/abs/2209.10890](http://arxiv.org/abs/2209.10890)
- 2023-05-25, **EMNS /Imz/ Corpus: An emotive single-speaker dataset for narrative storytelling in games, television and graphic novels**, Kari Ali Noriy et.al., Paper: [http://arxiv.org/abs/2305.13137](http://arxiv.org/abs/2305.13137), Code: **[https://github.com/knoriy/emns-dct](https://github.com/knoriy/emns-dct)**
- 2024-01-14, **ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided Sequence Reordering**, Yakun Song et.al., Paper: [http://arxiv.org/abs/2401.07333](http://arxiv.org/abs/2401.07333)
- 2023-11-02, **E3 TTS: Easy End-to-End Diffusion-based Text to Speech**, Yuan Gao et.al., Paper: [http://arxiv.org/abs/2311.00945](http://arxiv.org/abs/2311.00945)
- 2023-02-27, **Duration-aware pause insertion using pre-trained language model for multi-speaker text-to-speech**, Dong Yang et.al., Paper: [http://arxiv.org/abs/2302.13652](http://arxiv.org/abs/2302.13652)
- 2023-09-22, **DurIAN-E: Duration Informed Attention Network For Expressive Text-to-Speech Synthesis**, Yu Gu et.al., Paper: [http://arxiv.org/abs/2309.12792](http://arxiv.org/abs/2309.12792)
- 2023-01-02, **Dreamento: an open-source dream engineering toolbox for sleep EEG wearables**, Mahdad Jafarzadeh Esfahani et.al., Paper: [http://arxiv.org/abs/2207.03977](http://arxiv.org/abs/2207.03977), Code: **[https://github.com/dreamento/dreamento](https://github.com/dreamento/dreamento)**
- 2023-03-07, **Do Prosody Transfer Models Transfer Prosody?**, Atli Thor Sigurgeirsson et.al., Paper: [http://arxiv.org/abs/2303.04289](http://arxiv.org/abs/2303.04289)
- 2023-09-15, **Diversity-based core-set selection for text-to-speech with linguistic and acoustic features**, Kentaro Seki et.al., Paper: [http://arxiv.org/abs/2309.08127](http://arxiv.org/abs/2309.08127)
- 2023-05-26, **DisfluencyFixer: A tool to enhance Language Learning through Speech To Speech Disfluency Correction**, Vineet Bhat et.al., Paper: [http://arxiv.org/abs/2305.16957](http://arxiv.org/abs/2305.16957)
- 2023-09-14, **Direct Text to Speech Translation System using Acoustic Units**, Victoria Mingote et.al., Paper: [http://arxiv.org/abs/2309.07478](http://arxiv.org/abs/2309.07478)
- 2023-09-30, **Diffusion-Based Mel-Spectrogram Enhancement for Personalized Speech Synthesis with Found Data**, Yusheng Tian et.al., Paper: [http://arxiv.org/abs/2305.10891](http://arxiv.org/abs/2305.10891), Code: **[https://github.com/dmse4tts/dmse4tts](https://github.com/dmse4tts/dmse4tts)**
- 2023-04-11, **Diffusion Models for Constrained Domains**, Nic Fishman et.al., Paper: [http://arxiv.org/abs/2304.05364](http://arxiv.org/abs/2304.05364)
- 2023-04-23, **DiffVoice: Text-to-Speech with Latent Diffusion**, Zhijun Liu et.al., Paper: [http://arxiv.org/abs/2304.11750](http://arxiv.org/abs/2304.11750)
- 2023-07-31, **DiffProsody: Diffusion-based Latent Prosody Generation for Expressive Speech Synthesis with Prosody Conditional Adversarial Training**, Hyung-Seok Oh et.al., Paper: [http://arxiv.org/abs/2307.16549](http://arxiv.org/abs/2307.16549), Code: **[https://github.com/hsoh0306/diffprosody](https://github.com/hsoh0306/diffprosody)**
- 2023-09-02, **DiCLET-TTS: Diffusion Model based Cross-lingual Emotion Transfer for Text-to-Speech -- A Study between English and Mandarin**, Tao Li et.al., Paper: [http://arxiv.org/abs/2309.00883](http://arxiv.org/abs/2309.00883)
- 2022-07-11, **DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders**, Yanqing Liu et.al., Paper: [http://arxiv.org/abs/2207.04646](http://arxiv.org/abs/2207.04646)
- 2024-01-19, **Data-driven grapheme-to-phoneme representations for a lexicon-free text-to-speech**, Abhinav Garg et.al., Paper: [http://arxiv.org/abs/2401.10465](http://arxiv.org/abs/2401.10465)
- 2024-02-20, **Data Redaction from Conditional Generative Models**, Zhifeng Kong et.al., Paper: [http://arxiv.org/abs/2305.11351](http://arxiv.org/abs/2305.11351)
- 2023-08-09, **Data Player: Automatic Generation of Data Videos with Narration-Animation Interplay**, Leixian Shen et.al., Paper: [http://arxiv.org/abs/2308.04703](http://arxiv.org/abs/2308.04703)
- 2023-11-18, **Data Center Audio/Video Intelligence on Device (DAVID) -- An Edge-AI Platform for Smart-Toys**, Gabriel Cosache et.al., Paper: [http://arxiv.org/abs/2311.11030](http://arxiv.org/abs/2311.11030)
- 2024-02-22, **Daisy-TTS: Simulating Wider Spectrum of Emotions via Prosody Embedding Decomposition**, Rendi Chevi et.al., Paper: [http://arxiv.org/abs/2402.14523](http://arxiv.org/abs/2402.14523)
- 2023-03-13, **DailyTalk: Spoken Dialogue Dataset for Conversational Text-to-Speech**, Keon Lee et.al., Paper: [http://arxiv.org/abs/2207.01063](http://arxiv.org/abs/2207.01063), Code: **[https://github.com/keonlee9420/DailyTalk](https://github.com/keonlee9420/DailyTalk)**
- 2023-03-01, **DTW-SiameseNet: Dynamic Time Warped Siamese Network for Mispronunciation Detection and Correction**, Raviteja Anantha et.al., Paper: [http://arxiv.org/abs/2303.00171](http://arxiv.org/abs/2303.00171)
- 2023-05-28, **DSPGAN: a GAN-based universal vocoder for high-fidelity TTS by time-frequency domain supervision from DSP**, Kun Song et.al., Paper: [http://arxiv.org/abs/2211.01087](http://arxiv.org/abs/2211.01087)
- 2023-06-25, **DSE-TTS: Dual Speaker Embedding for Cross-Lingual Text-to-Speech**, Sen Liu et.al., Paper: [http://arxiv.org/abs/2306.14145](http://arxiv.org/abs/2306.14145)
- 2024-02-02, **DQR-TTS: Semi-supervised Text-to-speech Synthesis with Dynamic Quantized Representation**, Jianzong Wang et.al., Paper: [http://arxiv.org/abs/2311.07965](http://arxiv.org/abs/2311.07965)
- 2023-10-23, **DPP-TTS: Diversifying prosodic features of speech via determinantal point processes**, Seongho Joo et.al., Paper: [http://arxiv.org/abs/2310.14663](http://arxiv.org/abs/2310.14663)
- 2023-09-13, **DCTTS: Discrete Diffusion Model with Contrastive Learning for Text-to-speech Generation**, Zhichao Wu et.al., Paper: [http://arxiv.org/abs/2309.06787](http://arxiv.org/abs/2309.06787)
- 2023-12-22, **Crowdsourced and Automatic Speech Prominence Estimation**, Max Morrison et.al., Paper: [http://arxiv.org/abs/2310.08464](http://arxiv.org/abs/2310.08464), Code: **[https://github.com/reseval/reseval](https://github.com/reseval/reseval)**
- 2023-06-12, **CrossSpeech: Speaker-independent Acoustic Representation for Cross-lingual Speech Synthesis**, Ji-Hoon Kim et.al., Paper: [http://arxiv.org/abs/2302.14370](http://arxiv.org/abs/2302.14370)
- 2023-03-15, **Cross-speaker Emotion Transfer by Manipulating Speech Style Latents**, Suhee Jo et.al., Paper: [http://arxiv.org/abs/2303.08329](http://arxiv.org/abs/2303.08329)
- 2024-02-27, **Cross-lingual Text-To-Speech with Flow-based Voice Conversion for Improved Pronunciation**, Nikolaos Ellinas et.al., Paper: [http://arxiv.org/abs/2210.17264](http://arxiv.org/abs/2210.17264)
- 2023-09-15, **Cross-lingual Knowledge Distillation via Flow-based Voice Conversion for Robust Polyglot Text-To-Speech**, Dariusz Piotrowski et.al., Paper: [http://arxiv.org/abs/2309.08255](http://arxiv.org/abs/2309.08255)
- 2023-09-08, **Cross-Utterance Conditioned VAE for Speech Generation**, Yang Li et.al., Paper: [http://arxiv.org/abs/2309.04156](http://arxiv.org/abs/2309.04156)
- 2023-06-05, **Cross-Lingual Transfer Learning for Phrase Break Prediction with Multilingual Language Model**, Hoyeon Lee et.al., Paper: [http://arxiv.org/abs/2306.02579](http://arxiv.org/abs/2306.02579)
- 2023-12-22, **Creating New Voices using Normalizing Flows**, Piotr Bilinski et.al., Paper: [http://arxiv.org/abs/2312.14569](http://arxiv.org/abs/2312.14569)
- 2022-10-26, **Cover Reproducible Steganography via Deep Generative Models**, Kejiang Chen et.al., Paper: [http://arxiv.org/abs/2210.14632](http://arxiv.org/abs/2210.14632)
- 2023-03-14, **Controlling High-Dimensional Data With Sparse Input**, Dan Andrei Iliescu et.al., Paper: [http://arxiv.org/abs/2303.09446](http://arxiv.org/abs/2303.09446)
- 2022-11-29, **Controllable speech synthesis by learning discrete phoneme-level prosodic representations**, Nikolaos Ellinas et.al., Paper: [http://arxiv.org/abs/2211.16307](http://arxiv.org/abs/2211.16307)
- 2022-07-13, **Controllable and Lossless Non-Autoregressive End-to-End Text-to-Speech**, Zhengxi Liu et.al., Paper: [http://arxiv.org/abs/2207.06088](http://arxiv.org/abs/2207.06088)
- 2023-09-19, **Controllable Speaking Styles Using a Large Language Model**, Atli Thor Sigurgeirsson et.al., Paper: [http://arxiv.org/abs/2305.10321](http://arxiv.org/abs/2305.10321)
- 2023-07-13, **Controllable Emphasis with zero data for text-to-speech**, Arnaud Joly et.al., Paper: [http://arxiv.org/abs/2307.07062](http://arxiv.org/abs/2307.07062)
- 2022-09-22, **Controllable Accented Text-to-Speech Synthesis**, Rui Liu et.al., Paper: [http://arxiv.org/abs/2209.10804](http://arxiv.org/abs/2209.10804)
- 2022-11-26, **Contextual Expressive Text-to-Speech**, Jianhong Tu et.al., Paper: [http://arxiv.org/abs/2211.14548](http://arxiv.org/abs/2211.14548)
- 2023-10-07, **ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading**, Yujia Xiao et.al., Paper: [http://arxiv.org/abs/2307.00782](http://arxiv.org/abs/2307.00782)
- 2023-04-13, **Context-aware Coherent Speaking Style Prediction with Hierarchical Transformers for Audiobook Speech Synthesis**, Shun Lei et.al., Paper: [http://arxiv.org/abs/2304.06359](http://arxiv.org/abs/2304.06359)
- 2022-07-02, **Computer-assisted Pronunciation Training -- Speech synthesis is almost all you need**, Daniel Korzekwa et.al., Paper: [http://arxiv.org/abs/2207.00774](http://arxiv.org/abs/2207.00774)
- 2023-07-31, **Comparing normalizing flows and diffusion models for prosody and acoustic modelling in text-to-speech**, Guangyan Zhang et.al., Paper: [http://arxiv.org/abs/2307.16679](http://arxiv.org/abs/2307.16679)
- 2023-10-08, **Comparative Analysis of Transfer Learning in Deep Learning Text-to-Speech Models on a Few-Shot, Low-Resource, Customized Dataset**, Ze Liu et.al., Paper: [http://arxiv.org/abs/2310.04982](http://arxiv.org/abs/2310.04982)
- 2023-05-20, **ComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource Scenarios**, Yuyue Wang et.al., Paper: [http://arxiv.org/abs/2305.12200](http://arxiv.org/abs/2305.12200)
- 2022-10-31, **Combining Automatic Speaker Verification and Prosody Analysis for Synthetic Speech Detection**, Luigi Attorresi et.al., Paper: [http://arxiv.org/abs/2210.17222](http://arxiv.org/abs/2210.17222)
- 2023-03-20, **Code-Switching Text Generation and Injection in Mandarin-English ASR**, Haibin Yu et.al., Paper: [http://arxiv.org/abs/2303.10949](http://arxiv.org/abs/2303.10949)
- 2023-12-02, **Code-Mixed Text to Speech Synthesis under Low-Resource Constraints**, Raviraj Joshi et.al., Paper: [http://arxiv.org/abs/2312.01103](http://arxiv.org/abs/2312.01103)
- 2023-09-24, **Coco-Nut: Corpus of Japanese Utterance and Voice Characteristics Description for Prompt-based Control**, Aya Watanabe et.al., Paper: [http://arxiv.org/abs/2309.13509](http://arxiv.org/abs/2309.13509)
- 2023-10-29, **CoMoSpeech: One-Step Speech and Singing Voice Synthesis via Consistency Model**, Zhen Ye et.al., Paper: [http://arxiv.org/abs/2305.06908](http://arxiv.org/abs/2305.06908), Code: **[https://github.com/zhenye234/CoMoSpeech](https://github.com/zhenye234/CoMoSpeech)**
- 2023-02-28, **ClArTTS: An Open-Source Classical Arabic Text-to-Speech Corpus**, Ajinkya Kulkarni et.al., Paper: [http://arxiv.org/abs/2303.00069](http://arxiv.org/abs/2303.00069)
- 2023-11-12, **ChatAnything: Facetime Chat with LLM-Enhanced Personas**, Yilin Zhao et.al., Paper: [http://arxiv.org/abs/2311.06772](http://arxiv.org/abs/2311.06772)
- 2023-11-07, **Character-Level Bangla Text-to-IPA Transcription Using Transformer Architecture with Sequence Alignment**, Jakir Hasan et.al., Paper: [http://arxiv.org/abs/2311.03792](http://arxiv.org/abs/2311.03792)
- 2022-10-12, **Can we use Common Voice to train a Multi-Speaker TTS system?**, Sewade Ogun et.al., Paper: [http://arxiv.org/abs/2210.06370](http://arxiv.org/abs/2210.06370)
- 2023-03-21, **Can Knowledge of End-to-End Text-to-Speech Models Improve Neural MIDI-to-Audio Synthesis Systems?**, Xuan Shi et.al., Paper: [http://arxiv.org/abs/2211.13868](http://arxiv.org/abs/2211.13868), Code: **[https://github.com/nii-yamagishilab/midi-to-audio](https://github.com/nii-yamagishilab/midi-to-audio)**
- 2023-06-16, **CML-TTS A Multilingual Dataset for Speech Synthesis in Low-Resource Languages**, Frederico S. Oliveira et.al., Paper: [http://arxiv.org/abs/2306.10097](http://arxiv.org/abs/2306.10097)
- 2023-05-18, **CLAPSpeech: Learning Prosody from Text Context with Contrastive Language-Audio Pre-training**, Zhenhui Ye et.al., Paper: [http://arxiv.org/abs/2305.10763](http://arxiv.org/abs/2305.10763)
- 2023-08-30, **CALM: Contrastive Cross-modal Speaking Style Modeling for Expressive Text-to-Speech Synthesis**, Yi Meng et.al., Paper: [http://arxiv.org/abs/2308.16021](http://arxiv.org/abs/2308.16021)
- 2022-07-01, **Building African Voices**, Perez Ogayo et.al., Paper: [http://arxiv.org/abs/2207.00688](http://arxiv.org/abs/2207.00688), Code: **[https://github.com/neulab/africanvoices](https://github.com/neulab/africanvoices)**
- 2024-03-04, **Brilla AI: AI Contestant for the National Science and Maths Quiz**, George Boateng et.al., Paper: [http://arxiv.org/abs/2403.01699](http://arxiv.org/abs/2403.01699)
- 2023-12-30, **Boosting Large Language Model for Speech Synthesis: An Empirical Study**, Hongkun Hao et.al., Paper: [http://arxiv.org/abs/2401.00246](http://arxiv.org/abs/2401.00246)
- 2022-07-07, **BibleTTS: a large, high-fidelity, multilingual, and uniquely African speech corpus**, Josh Meyer et.al., Paper: [http://arxiv.org/abs/2207.03546](http://arxiv.org/abs/2207.03546), Code: **[https://github.com/alpoktem/bible2speechdb](https://github.com/alpoktem/bible2speechdb)**
- 2024-01-09, **BiSinger: Bilingual Singing Voice Synthesis**, Huali Zhou et.al., Paper: [http://arxiv.org/abs/2309.14089](http://arxiv.org/abs/2309.14089)
- 2023-05-23, **Better speech synthesis through scaling**, James Betker et.al., Paper: [http://arxiv.org/abs/2305.07243](http://arxiv.org/abs/2305.07243), Code: **[https://github.com/neonbjb/tortoise-tts](https://github.com/neonbjb/tortoise-tts)**
- 2023-05-25, **Betray Oneself: A Novel Audio DeepFake Detection Model via Mono-to-Stereo Conversion**, Rui Liu et.al., Paper: [http://arxiv.org/abs/2305.16353](http://arxiv.org/abs/2305.16353), Code: **[https://github.com/ai-s2-lab/m2s-add](https://github.com/ai-s2-lab/m2s-add)**
- 2024-01-22, **Benchmarking Large Multimodal Models against Common Corruptions**, Jiawei Zhang et.al., Paper: [http://arxiv.org/abs/2401.11943](http://arxiv.org/abs/2401.11943), Code: **[https://github.com/sail-sg/mmcbench](https://github.com/sail-sg/mmcbench)**
- 2024-02-19, **Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting**, Haolin Chen et.al., Paper: [http://arxiv.org/abs/2402.12220](http://arxiv.org/abs/2402.12220)
- 2022-11-17, **Back-Translation-Style Data Augmentation for Mandarin Chinese Polyphone Disambiguation**, Chunyu Qiang et.al., Paper: [http://arxiv.org/abs/2211.09495](http://arxiv.org/abs/2211.09495)
- 2022-07-04, **BERT, can HE predict contrastive focus? Predicting and controlling prominence in neural TTS using a language model**, Brooke Stephenson et.al., Paper: [http://arxiv.org/abs/2207.01718](http://arxiv.org/abs/2207.01718)
- 2022-12-11, **BASPRO: a balanced script producer for speech corpus collection based on the genetic algorithm**, Yu-Wen Chen et.al., Paper: [http://arxiv.org/abs/2301.04120](http://arxiv.org/abs/2301.04120), Code: **[https://github.com/yuwchen/baspro](https://github.com/yuwchen/baspro)**
- 2024-02-15, **BASE TTS: Lessons from building a billion-parameter Text-to-Speech model on 100K hours of data**, Mateusz Łajszczak et.al., Paper: [http://arxiv.org/abs/2402.08093](http://arxiv.org/abs/2402.08093)
- 2023-05-24, **Autovocoder: Fast Waveform Generation from a Learned Speech Representation using Differentiable Digital Signal Processing**, Jacob J Webber et.al., Paper: [http://arxiv.org/abs/2211.06989](http://arxiv.org/abs/2211.06989)
- 2023-02-28, **Automatic Heteronym Resolution Pipeline Using RAD-TTS Aligners**, Jocelyn Huang et.al., Paper: [http://arxiv.org/abs/2302.14523](http://arxiv.org/abs/2302.14523)
- 2023-05-29, **Automatic Evaluation of Turn-taking Cues in Conversational Speech Synthesis**, Erik Ekstedt et.al., Paper: [http://arxiv.org/abs/2305.17971](http://arxiv.org/abs/2305.17971)
- 2022-07-01, **Automatic Evaluation of Speaker Similarity**, Deja Kamil et.al., Paper: [http://arxiv.org/abs/2207.00344](http://arxiv.org/abs/2207.00344)
- 2023-09-17, **Augmenting text for spoken language understanding with Large Language Models**, Roshan Sharma et.al., Paper: [http://arxiv.org/abs/2309.09390](http://arxiv.org/abs/2309.09390)
- 2023-09-09, **AudioLDM 2: Learning Holistic Audio Generation with Self-supervised Pretraining**, Haohe Liu et.al., Paper: [http://arxiv.org/abs/2308.05734](http://arxiv.org/abs/2308.05734), Code: **[https://github.com/haoheliu/AudioLDM2](https://github.com/haoheliu/AudioLDM2)**
- 2023-10-14, **Attentive Multi-Layer Perceptron for Non-autoregressive Generation**, Shuyang Jiang et.al., Paper: [http://arxiv.org/abs/2310.09512](http://arxiv.org/abs/2310.09512), Code: **[https://github.com/shark-nlp/attentivemlp](https://github.com/shark-nlp/attentivemlp)**
- 2024-03-05, **AttentionStitch: How Attention Solves the Speech Editing Problem**, Antonios Alexos et.al., Paper: [http://arxiv.org/abs/2403.04804](http://arxiv.org/abs/2403.04804)
- 2024-03-07, **Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation**, Sai Akarsh et.al., Paper: [http://arxiv.org/abs/2403.04178](http://arxiv.org/abs/2403.04178)
- 2023-12-18, **Assisting Blind People Using Object Detection with Vocal Feedback**, Heba Najm et.al., Paper: [http://arxiv.org/abs/2401.01362](http://arxiv.org/abs/2401.01362)
- 2023-07-07, **Artificial Eye for the Blind**, Abhinav Benagi et.al., Paper: [http://arxiv.org/abs/2308.00801](http://arxiv.org/abs/2308.00801)
- 2023-04-07, **ArmanTTS single-speaker Persian dataset**, Mohammd Hasan Shamgholi et.al., Paper: [http://arxiv.org/abs/2304.03585](http://arxiv.org/abs/2304.03585)
- 2023-03-29, **AraSpot: Arabic Spoken Command Spotting**, Mahmoud Salhab et.al., Paper: [http://arxiv.org/abs/2303.16621](http://arxiv.org/abs/2303.16621), Code: **[https://github.com/msalhab96/araspot](https://github.com/msalhab96/araspot)**
- 2023-10-25, **ArTST: Arabic Text and Speech Transformer**, Hawau Olamide Toyin et.al., Paper: [http://arxiv.org/abs/2310.16621](http://arxiv.org/abs/2310.16621), Code: **[https://github.com/mbzuai-nlp/artst](https://github.com/mbzuai-nlp/artst)**
- 2023-09-19, **Applying Automated Machine Translation to Educational Video Courses**, Linden Wang et.al., Paper: [http://arxiv.org/abs/2301.03141](http://arxiv.org/abs/2301.03141)
- 2022-10-20, **Anonymizing Speech with Generative Adversarial Networks to Preserve Speaker Privacy**, Sarina Meyer et.al., Paper: [http://arxiv.org/abs/2210.07002](http://arxiv.org/abs/2210.07002), Code: **[https://github.com/digitalphonetics/speaker-anonymization](https://github.com/digitalphonetics/speaker-anonymization)**
- 2022-12-07, **Analysis and Utilization of Entrainment on Acoustic and Emotion Features in User-agent Dialogue**, Daxin Tan et.al., Paper: [http://arxiv.org/abs/2212.03398](http://arxiv.org/abs/2212.03398)
- 2023-10-22, **An overview of text-to-speech systems and media applications**, Mohammad Reza Hasanabadi et.al., Paper: [http://arxiv.org/abs/2310.14301](http://arxiv.org/abs/2310.14301)
- 2022-10-06, **An Overview of Affective Speech Synthesis and Conversion in the Deep Learning Era**, Andreas Triantafyllopoulos et.al., Paper: [http://arxiv.org/abs/2210.03538](http://arxiv.org/abs/2210.03538)
- 2023-10-31, **An Implementation of Multimodal Fusion System for Intelligent Digital Human Generation**, Yingjie Zhou et.al., Paper: [http://arxiv.org/abs/2310.20251](http://arxiv.org/abs/2310.20251), Code: **[https://github.com/zyj-2000/cumt_2d_photospeaker](https://github.com/zyj-2000/cumt_2d_photospeaker)**
- 2023-12-08, **An Experimental Study: Assessing the Combined Framework of WavLM and BEST-RQ for Text-to-Speech Synthesis**, Via Nielson et.al., Paper: [http://arxiv.org/abs/2312.05415](http://arxiv.org/abs/2312.05415)
- 2023-03-10, **An End-to-End Neural Network for Image-to-Audio Transformation**, Liu Chen et.al., Paper: [http://arxiv.org/abs/2303.06078](http://arxiv.org/abs/2303.06078)
- 2022-11-06, **An Empirical Study on L2 Accents of Cross-lingual Text-to-Speech Systems via Vowel Space**, Jihwan Lee et.al., Paper: [http://arxiv.org/abs/2211.03078](http://arxiv.org/abs/2211.03078)
- 2023-10-09, **An Efficient Membership Inference Attack for the Diffusion Model by Proximal Initialization**, Fei Kong et.al., Paper: [http://arxiv.org/abs/2305.18355](http://arxiv.org/abs/2305.18355), Code: **[https://github.com/kong13661/pia](https://github.com/kong13661/pia)**
- 2024-02-26, **An Automated End-to-End Open-Source Software for High-Quality Text-to-Speech Dataset Generation**, Ahmet Gunduz et.al., Paper: [http://arxiv.org/abs/2402.16380](http://arxiv.org/abs/2402.16380), Code: **[https://github.com/aixplain/tts-qa](https://github.com/aixplain/tts-qa)**
- 2024-02-22, **Amphion: An Open-Source Audio, Music and Speech Generation Toolkit**, Xueyao Zhang et.al., Paper: [http://arxiv.org/abs/2312.09911](http://arxiv.org/abs/2312.09911), Code: **[https://github.com/open-mmlab/amphion](https://github.com/open-mmlab/amphion)**
- 2024-02-18, **Ain't Misbehavin' -- Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru**, Zining Wang et.al., Paper: [http://arxiv.org/abs/2402.11571](http://arxiv.org/abs/2402.11571)
- 2023-08-16, **AffectEcho: Speaker Independent and Language-Agnostic Emotion and Affect Transfer for Speech Synthesis**, Hrishikesh Viswanath et.al., Paper: [http://arxiv.org/abs/2308.08577](http://arxiv.org/abs/2308.08577)
- 2024-01-22, **Adversarial speech for voice privacy protection from Personalized Speech generation**, Shihao Chen et.al., Paper: [http://arxiv.org/abs/2401.11857](http://arxiv.org/abs/2401.11857)
- 2022-11-22, **Adversarial Speaker-Consistency Learning Using Untranscribed Speech Data for Zero-Shot Multi-Speaker Text-to-Speech**, Byoung Jin Choi et.al., Paper: [http://arxiv.org/abs/2210.05979](http://arxiv.org/abs/2210.05979)
- 2022-10-21, **Adaptive re-calibration of channel-wise features for Adversarial Audio Classification**, Vardhan Dongre et.al., Paper: [http://arxiv.org/abs/2210.11722](http://arxiv.org/abs/2210.11722)
- 2022-11-01, **Adapter-Based Extension of Multi-Speaker Text-to-Speech Model for New Speakers**, Cheng-Ping Hsieh et.al., Paper: [http://arxiv.org/abs/2211.00585](http://arxiv.org/abs/2211.00585), Code: **[https://github.com/NVIDIA/NeMo](https://github.com/NVIDIA/NeMo)**
- 2022-10-25, **Adapitch: Adaption Multi-Speaker Text-to-Speech Conditioned on Pitch Disentangling with Untranscribed Data**, Xulong Zhang et.al., Paper: [http://arxiv.org/abs/2210.13803](http://arxiv.org/abs/2210.13803)
- 2023-08-02, **Ada-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis**, Zhenhui Ye et.al., Paper: [http://arxiv.org/abs/2306.03504](http://arxiv.org/abs/2306.03504)
- 2022-11-07, **Accented Text-to-Speech Synthesis with a Conditional Variational Autoencoder**, Jan Melechovsky et.al., Paper: [http://arxiv.org/abs/2211.03316](http://arxiv.org/abs/2211.03316), Code: **[https://github.com/dapwner/cvae-tacotron](https://github.com/dapwner/cvae-tacotron)**
- 2023-05-08, **Accented Text-to-Speech Synthesis with Limited Data**, Xuehao Zhou et.al., Paper: [http://arxiv.org/abs/2305.04816](http://arxiv.org/abs/2305.04816)
- 2023-12-27, **AE-Flow: AutoEncoder Normalizing Flow**, Jakub Mosiński et.al., Paper: [http://arxiv.org/abs/2312.16552](http://arxiv.org/abs/2312.16552)
- 2023-05-29, **ADAPTERMIX: Exploring the Efficacy of Mixture of Adapters for Low-Resource TTS Adaptation**, Ambuj Mehrish et.al., Paper: [http://arxiv.org/abs/2305.18028](http://arxiv.org/abs/2305.18028), Code: **[https://github.com/declare-lab/adapter-mix](https://github.com/declare-lab/adapter-mix)**
- 2023-12-17, **A review-based study on different Text-to-Speech technologies**, Md. Jalal Uddin Chowdhury et.al., Paper: [http://arxiv.org/abs/2312.11563](http://arxiv.org/abs/2312.11563)
- 2023-04-16, **A Virtual Simulation-Pilot Agent for Training of Air Traffic Controllers**, Juan Zuluaga-Gomez et.al., Paper: [http://arxiv.org/abs/2304.07842](http://arxiv.org/abs/2304.07842)
- 2023-02-08, **A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech**, Li-Wei Chen et.al., Paper: [http://arxiv.org/abs/2302.04215](http://arxiv.org/abs/2302.04215), Code: **[https://github.com/b04901014/mqtts](https://github.com/b04901014/mqtts)**
- 2023-04-02, **A Survey on Audio Diffusion Models: Text To Speech Synthesis and Enhancement in Generative AI**, Chenshuang Zhang et.al., Paper: [http://arxiv.org/abs/2303.13336](http://arxiv.org/abs/2303.13336)
- 2023-11-17, **A Study on Altering the Latent Space of Pretrained Text to Speech Models for Improved Expressiveness**, Mathias Vogel et.al., Paper: [http://arxiv.org/abs/2311.10804](http://arxiv.org/abs/2311.10804)
- 2022-08-03, **A Study of Modeling Rising Intonation in Cantonese Neural Speech Synthesis**, Qibing Bai et.al., Paper: [http://arxiv.org/abs/2208.02189](http://arxiv.org/abs/2208.02189)
- 2023-05-30, **A Review of Deep Learning Techniques for Speech Processing**, Ambuj Mehrish et.al., Paper: [http://arxiv.org/abs/2305.00359](http://arxiv.org/abs/2305.00359)
- 2022-07-01, **A Polyphone BERT for Polyphone Disambiguation in Mandarin Chinese**, Song Zhang et.al., Paper: [http://arxiv.org/abs/2207.12089](http://arxiv.org/abs/2207.12089)
- 2024-02-09, **A New Approach to Voice Authenticity**, Nicolas M. Müller et.al., Paper: [http://arxiv.org/abs/2402.06304](http://arxiv.org/abs/2402.06304)
- 2022-07-13, **A Cyclical Approach to Synthetic and Natural Speech Mismatch Refinement of Neural Post-filter for Low-cost Text-to-speech System**, Yi-Chiao Wu et.al., Paper: [http://arxiv.org/abs/2207.05913](http://arxiv.org/abs/2207.05913)
- 2023-09-04, **A Comparative Analysis of Pretrained Language Models for Text-to-Speech**, Marcel Granero-Moya et.al., Paper: [http://arxiv.org/abs/2309.01576](http://arxiv.org/abs/2309.01576)

<p align=right>(<a href=#updated-on-20240312>back to top</a>)</p>

## Video Diffusion

- 2024-01-17, **Vlogger: Make Your Dream A Vlog**, Shaobin Zhuang et.al., Paper: [http://arxiv.org/abs/2401.09414](http://arxiv.org/abs/2401.09414), Code: **[https://github.com/zhuangshaobin/vlogger](https://github.com/zhuangshaobin/vlogger)**
- 2023-12-14, **VideoLCM: Video Latent Consistency Model**, Xiang Wang et.al., Paper: [http://arxiv.org/abs/2312.09109](http://arxiv.org/abs/2312.09109)
- 2024-03-08, **VideoElevator: Elevating Video Generation Quality with Versatile Text-to-Image Diffusion Models**, Yabo Zhang et.al., Paper: [http://arxiv.org/abs/2403.05438](http://arxiv.org/abs/2403.05438), Code: **[https://github.com/ybybzhang/videoelevator](https://github.com/ybybzhang/videoelevator)**
- 2024-01-17, **VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models**, Haoxin Chen et.al., Paper: [http://arxiv.org/abs/2401.09047](http://arxiv.org/abs/2401.09047), Code: **[https://github.com/ailab-cvc/videocrafter](https://github.com/ailab-cvc/videocrafter)**
- 2023-08-03, **VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet**, Zhihao Hu et.al., Paper: [http://arxiv.org/abs/2307.14073](http://arxiv.org/abs/2307.14073)
- 2023-03-30, **Video Probabilistic Diffusion Models in Projected Latent Space**, Sihyun Yu et.al., Paper: [http://arxiv.org/abs/2302.07685](http://arxiv.org/abs/2302.07685)
- 2023-06-05, **Video Diffusion Models with Local-Global Context Guidance**, Siyuan Yang et.al., Paper: [http://arxiv.org/abs/2306.02562](http://arxiv.org/abs/2306.02562), Code: **[https://github.com/exisas/lgc-vd](https://github.com/exisas/lgc-vd)**
- 2022-06-22, **Video Diffusion Models**, Jonathan Ho et.al., Paper: [http://arxiv.org/abs/2204.03458](http://arxiv.org/abs/2204.03458)
- 2023-12-03, **ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models**, Jeong-gi Kwak et.al., Paper: [http://arxiv.org/abs/2312.01305](http://arxiv.org/abs/2312.01305)
- 2023-12-01, **VMC: Video Motion Customization using Temporal Attention Adaption for Text-to-Video Diffusion Models**, Hyeonho Jeong et.al., Paper: [http://arxiv.org/abs/2312.00845](http://arxiv.org/abs/2312.00845)
- 2023-10-11, **VDT: General-purpose Video Diffusion Transformers via Mask Modeling**, Haoyu Lu et.al., Paper: [http://arxiv.org/abs/2305.13311](http://arxiv.org/abs/2305.13311), Code: **[https://github.com/rerv/vdt](https://github.com/rerv/vdt)**
- 2024-03-06, **UniCtrl: Improving the Spatiotemporal Consistency of Text-to-Video Diffusion Models via Training-Free Unified Attention Control**, Xuweiyi Chen et.al., Paper: [http://arxiv.org/abs/2403.02332](http://arxiv.org/abs/2403.02332), Code: **[https://github.com/XuweiyiChen/UniCtrl](https://github.com/XuweiyiChen/UniCtrl)**
- 2024-03-05, **Tuning-Free Noise Rectification for High Fidelity Image-to-Video Generation**, Weijie Li et.al., Paper: [http://arxiv.org/abs/2403.02827](http://arxiv.org/abs/2403.02827)
- 2023-05-09, **Style-A-Video: Agile Diffusion for Arbitrary Text-based Video Style Transfer**, Nisha Huang et.al., Paper: [http://arxiv.org/abs/2305.05464](http://arxiv.org/abs/2305.05464), Code: **[https://github.com/haha-lisa/style-a-video](https://github.com/haha-lisa/style-a-video)**
- 2023-02-06, **Structure and Content-Guided Video Synthesis with Diffusion Models**, Patrick Esser et.al., Paper: [http://arxiv.org/abs/2302.03011](http://arxiv.org/abs/2302.03011)
- 2023-11-25, **Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets**, Andreas Blattmann et.al., Paper: [http://arxiv.org/abs/2311.15127](http://arxiv.org/abs/2311.15127), Code: **[https://github.com/stability-ai/generative-models](https://github.com/stability-ai/generative-models)**
- 2023-11-28, **SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models**, Yuwei Guo et.al., Paper: [http://arxiv.org/abs/2311.16933](http://arxiv.org/abs/2311.16933)
- 2023-12-03, **Space-Time Diffusion Features for Zero-Shot Text-Driven Motion Transfer**, Danah Yatim et.al., Paper: [http://arxiv.org/abs/2311.17009](http://arxiv.org/abs/2311.17009)
- 2023-10-17, **Show-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation**, David Junhao Zhang et.al., Paper: [http://arxiv.org/abs/2309.15818](http://arxiv.org/abs/2309.15818), Code: **[https://github.com/showlab/show-1](https://github.com/showlab/show-1)**
- 2023-11-06, **SEINE: Short-to-Long Video Diffusion Model for Generative Transition and Prediction**, Xinyuan Chen et.al., Paper: [http://arxiv.org/abs/2310.20700](http://arxiv.org/abs/2310.20700)
- 2023-12-11, **Probabilistic Precipitation Downscaling with Optical Flow-Guided Diffusion**, Prakhar Srivastava et.al., Paper: [http://arxiv.org/abs/2312.06071](http://arxiv.org/abs/2312.06071)
- 2023-06-02, **Probabilistic Adaptation of Text-to-Video Models**, Mengjiao Yang et.al., Paper: [http://arxiv.org/abs/2306.01872](http://arxiv.org/abs/2306.01872)
- 2023-08-30, **Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models**, Songwei Ge et.al., Paper: [http://arxiv.org/abs/2305.10474](http://arxiv.org/abs/2305.10474)
- 2023-12-11, **Photorealistic Video Generation with Diffusion Models**, Agrim Gupta et.al., Paper: [http://arxiv.org/abs/2312.06662](http://arxiv.org/abs/2312.06662)
- 2022-12-06, **Neural Cell Video Synthesis via Optical-Flow Diffusion**, Manuel Serna-Aguilera et.al., Paper: [http://arxiv.org/abs/2212.03250](http://arxiv.org/abs/2212.03250)
- 2023-10-12, **MotionDirector: Motion Customization of Text-to-Video Diffusion Models**, Rui Zhao et.al., Paper: [http://arxiv.org/abs/2310.08465](http://arxiv.org/abs/2310.08465), Code: **[https://github.com/showlab/MotionDirector](https://github.com/showlab/MotionDirector)**
- 2024-01-22, **Motion-Zero: Zero-Shot Moving Object Control Framework for Diffusion-Based Video Generation**, Changgu Chen et.al., Paper: [http://arxiv.org/abs/2401.10150](http://arxiv.org/abs/2401.10150)
- 2024-01-03, **Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions**, David Junhao Zhang et.al., Paper: [http://arxiv.org/abs/2401.01827](http://arxiv.org/abs/2401.01827), Code: **[https://github.com/salesforce/lavis](https://github.com/salesforce/lavis)**
- 2024-02-19, **Make-A-Protagonist: Generic Video Editing with An Ensemble of Experts**, Yuyang Zhao et.al., Paper: [http://arxiv.org/abs/2305.08850](http://arxiv.org/abs/2305.08850)
- 2023-05-11, **MagicVideo: Efficient Video Generation With Latent Diffusion Models**, Daquan Zhou et.al., Paper: [http://arxiv.org/abs/2211.11018](http://arxiv.org/abs/2211.11018)
- 2024-01-09, **MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation**, Weimin Wang et.al., Paper: [http://arxiv.org/abs/2401.04468](http://arxiv.org/abs/2401.04468)
- 2023-11-27, **MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model**, Zhongcong Xu et.al., Paper: [http://arxiv.org/abs/2311.16498](http://arxiv.org/abs/2311.16498)
- 2022-10-12, **MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation**, Vikram Voleti et.al., Paper: [http://arxiv.org/abs/2205.09853](http://arxiv.org/abs/2205.09853), Code: **[https://github.com/voletiv/mcvd-pytorch](https://github.com/voletiv/mcvd-pytorch)**
- 2024-02-05, **Lumiere: A Space-Time Diffusion Model for Video Generation**, Omer Bar-Tal et.al., Paper: [http://arxiv.org/abs/2401.12945](http://arxiv.org/abs/2401.12945)
- 2023-04-18, **Latent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation**, Jie An et.al., Paper: [http://arxiv.org/abs/2304.08477](http://arxiv.org/abs/2304.08477)
- 2023-03-20, **Latent Video Diffusion Models for High-Fidelity Long Video Generation**, Yingqing He et.al., Paper: [http://arxiv.org/abs/2211.13221](http://arxiv.org/abs/2211.13221), Code: **[https://github.com/yingqinghe/lvdm](https://github.com/yingqinghe/lvdm)**
- 2023-10-02, **LLM-grounded Video Diffusion Models**, Long Lian et.al., Paper: [http://arxiv.org/abs/2309.17444](http://arxiv.org/abs/2309.17444)
- 2023-10-16, **LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation**, Ruiqi Wu et.al., Paper: [http://arxiv.org/abs/2310.10769](http://arxiv.org/abs/2310.10769), Code: **[https://github.com/RQ-Wu/LAMP](https://github.com/RQ-Wu/LAMP)**
- 2023-05-31, **Inverse-design of nonlinear mechanical metamaterials via video denoising diffusion models**, Jan-Hendrik Bastek et.al., Paper: [http://arxiv.org/abs/2305.19836](http://arxiv.org/abs/2305.19836), Code: **[https://github.com/jhbastek/videometamaterials](https://github.com/jhbastek/videometamaterials)**
- 2023-12-19, **InstructVideo: Instructing Video Diffusion Models with Human Feedback**, Hangjie Yuan et.al., Paper: [http://arxiv.org/abs/2312.12490](http://arxiv.org/abs/2312.12490)
- 2023-11-02, **Infusion: Internal Diffusion for Video Inpainting**, Nicolas Cherel et.al., Paper: [http://arxiv.org/abs/2311.01090](http://arxiv.org/abs/2311.01090)
- 2022-10-05, **Imagen Video: High Definition Video Generation with Diffusion Models**, Jonathan Ho et.al., Paper: [http://arxiv.org/abs/2210.02303](http://arxiv.org/abs/2210.02303)
- 2023-12-27, **I2V-Adapter: A General Image-to-Video Adapter for Video Diffusion Models**, Xun Guo et.al., Paper: [http://arxiv.org/abs/2312.16693](http://arxiv.org/abs/2312.16693)
- 2024-02-21, **Hybrid Video Diffusion Models with 2D Triplane and 3D Wavelet Representation**, Kihong Kim et.al., Paper: [http://arxiv.org/abs/2402.13729](http://arxiv.org/abs/2402.13729)
- 2024-01-11, **HiCAST: Highly Customized Arbitrary Style Transfer with Adapter Enhanced Diffusion Models**, Hanzhang Wang et.al., Paper: [http://arxiv.org/abs/2401.05870](http://arxiv.org/abs/2401.05870)
- 2024-01-29, **Generative Video Diffusion for Unseen Cross-Domain Video Moment Retrieval**, Dezhao Luo et.al., Paper: [http://arxiv.org/abs/2401.13329](http://arxiv.org/abs/2401.13329)
- 2023-12-03, **Generative Rendering: Controllable 4D-Guided Video Generation with 2D Diffusion Models**, Shengqu Cai et.al., Paper: [http://arxiv.org/abs/2312.01409](http://arxiv.org/abs/2312.01409)
- 2023-05-29, **Gen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising**, Fu-Yun Wang et.al., Paper: [http://arxiv.org/abs/2305.18264](http://arxiv.org/abs/2305.18264), Code: **[https://github.com/g-u-n/gen-l-video](https://github.com/g-u-n/gen-l-video)**
- 2024-01-30, **FreeNoise: Tuning-Free Longer Video Diffusion via Noise Rescheduling**, Haonan Qiu et.al., Paper: [http://arxiv.org/abs/2310.15169](http://arxiv.org/abs/2310.15169), Code: **[https://github.com/AILab-CVC/FreeNoise](https://github.com/AILab-CVC/FreeNoise)**
- 2023-12-12, **FreeInit: Bridging Initialization Gap in Video Diffusion Models**, Tianxing Wu et.al., Paper: [http://arxiv.org/abs/2312.07537](http://arxiv.org/abs/2312.07537), Code: **[https://github.com/tianxingwu/freeinit](https://github.com/tianxingwu/freeinit)**
- 2023-11-26, **Flow-Guided Diffusion for Video Inpainting**, Bohai Gu et.al., Paper: [http://arxiv.org/abs/2311.15368](http://arxiv.org/abs/2311.15368), Code: **[https://github.com/nevsnev/fgdvi](https://github.com/nevsnev/fgdvi)**
- 2024-02-21, **Feature-Conditioned Cascaded Video Diffusion Models for Precise Echocardiogram Synthesis**, Hadrien Reynaud et.al., Paper: [http://arxiv.org/abs/2303.12644](http://arxiv.org/abs/2303.12644), Code: **[https://github.com/HReynaud/EchoDiffusion](https://github.com/HReynaud/EchoDiffusion)**
- 2023-08-26, **Empowering Dynamics-aware Text-to-Video Diffusion with Large Language Models**, Hao Fei et.al., Paper: [http://arxiv.org/abs/2308.13812](http://arxiv.org/abs/2308.13812)
- 2023-10-11, **Echocardiography video synthesis from end diastolic semantic map via diffusion model**, Phi Nguyen Van et.al., Paper: [http://arxiv.org/abs/2310.07131](http://arxiv.org/abs/2310.07131)
- 2023-11-27, **DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors**, Jinbo Xing et.al., Paper: [http://arxiv.org/abs/2310.12190](http://arxiv.org/abs/2310.12190), Code: **[https://github.com/Doubiiu/DynamiCrafter](https://github.com/Doubiiu/DynamiCrafter)**
- 2023-02-02, **Dreamix: Video Diffusion Models are General Video Editors**, Eyal Molad et.al., Paper: [http://arxiv.org/abs/2302.01329](http://arxiv.org/abs/2302.01329)
- 2023-12-12, **DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance**, Cong Wang et.al., Paper: [http://arxiv.org/abs/2312.03018](http://arxiv.org/abs/2312.03018)
- 2023-12-07, **DreamVideo: Composing Your Dream Videos with Customized Subject and Motion**, Yujie Wei et.al., Paper: [http://arxiv.org/abs/2312.04433](http://arxiv.org/abs/2312.04433)
- 2024-02-05, **Direct-a-Video: Customized Video Generation with User-Directed Camera Movement and Object Motion**, Shiyuan Yang et.al., Paper: [http://arxiv.org/abs/2402.03162](http://arxiv.org/abs/2402.03162)
- 2023-12-21, **Diffusion Reward: Learning Rewards via Conditional Video Diffusion**, Tao Huang et.al., Paper: [http://arxiv.org/abs/2312.14134](http://arxiv.org/abs/2312.14134)
- 2022-12-08, **Diffusion Probabilistic Modeling for Video Generation**, Ruihan Yang et.al., Paper: [http://arxiv.org/abs/2203.09481](http://arxiv.org/abs/2203.09481), Code: **[https://github.com/buggyyang/rvd](https://github.com/buggyyang/rvd)**
- 2022-11-14, **Diffusion Models for Video Prediction and Infilling**, Tobias Höppe et.al., Paper: [http://arxiv.org/abs/2206.07696](http://arxiv.org/abs/2206.07696), Code: **[https://github.com/Tobi-r9/RaMViD](https://github.com/Tobi-r9/RaMViD)**
- 2023-10-18, **DORSal: Diffusion for Object-centric Representations of Scenes et al**, Allan Jabri et.al., Paper: [http://arxiv.org/abs/2306.08068](http://arxiv.org/abs/2306.08068)
- 2023-12-07, **Customizing Motion in Text-to-Video Diffusion Models**, Joanna Materzynska et.al., Paper: [http://arxiv.org/abs/2312.04966](http://arxiv.org/abs/2312.04966)
- 2024-02-22, **Customize-A-Video: One-Shot Motion Customization of Text-to-Video Diffusion Models**, Yixuan Ren et.al., Paper: [http://arxiv.org/abs/2402.14780](http://arxiv.org/abs/2402.14780)
- 2024-01-18, **CustomVideo: Customizing Text-to-Video Generation with Multiple Subjects**, Zhao Wang et.al., Paper: [http://arxiv.org/abs/2401.09962](http://arxiv.org/abs/2401.09962)
- 2024-02-28, **Context-aware Talking Face Video Generation**, Meidai Xuanyuan et.al., Paper: [http://arxiv.org/abs/2402.18092](http://arxiv.org/abs/2402.18092)
- 2023-09-21, **Compositional Foundation Models for Hierarchical Planning**, Anurag Ajay et.al., Paper: [http://arxiv.org/abs/2309.08587](http://arxiv.org/abs/2309.08587)
- 2023-07-24, **Broken Neural Scaling Laws**, Ethan Caballero et.al., Paper: [http://arxiv.org/abs/2210.14891](http://arxiv.org/abs/2210.14891), Code: **[https://github.com/ethancaballero/broken_neural_scaling_laws](https://github.com/ethancaballero/broken_neural_scaling_laws)**
- 2023-11-21, **Breathing Life Into Sketches Using Text-to-Video Priors**, Rinon Gal et.al., Paper: [http://arxiv.org/abs/2311.13608](http://arxiv.org/abs/2311.13608)
- 2024-02-02, **Boximator: Generating Rich and Controllable Motions for Video Synthesis**, Jiawei Wang et.al., Paper: [http://arxiv.org/abs/2402.01566](http://arxiv.org/abs/2402.01566)
- 2023-12-05, **BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis via Bridging Image and Video Diffusion Models**, Fengyuan Shi et.al., Paper: [http://arxiv.org/abs/2312.02813](http://arxiv.org/abs/2312.02813)
- 2024-02-08, **Animated Stickers: Bringing Stickers to Life with Video Diffusion**, David Yan et.al., Paper: [http://arxiv.org/abs/2402.06088](http://arxiv.org/abs/2402.06088)
- 2023-12-06, **AnimateZero: Video Diffusion Models are Zero-Shot Image Animators**, Jiwen Yu et.al., Paper: [http://arxiv.org/abs/2312.03793](http://arxiv.org/abs/2312.03793), Code: **[https://github.com/vvictoryuki/animatezero](https://github.com/vvictoryuki/animatezero)**
- 2024-02-01, **AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning**, Fu-Yun Wang et.al., Paper: [http://arxiv.org/abs/2402.00769](http://arxiv.org/abs/2402.00769), Code: **[https://github.com/g-u-n/animatelcm](https://github.com/g-u-n/animatelcm)**
- 2023-12-04, **AnimateAnything: Fine-Grained Open Domain Image Animation with Motion Guidance**, Zuozhuo Dai et.al., Paper: [http://arxiv.org/abs/2311.12886](http://arxiv.org/abs/2311.12886), Code: **[https://github.com/alibaba/animate-anything](https://github.com/alibaba/animate-anything)**
- 2024-02-19, **Animate124: Animating One Image to 4D Dynamic Scene**, Yuyang Zhao et.al., Paper: [http://arxiv.org/abs/2311.14603](http://arxiv.org/abs/2311.14603)
- 2024-01-19, **ActAnywhere: Subject-Aware Video Background Generation**, Boxiao Pan et.al., Paper: [http://arxiv.org/abs/2401.10822](http://arxiv.org/abs/2401.10822)
- 2024-03-01, **Abductive Ego-View Accident Video Understanding for Safe Driving Perception**, Jianwu Fang et.al., Paper: [http://arxiv.org/abs/2403.00436](http://arxiv.org/abs/2403.00436)
- 2023-11-29, **A Unified Approach for Text- and Image-guided 4D Scene Generation**, Yufeng Zheng et.al., Paper: [http://arxiv.org/abs/2311.16854](http://arxiv.org/abs/2311.16854)
- 2023-10-16, **A Survey on Video Diffusion Models**, Zhen Xing et.al., Paper: [http://arxiv.org/abs/2310.10647](http://arxiv.org/abs/2310.10647), Code: **[https://github.com/ChenHsing/Awesome-Video-Diffusion-Models](https://github.com/ChenHsing/Awesome-Video-Diffusion-Models)**
- 2023-12-28, **4DGen: Grounded 4D Content Generation with Spatial-temporal Consistency**, Yuyang Yin et.al., Paper: [http://arxiv.org/abs/2312.17225](http://arxiv.org/abs/2312.17225)
- 2024-01-12, **360DVD: Controllable Panorama Video Generation with 360-Degree Video Diffusion Model**, Qian Wang et.al., Paper: [http://arxiv.org/abs/2401.06578](http://arxiv.org/abs/2401.06578)

<p align=right>(<a href=#updated-on-20240312>back to top</a>)</p>

Notes: 

* We have modified the `sorting rule` of the above table to prioritize papers based on the time of their latest update rather than their initial publication date. If an article has been recently modified, it will appear earlier in the list. 

* However, recent trends are still based on `ten` papers sorted by the initial publication date. 

Function added: 

* Support more reliable text parser. [Link](https://github.com/pdfminer/pdfminer.six) 

* Support rich markdown format (better at parsing experimental tables). [Link](https://github.com/davendw49/sciparser) 

* Supports the analysis of more than 10 papers in a single conversation, which exceeds the attachment size limit. 

[contributors-shield]: https://img.shields.io/github/contributors/liutaocode/talking-face-arxiv-daily.svg?style=for-the-badge
[contributors-url]: https://github.com/liutaocode/talking-face-arxiv-daily/graphs/contributors
[forks-shield]: https://img.shields.io/github/forks/liutaocode/talking-face-arxiv-daily.svg?style=for-the-badge
[forks-url]: https://github.com/liutaocode/talking-face-arxiv-daily/network/members
[stars-shield]: https://img.shields.io/github/stars/liutaocode/talking-face-arxiv-daily.svg?style=for-the-badge
[stars-url]: https://github.com/liutaocode/talking-face-arxiv-daily/stargazers
[issues-shield]: https://img.shields.io/github/issues/liutaocode/talking-face-arxiv-daily.svg?style=for-the-badge
[issues-url]: https://github.com/liutaocode/talking-face-arxiv-daily/issues

